0:00:00.000,0:00:02.760
돌아오신 것을 환영 합니다.

0:00:03.400,0:00:08.440
지난 주엔 꽤 바쁜 레슨을 가졌었습니다.

0:00:10.060,0:00:13.475
저는 여기 USF의 한 석사생이

0:00:13.480,0:00:19.240
저희가 배운 내용을 사용해서 
구조화된 딥러닝 관련 내용을

0:00:19.260,0:00:23.800
블로그 글로 적은 것을 봤는데

0:00:23.800,0:00:29.660
매우 인기 있는 글이 되어서, 흥분되더군요.

0:00:29.660,0:00:33.540
왜냐하면, 아직 많은 사람들이 
잘 모르던 내용이었잖아요?

0:00:33.540,0:00:35.280
그 글은

0:00:35.340,0:00:40.560
제가 데이터과학쪽의 트렌드를 쫓는데 
꽤 잘 사용하고 있는

0:00:40.560,0:00:46.120
Towards Data Science 라는 
Medium의 출판사에 선정되었습니다.

0:00:46.180,0:00:49.215
이 글의 저자인 Karen은 기본적으로

0:00:49.220,0:00:52.840
구조화된 딥러닝을 다루고 있는데

0:00:52.920,0:00:56.520
저희가 지난 주에 배운 내용 입니다.

0:00:56.520,0:01:00.800
이 글은 꽤 많은 사람들로 부터 선정 되었습니다.

0:01:00.900,0:01:05.160
특히, 제가 좋아하는 연구자 중 한 명인

0:01:05.160,0:01:09.420
Sebastian Ruder가 지난 주 강의에 대해서 
트위터 글을 남겨주기도 했습니다.

0:01:09.420,0:01:11.320
또, Stitchfix 에 있는 누군가가

0:01:11.320,0:01:14.820
이 방법을 꽤 오랬동안 사용 해 왔다고 
언급하기도 했었습니다.

0:01:17.240,0:01:20.315
저는 이 방법이 산업에서 꽤 많이 
사용되고 있음을 알고 있고,

0:01:20.320,0:01:23.880
이 사실을 많은 분들께 말씀 드리기도 했지만
그 누구도 이에 대해서 언급하진 않았었습니다.

0:01:23.880,0:01:26.180
근데 지금 Karen 이

0:01:26.180,0:01:29.260
이 멋진 방법을 확인해 보는게 어때? 
하는 생각으로 글을 적었더니

0:01:29.260,0:01:31.400
stitchfix에선
이미 해 오던 것이라고 하는군요

0:01:31.400,0:01:33.800
아주 좋습니다.

0:01:33.800,0:01:35.520
제 생각엔

0:01:35.520,0:01:40.480
구조화된 딥러닝에는 여전히 
파고들 내용이 많다고 생각 합니다.

0:01:40.480,0:01:43.900
Keran이 적은 글의 내용에 기반해서

0:01:43.900,0:01:46.700
다른 데이터셋을 가지고 실험을 해 볼 수 있고,

0:01:46.700,0:01:49.840
예전에 진행된 Kaggle 경연의 문제를 풀어봐서,

0:01:49.920,0:01:53.620
이 방법을 사용하면 그 경연에서 
우승할 수 있음을 확인 해 보거나

0:01:53.620,0:01:57.380
어떤 문제에는 잘 동작하지 않는다는 것을 
확인 해 볼 수 있을 겁니다.

0:01:57.380,0:02:00.500
또는 다른 정도의 dropout의 적용이나,

0:02:00.500,0:02:03.900
계층 크기 달리하여 적용 해 볼 수도 있겠죠.

0:02:03.900,0:02:06.720
왜냐하면, 이 것에 대해서 
언급한 사람들이 거의 없기 때문입니다.

0:02:06.780,0:02:10.700
제 생각엔 이전엔, 이 내용 관련으로 작성된 
블로그 글을 본 적이 전혀 없습니다.

0:02:11.280,0:02:14.260
한마디로, 아직 개척되지 않은 영역이 있다는 것이고

0:02:14.260,0:02:17.620
이 개념을 기반으로 
많은 것을 만들어 볼 기회가 있을 것입니다.

0:02:17.620,0:02:20.220
현재는 많은 관심을 얻고 있는 것 같습니다.

0:02:20.220,0:02:24.000
트위터의 누군가가 아주 오랫동안 
찾아오던 방법이라고 언급하기도 했지요

0:02:24.660,0:02:28.000
또 소개해 드리고 싶은 글은

0:02:28.095,0:02:31.235
Nikhil 이 작성한 것으로, 레슨1 에 대해서

0:02:31.235,0:02:33.775
크리켓과 야구를 분류하는 것에 대한 내용과

0:02:33.775,0:02:36.075
각국의 통화를 분류하는 내용을 다뤘습니다.

0:02:36.385,0:02:39.495
그리고, 레슨2에 대한 것도 작성 했는데

0:02:39.520,0:02:43.040
200장의 배우들의 이미지에 대한 것으로

0:02:43.040,0:02:46.180
약간 더 큰 데이터를 다뤘습니다.

0:02:46.180,0:02:49.100
이 데이터는 구글에서 안경을 쓴 배우와

0:02:49.100,0:02:52.360
그렇지 않은 배우들의 사진을 
구글에서 검색/다운로드 한 것으로

0:02:52.360,0:02:57.100
사진들이 각 카테고리별로 잘 분류(레이블링) 
되어 있는지 직접 확인 하였습니다.

0:02:57.100,0:02:59.140
이 경우는

0:02:59.160,0:03:03.260
아주기본 ResNet에 단순히 마지막 계층을
추가 했을때, 별로 잘 동작하지 못한 좋은 예가 됩니다.

0:03:03.300,0:03:06.025
이를 해결하기 위해서 Nikhil은

0:03:06.025,0:03:08.865
계층들의 freeze를 해제한 후,

0:03:08.865,0:03:12.360
차등 학습률을 사용했습니다.
그 결과로 100%의 정확도를 얻을 수 있었죠.

0:03:12.360,0:03:15.460
이 글에서 Nikhil이 한것 중, 제가 좋아한 것은

0:03:15.460,0:03:18.660
Kaggle 데이터셋을 다운로드 한 것이 아니라

0:03:18.660,0:03:21.880
해결 하고자 하는 문제를 완전히 처음부터

0:03:21.880,0:03:25.060
스스로 정하고 해결 했다는 점입니다.

0:03:25.060,0:03:29.560
또한 구글에서 이미지를 다운로드 하는 
방법 또한 소개 해 주고 있지요.

0:03:29.565,0:03:32.775
아주 좋은 글이라고 생각 합니다.

0:03:32.835,0:03:35.825
저는 오늘 오후에 Singularity 대학에서

0:03:35.825,0:03:39.480
전세계에서 통신관련 가장 큰 회사의 경영진과

0:03:39.480,0:03:41.545
대화 할 기회가 있었는데,

0:03:41.545,0:03:44.295
이 블로그 글을 보여 줬었습니다.

0:03:44.295,0:03:47.445
그 회사분들이 저에게 말하길
수 많은 고객들이 찾아와서

0:03:47.445,0:03:50.665
엄청나게 많은 이미지들이 필요한데

0:03:50.665,0:03:53.820
이를 위한 특별한 소프트웨어와
하드웨어가 갖춰진

0:03:53.820,0:03:56.880
데이터센터가 필요하다고 했답니다.

0:03:56.880,0:03:58.905
그래서 제가

0:03:58.905,0:04:01.560
이 글을 적은분은 제 강의를 3주 정도 들었는데

0:04:01.560,0:04:03.645
그가 한 일을 보여주면서,

0:04:03.645,0:04:07.160
시간당 약 60센트 정도하는
컴퓨터를 사용 했다고 말해 줬죠.

0:04:07.160,0:04:09.260
그러자 그분들은

0:04:09.260,0:04:13.200
보통의 일반인도 해 낼 수 있다는 사실에
매우 기뻐 하더군요.

0:04:13.400,0:04:17.120
실제 만난 적은 없지만, 
Nikhil이 보통의 일반인 이라고 생각 합니다.

0:04:17.120,0:04:20.960
만약 스스로가 일반인이 아니라고 생각 한다면, 
사과 드리겠습니다

0:04:21.380,0:04:26.520
저는 사실, 실제로
이 분의 크리켓 분류 모델을 살펴 봤는데

0:04:26.520,0:04:31.760
사용하신 코드가 레슨1 강의에서의 것과 
완전히 동일하다는 사실에 기뻤습니다.

0:04:31.760,0:04:34.160
다른 문제에 사용되어도

0:04:34.160,0:04:36.220
좋은 결과를 내길 기대 했었거든요.

0:04:36.220,0:04:39.620
단지 이 분이 바꾼 유일한 부분은
에포크의 횟수 인것 같습니다.

0:04:39.620,0:04:42.820
이 예제와 마찬가지로
단 4줄의 코드를 복사해서

0:04:42.820,0:04:46.420
다른 문제에 적용 해도
옳을 수 있다 라는것입니다.

0:04:47.360,0:04:50.375
여러분이 제가 오늘 대화를 나눈 회사처럼

0:04:50.375,0:04:52.895
큰 회사에 소속된 경영진 이시라면,

0:04:52.900,0:04:56.260
만약 이 방법이 먹히는게 사실이면

0:04:56.260,0:05:00.240
왜 모든 사람들이 이 방법을 사용하지 않는지를

0:05:00.280,0:05:04.420
의심하는 등의 약간의 반발이 있는건 
별로 놀라운 일이 아닐 것입니다.

0:05:04.420,0:05:07.420
그러면, 단순히 그럴 수 있다는 것을

0:05:07.420,0:05:09.840
회사 내부에서 사용되는 데이터에 대해서

0:05:09.840,0:05:12.980
직접 모델을 만들어서 보여주면 됩니다.

0:05:12.980,0:05:16.940
비용도 별로 들이지 않고, 
완료 할 수 있다는 것을요 말이죠.

0:05:19.120,0:05:22.060
이 분의 이름을 어떻게 발음해야 할지 잘 모르겠지만,

0:05:22.060,0:05:25.835
Vitaly 라는 분은 
뉴럴넷을 학습시키는 방법을 소개하는

0:05:25.840,0:05:28.845
또 다른 아주 좋은 글을 적어 주셨습니다.

0:05:29.400,0:05:32.540
제가 이 글을 선정한 이유는

0:05:32.540,0:05:36.600
Vitaly는 기술적인 대화에 
재능을 가진 분인것 같은데

0:05:36.600,0:05:38.920
저희 모두가 이 분의 글을 읽어봄으로써

0:05:38.945,0:05:42.075
좋은 기술적인 글 작성법을 
배울 수 있기 때문입니다.

0:05:42.075,0:05:44.905
제가 특히나 좋아했던 부분은

0:05:44.905,0:05:47.735
이 분은 독자가 아무것도 모른다고 가정을 한채

0:05:47.740,0:05:51.380
아주 친근한 어조를 사용해서 
모든 내용을 전체적으로 묘사 했습니다.

0:05:51.380,0:05:54.960
하지만, 논문이나 수학공식 등을 언급하는 등

0:05:54.960,0:05:58.320
동시에 독자들이 똑똑할 것이라고 가정을 했죠.

0:05:58.320,0:06:02.540
그리곤, 수학공식들이 의미하는 바가 
무엇인지 친절하게 설명을 했습니다.

0:06:02.540,0:06:06.940
따라서, 똑똑한 독자를 준중한것 뿐만 아니라

0:06:06.940,0:06:12.600
아무런 배경 지식이 없는 독자일 수도 있다는 것을
동시에 신경 쓴 것입니다.

0:06:14.895,0:06:17.835
이번 주 초에

0:06:17.835,0:06:20.575
Kaggle의 Planet Seedling 경연에서의

0:06:20.575,0:06:23.845
저의 1등한 장면의 사진을 포스트 하였는데

0:06:23.845,0:06:26.935
이 후, 이 코스를 듣는 다른 5명의 학생들이

0:06:26.935,0:06:29.955
제 성적을 추월한 적이 있었습니다.

0:06:31.100,0:06:35.180
지금 보고 계신 화면은
Planet Seedling 경연의 현재 리더보드 상황이고

0:06:35.180,0:06:38.980
상위 6위권 모두 
fastai 학생들이 차지 했습니다.

0:06:39.000,0:06:41.240
저를 포함해서 말이죠.

0:06:43.580,0:06:47.855
(James가 1등 이었는데 2등으로 밀려 났군요.)

0:06:47.860,0:06:51.340
여러분이 어느정도 까지 할 수 있는지를 보여주는

0:06:51.340,0:06:55.580
아주 좋은 예라고 볼 수 있습니다.

0:06:56.880,0:07:00.920
심지어 사용된 이미지는 수천개 정도로 적었고,

0:07:00.940,0:07:05.600
대부분의 이미지는 100x100 보다도 작았습니다.

0:07:07.680,0:07:10.320
제가 사용한 방식은 디폴트로

0:07:10.320,0:07:13.680
단순히 수업에서 배운 노트북 코드를
그대로 돌린 것으로

0:07:13.680,0:07:16.620
약 1시간 정도 걸린 것이었습니다.

0:07:18.200,0:07:21.915
다른 학생분들은 디폴트 보다는
좀 더 많은 뭔가를 하셨을 겁니다.

0:07:21.920,0:07:25.500
하지만, 크게 다르진 않을 것입니다.
따라서 이 결과가 말해주는것은

0:07:25.500,0:07:28.255
이 기법이 꽤나 안정적으로

0:07:28.260,0:07:33.700
fastai 를 사용하지 않은 사람들이
힘겹게 얻은 수준의 결과까지

0:07:33.700,0:07:38.480
도달할 수 있게 해준다는 것입니다.

0:07:40.580,0:07:46.100
꽤 흥미롭고 멋진 상황이었다고 생각 합니다.

0:07:48.100,0:07:51.200
오늘 저희가 배울 내용은

0:07:51.200,0:07:56.480
전체 코스의 두번째 중간지점 쯤 되는 것입니다.

0:07:56.480,0:07:58.775
첫번째 중간지점의 내용이 다룬 것은

0:07:58.775,0:08:01.905
여러가지 기법들을 적용 가능한

0:08:01.905,0:08:04.645
어플리케이션이 어떤 것들이 있는지

0:08:04.645,0:08:06.320
살펴 보았고,

0:08:06.400,0:08:09.860
그러기 위해 작성해야 하는 코드를 보여 드리는 등

0:08:09.940,0:08:13.720
상위레벨 차원의 설명을 곁들인 것이었습니다.

0:08:15.300,0:08:18.200
이런 종류의 과정은 지난 주로 끝났고,

0:08:18.200,0:08:20.975
지금부터 우리가 하게 될 것은 
되돌아 가는 것입니다.

0:08:20.980,0:08:24.160
전에 한 것과 동일한 주제로 다시 돌아가는데

0:08:24.160,0:08:27.540
이번에는 각 주제에 대해서, 
좀 더 깊이 있게 다루게 됩니다.

0:08:27.540,0:08:30.860
그러면서 Fastai 라이브러리의 
소스코드 내부도 살펴보고,

0:08:30.860,0:08:34.500
우리 스스로가 그 코드를 흉내내 보기도 할 것입니다.

0:08:34.500,0:08:36.880
학생분들을 위해서, 제가 해 줄 수 있는건

0:08:36.880,0:08:42.980
이보다 더 좋은 방법은 없을 것입니다.

0:08:43.280,0:08:47.115
지금까진 제가 아는한 가장 좋은 기법과 
코딩법을 소개해 드렸었지만,

0:08:47.115,0:08:50.205
이제는 그 방법을 기반으로,

0:08:50.205,0:08:53.325
코스의 두 번째 부분에 접어들면
새로운 것을 시도하기 위해 필요한

0:08:53.325,0:08:56.165
모델을 직접 만드는 것을 수행하면서,
디버깅도 함께 해 보게 될 것입니다.

0:08:56.165,0:09:00.020
이 방식대로라면, 내부적으로 일어나는 일을 
이해하는데 큰 도움이 될 것입니다.

0:09:00.020,0:09:02.355
오늘 강의의 목표는

0:09:02.360,0:09:07.980
꽤 효과적인 협업 필터링 모델을

0:09:07.980,0:09:12.200
거의 완전히 별다른 준비 없이
만들어 보고, 사용도 해 보는 것입니다.

0:09:12.200,0:09:13.595
그 과정에서

0:09:13.595,0:09:16.595
저희는 PyTorch를
자동 미분을 위한,

0:09:16.600,0:09:20.160
GPU 프로그래밍을 위한,
도구로써 사용하게 될 것입니다.

0:09:20.160,0:09:22.515
되도록 PyTorch의 뉴럴넷 기능은 
사용하지 않을 것이고

0:09:22.515,0:09:24.945
Fastai 라이브러리 또한

0:09:24.945,0:09:28.720
최소한으로 사용하게 될 것입니다.

0:09:29.460,0:09:33.260
지난 시간에 너무 빠르게 협업 필터링 내용을 훑은 관계로

0:09:33.260,0:09:36.860
일단 다시 처음으로 돌아가서 
한번 더 훑어보는 시간을 가져봅시다.

0:09:38.140,0:09:42.280
저희가 사용할 데이터셋은 
"MovieLens" 라는 것입니다.

0:09:43.160,0:09:46.720
MovieLens 데이터셋은 기본적으로

0:09:46.720,0:09:49.760
영화 평점의 목록을 가지고 있습니다.

0:09:49.760,0:09:52.820
ID로 구분되는 서로다른 사용자 정보,

0:09:52.820,0:09:55.540
ID로 구분되는 서로다른 영화 정보,

0:09:55.540,0:09:57.740
그리고, 평점 정보가 들어 있습니다.

0:09:57.960,0:10:00.205
또한, 타임스탬프도 기록되어 있는데,

0:10:00.205,0:10:02.925
단순히 평점이 매겨진 시간 정보일 뿐이라서

0:10:02.925,0:10:06.460
개인적으론 이 정보를 사용해 본적은 아직 없습니다.

0:10:07.240,0:10:09.940
세 개의 컬럼(행)인,

0:10:10.000,0:10:13.060
userId, movieId, rating 정보들이

0:10:13.065,0:10:15.835
모델링을 하기위해 사용되는 모든것들 입니다.

0:10:15.835,0:10:18.785
구조화된 데이터 관점에서 생각 해 보자면,

0:10:18.785,0:10:22.320
userId와 movieId는 범주형 변수이고

0:10:22.320,0:10:26.360
rating은 의존 변수일 것입니다. 
(의존-예측 대상)

0:10:28.280,0:10:31.195
아래의 데이터는 모델링에 사용되진 않지만,

0:10:31.195,0:10:33.885
나중에 참고용으로 사용될 수 있습니다.

0:10:33.885,0:10:37.205
실제 영화 타이틀을 확인하는 용도로 말입니다.

0:10:37.205,0:10:40.265
또한, 장르 정보도 사용될 수 있을 것 같은데
저는 아직 사용 해 보진 않았습니다.

0:10:40.265,0:10:43.980
제 추측엔 별 도움이 안될 것 같지만,
한주 동안, 누군가가 이 정보를 사용하는게

0:10:43.980,0:10:46.520
어떤 효과가 있는지 알아낸다면 좋겠군요.

0:10:48.720,0:10:52.840
이 데이터를 좀 더 나은 방법으로 바라보기 위해서

0:10:52.840,0:10:57.980
가장 많은 영화를 시청한 사용자들과,

0:10:57.980,0:11:01.960
가장 많이 시청된 영화 정보들을 가져와서

0:11:01.960,0:11:04.480
크로스 테이블로 표현 해 봤습니다

0:11:04.480,0:11:08.920
보이시는 것은 앞의 것과 동일한 데이터에서
일부분을 추출한 것인데

0:11:08.920,0:11:13.920
3개의 컬럼(행)으로 표현하기 보단
사용자와 영화별로 평점을 표현했습니다.

0:11:14.320,0:11:16.480
몇 사용자들은

0:11:16.480,0:11:21.040
나열된 영화의 몇을 시청하지 않은 경우도 있는데
NaN 값으로 표현되어 있습니다.

0:11:21.620,0:11:25.440
저는 이 내용을 엑셀로 복사 해 봤습니다.

0:11:26.780,0:11:31.015
collab_filter.xlsx 라는 파일을 확인 해 보시기 바랍니다.

0:11:31.020,0:11:34.340
저도 파일이 업로드 되었는지 다시 확인 해 보겠습니다.

0:11:35.540,0:11:41.740
일단 화면을 보시면, 
제가 복사해서 넣은 정보를 보실 수 있습니다.

0:11:42.200,0:11:45.275
저는 문제를 규정하고,

0:11:45.280,0:11:48.820
그 문제에 대한 설명을 하게 될 텐데

0:11:49.300,0:11:55.360
잘 이해가 안되시면 
직접 또는 포럼을 통해서 질문 해 주시기 바랍니다.

0:11:56.325,0:11:59.125
포럼을 통해서 질문 하시면,

0:11:59.125,0:12:02.155
다른 누군가가 대답해 주고, 여기선 제가 하겠지만

0:12:02.160,0:12:05.040
그 대답이 마음에 들면 좋아요를 클릭 해서

0:12:05.040,0:12:10.340
다른 분들도 볼 수 있도록 도와 주시기 바랍니다.

0:12:10.340,0:12:14.220
지금부턴 내부적으로 일어나는 내용을
상세하게 다루게 될 것이기 때문에,

0:12:14.220,0:12:18.240
각 설명마다 이를 확실히 이해하는 것이 중요합니다.

0:12:23.720,0:12:25.000
저희는

0:12:25.000,0:12:30.820
이 문제에 대한 시작을 뉴럴넷을 만드는 것 대신

0:12:30.940,0:12:35.540
행렬 분해(matrix factorization) 라는 것을 
하게 될 것입니다.

0:12:36.065,0:12:39.275
뉴럴넷을 만들지 않는 이유는

0:12:39.275,0:12:42.365
이 문제를 해결하기 위한

0:12:42.365,0:12:45.760
아주 아주 간단한 방법이 있기 때문입니다.

0:12:45.760,0:12:48.555
약간 화면을 아래로 내려보면

0:12:48.560,0:12:50.780
제가 선택한 영역이 있는데,

0:12:50.780,0:12:54.140
위와 동일한 형태를 가지는 테이블이 있습니다.

0:12:54.140,0:12:56.760
단, 이번에는 예측된 결과가 담깁니다.

0:12:56.760,0:12:59.075
이 값들을 어떻게 채워 넣었는지를 설명 드리겠습니다.

0:12:59.075,0:13:01.945
여기 위쪽에는 실제 (레이블링된) 값들이 있고,

0:13:01.945,0:13:04.760
아래 쪽에는 예측값들이 있고,

0:13:05.100,0:13:08.760
가장 오른쪽 하단 부분에는

0:13:08.760,0:13:18.240
RMSE(Root Mean Squared Error)들의 
평균으로 구해진 점수가 있습니다.

0:13:18.560,0:13:22.440
현재 랜덤으로 초기화된 파라메터를 통해서 구해진

0:13:22.440,0:13:24.700
이 점수는 약 2.8 입니다.

0:13:24.705,0:13:27.085
그러면, 이 모델이 무엇인지

0:13:27.085,0:13:32.000
어떻게 userid가 14인 사람이 
movieid 가 27인 것을

0:13:32.000,0:13:36.740
얼마나 좋아 할 지를 추측하는 것을 예로 들어서
보여드리도록 하겠습니다.

0:13:36.980,0:13:43.060
랜덤한 상태에서의 이 예측값은 0.91 입니다.

0:13:43.200,0:13:46.600
어떻게 0.91이 계산된 것일까요?

0:13:46.600,0:13:50.040
이 질문에 대한 대답으로, 이 값은

0:13:50.100,0:13:52.820
위의 보라색 벡터에,

0:13:52.860,0:13:55.915
왼쪽의 빨간색 벡터를
닷( . ) 연산한 결과 입니다.

0:13:55.915,0:13:58.745
닷( . ) 연산이라고 함은
0.71 * 0.19

0:13:58.745,0:14:01.755
더하기 0.81 * 0.63

0:14:01.755,0:14:05.055
더하기 0.74 * 0.31
등등 같은 과정 입니다.

0:14:05.645,0:14:08.535
선형대수의 관점에서 보면

0:14:08.540,0:14:10.900
하나는 행이고, 다른 하나는 열이기 때문에
(Column, Row)

0:14:10.900,0:14:13.240
행렬 곱셈과 동일한 것으로 볼 수 있습니다.

0:14:13.280,0:14:16.520
여기서는, 엑셀에서 제공하는 
행렬곱셈 함수를 사용 하고 있습니다. (MMULT)

0:14:16.520,0:14:19.460
이렇게 해서 구해진것이 예측값 입니다.

0:14:21.180,0:14:26.640
만약 오리지널 평점값(기존 데이터)이 
존재하지 않는 부분이 있다면

0:14:26.640,0:14:30.635
예측값은 0으로 설정 됩니다.

0:14:30.640,0:14:34.860
왜냐하면 일어나지 않은 일에대한 예측에는 
에러도 없기 때문입니다.

0:14:35.700,0:14:38.640
모든 예측된 평점들에 대해선

0:14:38.645,0:14:41.445
뉴럴넷이 사용되지 않았고,

0:14:41.445,0:14:47.020
단순히 한번의 행렬 곱셈만이 수행 되었습니다.

0:14:48.080,0:14:50.805
행렬 곱셈의 수행은 기본적으로

0:14:50.805,0:14:55.880
위에 있는 행렬과

0:14:55.880,0:15:01.600
왼쪽에 있는 행렬 사이에 일어나는 것입니다.

0:15:01.960,0:15:07.200
따라서, 예측값의 각 셀은 
이 행렬 곱셈 결과의 부분부분들인 것이지요.

0:15:09.080,0:15:14.300
저는 이 행렬들의 값을 랜덤하게 초기화 했습니다.

0:15:14.300,0:15:20.220
랜덤값으로 세팅된 두 개의 
행렬을 가지고 시작을 한 것입니다.

0:15:20.220,0:15:23.380
그러면, 지금부터는

0:15:23.380,0:15:27.655
모든 평점들이 이 행렬간의 곱셈으로

0:15:27.655,0:15:30.615
도출된 것이라고 간주 해 볼 수 있을 것입니다.

0:15:30.615,0:15:32.745
그러고 나면, 엑셀에서

0:15:32.745,0:15:36.900
실제로 "경사 하강"을 수행 해 볼 수 있는데요

0:15:36.900,0:15:41.960
옵션에 가셔서 Add-ins 부분을 "켜" 주시면
(Turn it on  체크 박스)

0:15:41.960,0:15:44.940
Data 메뉴에 Solver 라는 것이 생기게 됩니다.

0:15:45.280,0:15:48.100
Solver 를 실행 해 보면,

0:15:48.100,0:15:51.900
목적 함수로 사용할 것을 입력하게 되어 있는데

0:15:51.900,0:15:54.580
목적함수의 공식이 있는 셀을 선택 해 줄 수 있습니다.

0:15:54.580,0:15:57.480
우리는 RMSE 공식이 들어있는 셀을 선택 하면 됩니다.

0:15:57.580,0:16:02.600
그 다음엔, 어떤 값들을
바꾸고 싶은지 알려줘야 하는데

0:16:02.600,0:16:06.360
왼쪽과 윗쪽의 행렬을 선택 해 주면 됩니다.

0:16:06.360,0:16:08.620
그러면, 경사 하강을 수행할 때

0:16:08.625,0:16:11.375
이 두 행렬의 값들이 바뀌어지게 됩니다.

0:16:11.375,0:16:14.455
그리고, Min (최소화)를 선택 해 줬는데,

0:16:14.455,0:16:17.625
목적함수 결과를 최소화 하라는 의미를 가집니다.

0:16:17.625,0:16:21.095
또한, GRG Nonlinear 라는 것은 
경사하강 기법의 한 종류를 의미 합니다.

0:16:21.600,0:16:25.320
현재의 결과가 2.81 이라는 것을
머릿속에 기억해 보신 후,

0:16:25.320,0:16:29.055
Solve 버튼을 누르면, 아랫쪽 상태바에 표시된 
숫자가 작아지는 것을 볼 수 있습니다.

0:16:29.060,0:16:33.460
실제로 무슨 일이 일어나는지를 보여주진 않지만,
숫자가 작아진다는 것을 알 수는 있습니다.

0:16:35.940,0:16:39.775
행렬 곱셈도 하고, 경사 하강도 하기 때문에

0:16:39.780,0:16:43.000
일종의 뉴럴넷 스러운 느낌이 들게 해 주긴 하지만,

0:16:43.060,0:16:46.220
실제론 비선형 계층도 없고,

0:16:46.220,0:16:49.395
비선형 다음에 있어야 할 선형 계층도 없습니다.

0:16:49.400,0:16:52.100
그러니까 딥러닝이라고 볼 수 없는 것입니다.

0:16:52.100,0:16:55.415
사람들은 이렇게 
행렬 곱셈, 경사하강 등이 수행되지만

0:16:55.415,0:16:58.305
그 계층이 깊지 않은 경우에 대해서

0:16:58.305,0:17:01.465
얕은(shallow) 학습 이라고 부르곤 합니다.

0:17:01.465,0:17:04.865
따라서, 저희가 한 것도 
얕은 학습 이라고 볼 수 있을겁니다.

0:17:04.865,0:17:07.000
기다리기 지쳐서 일단은

0:17:07.000,0:17:11.860
경사하강 프로세스를 멈추기 위해서 
ESC 키를 누르겠습니다.

0:17:11.960,0:17:16.940
결과를 보시면, 목적함수의 값이 
0.39 까지 작아진 것을 알 수 있습니다.

0:17:17.720,0:17:22.840
예를 들어서, 
userid 72에 의해서 평가된 movieid 27은

0:17:22.840,0:17:27.680
4.44 평점을 예측값으로 가지게 되는데,

0:17:28.900,0:17:32.340
실제 레이블된 값(정답)은 
4라는것과 비교 해 볼 수 있을 겁니다.

0:17:32.340,0:17:36.460
꽤 의미있는 작업을 수행했다고
볼 수 있는 결과인 것입니다.

0:17:37.060,0:17:41.240
어째서 의미있는 작업일까요?

0:17:41.240,0:17:48.020
여기엔 225개의 예측하고자 하는 숫자들이 있는데

0:17:48.700,0:17:51.735
예측에 사용되는 숫자들 (윗쪽/왼쪽) 은

0:17:51.740,0:17:55.240
각각 75개로, 총 150개가 있습니다.

0:17:55.680,0:18:02.020
이 두가지의 숫자가 일치하지 않기 때문에,
일종의 머신러닝을 수행되어야 했습니다..

0:18:02.740,0:18:10.320
기본적으로는, 이렇게 예측을 
할 수 있게 해주는 어떤 방식이 있다는 이야기가 되는데

0:18:10.525,0:18:13.545
선형대수를 공부하신적이 있다면,

0:18:13.545,0:18:16.205
사실상 "행렬분해" 라는 것으로 볼 수 있을 겁니다.
(matrix decomposition)

0:18:16.205,0:18:19.365
일반적으로 선형대수에선, 이런 문제를

0:18:19.365,0:18:23.020
Analytical 기법같이 
이러한 목적에 특별히 디자인된 기법을 사용해서

0:18:23.020,0:18:24.800
해결하려고 하곤 합니다.

0:18:25.195,0:18:28.305
하지만, 다행인 것은
경사하강법의 사용이

0:18:28.305,0:18:31.455
이 경우를 포함한 거의 모든 문제를 
해결 할 수 있게 해 준다는 것입니다.

0:18:31.640,0:18:34.480
저는 개인적으로 선형대수적인 관점으로 
생각하기를 좋아하지 않습니다.

0:18:34.480,0:18:37.205
대신 직관적으로 생각해 보는것을 좋아하는데,

0:18:37.205,0:18:39.800
그렇게 한번 설명해 보겠습니다.

0:18:40.240,0:18:45.420
movieid 27이 
반지의제왕:파트1 이라고 가정해 봅시다.

0:18:45.720,0:18:48.480
그리고

0:18:48.480,0:18:54.600
이 영화에대한 userid 72의 평점을 예측해서

0:18:54.600,0:18:58.120
이 사용자가 반지의제왕을 좋아할지 
알아내 본다고 가정해 봅시다.

0:18:58.120,0:19:00.620
개념적으로 보면

0:19:00.620,0:19:06.860
이 특정 영화에 대해서,
다섯개의 숫자가 사용된다는 것을 알 수 있습니다.

0:19:06.880,0:19:11.700
맨 아래 숫자는 영화가 얼마나 
공상과학과 판타지 요소를 포함하는지를 나타내고

0:19:12.120,0:19:17.860
그 위의 숫자는 얼마나 최근의 영화이며 
얼마나 많은 특수효과가 사용됐는지를 나타내고

0:19:17.860,0:19:22.160
맨 위의 숫자는 얼마나 대화 주도적인 
영화인지를 나타내는 숫자라고 생각해 볼 수 있습니다.

0:19:22.380,0:19:24.920
그러니까 이 다섯개의 숫자가

0:19:24.920,0:19:28.540
영화의 어떤 특징들을 표현한다고 
생각해 볼 수 있다는 것입니다.

0:19:28.540,0:19:30.060
그렇다면,

0:19:30.060,0:19:33.575
사용자에 대해서도, 똑같이 
다섯 개의 숫자를 부여할 수 있을 것입니다.

0:19:33.580,0:19:36.640
예를 들어서, 얼마나 
공상과학 판타지 영화를 좋아하는지,

0:19:36.720,0:19:42.580
얼마나 현대적인 컴퓨터그래픽이 
사용된 영화를 좋아하는지,

0:19:42.580,0:19:47.435
얼마나 대화 주도적인 영화를 
좋아하는지 와 같은 것들 말입니다.

0:19:47.440,0:19:51.260
그러면, 이 두 다섯개의 숫자들을 가지고
벡터의 외적을 수행함으로써

0:19:51.260,0:19:55.860
좋은 모델을 얻을 수 있길 기대해 볼 수 있을 겁니다.
(합리적인 평점)

0:19:55.940,0:19:59.680
단, 문제는 모든 사용자들 각각에 대해서 
이런 정보를 포함한 데이터가 없고,

0:19:59.680,0:20:03.500
모든 영화 각각에 대해서도 마찬가지라는 것입니다.

0:20:03.940,0:20:07.540
그래서, 이러한 생각이 
합리적일 것이라고 가정한 다음

0:20:07.540,0:20:13.220
확률적 경사하강법이 이 숫자들을 찾아내도록 해주자는 것입니다.

0:20:13.395,0:20:15.825
다른말로 표현해 보면,

0:20:15.825,0:20:18.505
이 두 행렬처럼 곱해질 수 있는 것들을

0:20:18.505,0:20:21.655
factor 라고 부르는데요,

0:20:21.660,0:20:26.080
이들은 서로 곱해져서 
이런 예측 결과 행렬을 만들어냅니다.

0:20:26.080,0:20:29.975
특히, 이 factor는 
latent(잠재적인) factor 라고 불립니다.

0:20:29.980,0:20:32.780
왜냐하면 이 factor들은 우리가 직접

0:20:32.780,0:20:37.880
어떤 의미를 부여하거나,
값을 수동으로 입력한 것이 아닙니다.

0:20:37.900,0:20:40.560
우리가 영화 평점을

0:20:40.560,0:20:43.260
이런식으로..

0:20:43.260,0:20:48.440
영화에 대한 몇 특징들과 
사람들이 영화에 대해서 좋아 할만한 몇 특징들

0:20:48.440,0:20:53.300
사이의 닷( . ) 연산/행렬 곱셈을 통해서
생각할 수 있다고 가정만 한 채 수행한 것입니다.

0:20:53.300,0:20:57.220
그리고나선, 경사하강기법에게

0:20:57.220,0:21:01.180
“잘 동작하게 만들어 줄 어떤 숫자를 찾아주세요” 
라고 말한 것입니다.

0:21:01.440,0:21:06.080
지금까지 설명드린 것이 
기본적으로 사용된 기법이고,

0:21:06.080,0:21:09.900
그 모든 내용이 
이 화면 속에 담겨 있다고 보시면 됩니다.

0:21:09.900,0:21:13.120
즉, 이것이 확률적 행렬 분해라고 
부르는 방법을 사용한

0:21:13.120,0:21:15.560
협업 필터링 이라는 것입니다.

0:21:15.560,0:21:19.720
그리고, 보시다시피 이 모든 것은 
엑셀에서 할 수 있을만큼 쉬운 것이고

0:21:19.780,0:21:24.200
실질적인 모든 내용은 이 하나의 행렬곱셈과

0:21:24.200,0:21:28.400
각 행렬들의 랜덤한 초기화 
라는 것에 담겨져 있다고 볼 수 있습니다

0:21:29.780,0:21:36.200
>> 예측 결과를 0~5 사이의 값으로 
잘라내는 것이 더 좋을까요?

0:21:36.200,0:21:39.420
네. 그렇습니다. 
잠시 후에 그렇게 적용할 것입니다.

0:21:39.420,0:21:43.280
이 결과를 개선하기 위한 수 많은 방법들이 있습니다.

0:21:43.360,0:21:46.220
지금 보여드린 것은 가능한한 
간단하게 만든 시작지점 이라고 보시면 됩니다.

0:21:46.220,0:21:48.240
지금부터 하게 될 것은

0:21:48.240,0:21:52.220
이 내용을 Python을 이용해서 구현해 보고

0:21:52.220,0:21:55.215
전체 데이터셋에 대해서 코드를 돌려보는 것입니다.

0:21:55.220,0:21:58.020
>> 또 다른 질문인데요,

0:21:58.020,0:22:01.660
>> 어떻게, 얼마나 이 행렬들이 클지를 정할 수 있는거죠?

0:22:01.665,0:22:04.065
>> 열의 크기가 5인 것 말이에요

0:22:05.260,0:22:07.200
생각 해 봅시다.

0:22:07.240,0:22:10.700
만약, movieid 가 49라고 할 때,

0:22:10.700,0:22:14.020
이 영화에 대한 평점을 보게 될 것입니다.

0:22:14.740,0:22:19.220
생각 해 보면, 
이 행렬은 사실상 임베딩 행렬입니다.

0:22:19.220,0:22:25.240
따라서, 
이 길이는 임베딩 행렬의 크기라는 것입니다.

0:22:25.240,0:22:27.620
어떤 비유적 표현을 하는것이 아니라,

0:22:27.620,0:22:30.680
말 그대로 임베딩 행렬, 그 자체라는 것입니다.

0:22:30.680,0:22:34.360
영화 72에 대해서, 72번째 위치의 값이 1인

0:22:34.360,0:22:37.860
원-핫 인코딩된 행렬을 색인하여

0:22:37.860,0:22:42.500
이 다섯개의 숫자를 찾아내서 사용 하는 것입니다.

0:22:42.500,0:22:45.980
그러니까, 질문은 어떻게 임베딩 벡터의 차원을

0:22:45.980,0:22:49.140
정할 수 있느냐는 말이 되는데,

0:22:49.140,0:22:52.340
이 질문에 대한 대답은 “알 수가 없다” 입니다.

0:22:52.340,0:22:55.880
몇 다른 크기들을 시도해 보고, 어떤 것이 
더 잘 동작 하는지를 찾아내야 할 것입니다.

0:22:55.880,0:22:58.700
근본적인 개념으로 볼때

0:22:58.780,0:23:03.660
진정으로 특정 문제를 반영하기 
충분할 만큼의 복잡도를 가진

0:23:03.700,0:23:08.700
임베딩 벡터의 차원이 
선택될 필요가 있다는 것입니다.

0:23:08.700,0:23:12.840
하지만 너무 크면, 
너무 많은 파라메터가 생기므로

0:23:12.845,0:23:15.835
학습하는데 무한의 시간이 걸리거나,

0:23:15.835,0:23:18.765
정규화 기법을 사용함에도 불구하고 
과적합이 발생 할 지도 모릅니다.

0:23:20.420,0:23:25.420
>> factor가 음수인 경우가 의미하는게 뭔가요?

0:23:26.020,0:23:28.720
영화에서 factor가 음수인 경우는

0:23:28.725,0:23:31.715
예를 들어서, 대화 주도적인 영화가 아니라는 
의미가 될 수 있습니다.

0:23:31.720,0:23:35.260
정 반대의 의미로 대화가 형편 없다는 것입니다.

0:23:35.600,0:23:38.660
사용자에 대한 음수값은

0:23:38.660,0:23:41.840
현대의 컴퓨터 그래픽 영화를 
좋아하지 않음을 의미할 수 있는 것입니다.

0:23:41.840,0:23:45.135
>> 그러니까, 이 값의 범위는 0보다 큰게 아니라는 거군요.

0:23:45.140,0:23:48.980
>> 이 값이 어떤 점수의 범위 인가요?

0:23:48.980,0:23:50.445
아닙니다.

0:23:50.445,0:23:53.535
숫자 범위엔 어떤 제한사항도 없습니다.

0:23:53.535,0:23:55.845
단지 표준적인 임베딩 행렬들 입니다.

0:23:57.755,0:24:00.155
>> 질문 두 개가 있는데요,

0:24:00.155,0:24:02.995
>> 첫 번째는,

0:24:03.000,0:24:06.820
>> 이 임베딩을 신뢰할 수 있는 이유가 뭔가요?

0:24:06.820,0:24:12.540
>> (잘 못알아 먹겠음) - 뒷받침 예를 설명 중

0:24:12.540,0:24:17.255
말씀하시는게 이 숫자들의 순서를 
변경할 수 있냐는 건가요?

0:24:17.260,0:24:21.860
>> 숫자값 자체가 달리지진 않고, 순서만요.

0:24:22.040,0:24:26.520
최고의 예측값을 구하기 위해서 
경사 하강법을 사용됩니다.

0:24:26.520,0:24:32.380
즉, 가장 좋은 최솟값(손실)을 
찾아내게 되는 순간을 발견하게 되는 것입니다.

0:24:32.660,0:24:35.625
물론, 다른 숫자들이 될 수도 있지만,

0:24:35.625,0:24:38.880
현재 발견된 최솟값만큼 
좋은 것은 아닐 수도 있습니다..

0:24:38.880,0:24:41.715
또한, 그 결과를 
검증 데이터셋에 대해서도 확인 해 봐야 합니다.

0:24:41.715,0:24:44.585
Python 버전에서 이를 해 볼 것입니다.

0:24:44.585,0:24:47.715
>> 두 번째 질문은
>> 새로운 영화나, 새로운 사용자 데이터가 생겼을 때,

0:24:47.720,0:24:50.060
>> 모델을 다시 재학습 시켜야 하나요?

0:24:50.060,0:24:53.860
아주 좋은 질문입니다.
하지만, 이 질문에 간결한 대답을 하긴 어렵습니다.

0:24:53.860,0:24:56.940
시간이 허락된다면, 
나중에 다시 돌아와서 대답 해 드리도록 하겠습니다.

0:24:56.940,0:24:58.920
간단하게 설명 드리면,

0:24:58.925,0:25:01.765
초기에 사용될

0:25:01.765,0:25:05.140
새로운 사용자에 대한 모델이나 
새로운 영화에 대한 모델이 필요하며

0:25:05.180,0:25:10.240
시간이 지나면서 재학습도 시켜줘야 할 것입니다.

0:25:10.500,0:25:13.860
아직도 그러는지 모르겠지만,
Netflix 에선 그런 경우가 있었습니다.

0:25:13.865,0:25:15.540
사용자가 Netflix 를 처음으로 사용할 때,

0:25:15.540,0:25:18.100
어떤 영화를 좋아하는지 사용자에게 물어봅니다.

0:25:18.120,0:25:21.200
그러면, 사용자가 몇 가지 좋아하는 영화를 선택해야 했고

0:25:21.205,0:25:24.375
그 것을 기반으로 모델을 학습 시켰습니다.

0:25:28.320,0:25:33.260
>> K Nearest Neighbors 알고리즘도 사용 가능할까요?

0:25:33.260,0:25:37.840
네, KNN 알고리즘도 물론 사용 가능합니다.

0:25:38.720,0:25:41.240
하지만, 최소한 이 경우에선

0:25:41.240,0:25:46.460
영화를 표현하기 위한 컬럼들이 없을 겁니다.습니다. 
(임베딩을 의미)

0:25:46.660,0:25:52.120
즉, 영화 장르, 출시일, 출연 배우등 같은 
정보가 존재하는 경우에만

0:25:52.120,0:25:56.435
일종의 협업필터링이아닌 
모델을 만들 수 있을 것입니다.

0:25:56.435,0:25:59.345
새로운 영화 모델에 대해서 
제가 의미했던 것과 일맥상통 하는데,

0:25:59.440,0:26:02.440
어떤 종류의 예측변수들이 
있어야만 한다는 것입니다.

0:26:07.340,0:26:11.000
지금부터 보실 내용은 
꽤나 친숙한 내용일 것입니다.

0:26:11.000,0:26:13.840
이번에도, 제가 진행할 방식은 
top down 접근 방식입니다.

0:26:14.180,0:26:18.020
시작엔, PyTorch와 fastai의 
기능들을 사용하게 될 것이고

0:26:18.020,0:26:22.520
똑같은 결과를 얻기 위해서, 다른 방법들을 사용하는 
과정을 몇 번 반복하게 됩니다.

0:26:22.520,0:26:25.860
그러면서 매번 
점점 더 깊이 있는 접근을 해 보게 될것입니다.

0:26:27.425,0:26:30.455
우선은 내용의 깊이와 상관 없이
검증 데이터셋이 필요할 것입니다.

0:26:30.460,0:26:34.360
이를 위해선, 
일종의 표준적인 get_cv_idxs 방법을 사용해서

0:26:34.360,0:26:38.220
랜덤하게 선택된 ID의 집합을 만들어 낼 수 있습니다.

0:26:38.900,0:26:43.360
wd 는 weight decay(가중치 붕괴)라는 것으로,
코스의 후반부에 이야기 될 것입니다.

0:26:43.360,0:26:45.565
머신러닝을 이미 해보신 분이라면,

0:26:45.565,0:26:48.800
기본적으로 L2 정규화라고 보시면 됩니다.

0:26:49.280,0:26:50.980
n_factors는

0:26:50.980,0:26:54.600
임베딩 행렬의 크기를 정하기 위한 변수 입니다.

0:26:54.900,0:26:59.260
친숙한 API로, “모델 데이터” 객체를 생성할 수 있는데,

0:26:59.260,0:27:08.400
from_csv() 함수에, 위에서 본 
평점 데이터의 CSV 파일을 넘겨주면 됩니다.

0:27:09.760,0:27:14.020
그 이후 한동안의 코드와 과정은
꽤 친숙해 보이는 경향이 있을 것입니다.

0:27:16.320,0:27:19.140
from_csv() 함수 두 번째 파라메터에는

0:27:19.140,0:27:22.280
크로스 테이블의 열이 될 컬럼명이,

0:27:22.280,0:27:24.820
세 번째에는 행이 될 컬럼명이,

0:27:24.820,0:27:28.755
네 번째에는 이것들에 의해 정해질 
값의 컬럼명이 뭔지 명시 됩니다.

0:27:28.760,0:27:32.220
협업 필터링을 사용한 어떤 종류의 추천 시스템이든

0:27:32.220,0:27:36.760
기본적으로 사용자(+변수)와 
아이템(평점) 같은 개념이 존재합니다.

0:27:36.760,0:27:40.380
하지만 반드시 사용자와 아이템이어야 하는 것이 아닙니다.

0:27:40.420,0:27:44.120
예를 들어서 에콰도르 식료품의 경연에서는
가게와 아이템이 있을 것이고

0:27:44.120,0:27:51.040
어떤 가게에서 얼마나 많은 아이템들을 특정시간에
판매할지(판매량) 예측하였습니다.

0:27:51.600,0:27:54.345
일반적으로 말해보면,

0:27:54.345,0:27:58.320
몇 개의 높은 cardinality의 범주형 변수가 있으며

0:27:58.320,0:28:01.160
이것으로 측정 하고자하는 
뭔가가 있다는 것입니다.

0:28:01.160,0:28:05.100
그리곤 이것들을 개념화 하고, 행렬 곱셈을 수행해서

0:28:05.100,0:28:09.420
평점같은 어떤 값을 예측할 수 있게 되는 것입니다.

0:28:10.300,0:28:15.880
흥미롭게도 이 내용은 직전 질문과도 관련이 있습니다.

0:28:16.140,0:28:19.740
이것을 설명하기위한 동일한 방법이 있는데,

0:28:19.740,0:28:27.640
사용자 72가 영화 27을 
좋아할지에 대해서 예측 한다는 것은

0:28:27.700,0:28:36.540
사용자 72와 비슷한 다른 사용자들이 
좋아한 영화는 무엇인지,

0:28:36.620,0:28:44.380
어떤 다른 영화가 사용자 72와 비슷한 다른 사용자들에 의해 
좋아해 질것인가로 생각해볼 수 있습니다.

0:28:44.400,0:28:50.560
똑같은 것을 두 가지 다른 방식으로 설명한 것입니다.

0:28:50.680,0:28:55.500
협업 필터링이 하는일을 개념적으로 설명 해 보자면

0:28:55.500,0:28:58.820
어떤 영화와 어떤 사용자에 대한 결과를 가지고

0:28:58.820,0:29:02.260
다른 어떤 영화가 비슷할 것인가 
라는 것을 판단하는 것입니다.

0:29:02.260,0:29:05.660
비슷한 사람들이 비슷한 영화를 좋아할 것이라는 것을

0:29:05.660,0:29:08.080
어떤 영화를 좋아한 특정 사용자에 기반해서

0:29:08.080,0:29:11.340
이와 비슷한 사람들이 
동일한 종류의 영화를 좋아할지를 알아내는 것입니다.

0:29:11.460,0:29:14.120
설명드린 것은 일종의 근본적인 구조입니다.

0:29:14.120,0:29:16.860
따라서, 이런 근본적인 구조가 내재된 문제에선

0:29:16.860,0:29:19.700
협업 필터링을 이용한 접근법이 유용할 가능성이 있는 것입니다.

0:29:19.880,0:29:24.820
어쨌든, 인수 분해(factoring) 할 두 부분과

0:29:24.820,0:29:30.060
의존(예측 대상) 변수가 정해져야 하는 것입니다.

0:29:30.240,0:29:33.580
다음으론, 평소처럼 
“모델 데이터” 객체의 get_learner(..) 메소드로

0:29:33.585,0:29:35.745
모델/learner 를 생성할 수 있습니다.

0:29:35.745,0:29:39.075
이때, 임베딩 행렬의 크기 정보와

0:29:39.080,0:29:42.600
검증 데이터셋에 사용되는 인덱스 리스트와

0:29:42.600,0:29:44.220
배치 크기와

0:29:44.220,0:29:47.600
어떤 Optimizer를 사용할지가 지정되어야 합니다.

0:29:47.600,0:29:52.000
Optimizer 에 대해서는 잠시 후 다룰 것인데,

0:29:52.040,0:29:55.620
일단은 Adam Optimizer를 사용할 것입니다.

0:29:56.340,0:29:59.720
다음으론 fit(..)을 수행할 수 있습니다.

0:29:59.720,0:30:03.080
모양새가 매우 유사해 보입니다.

0:30:03.080,0:30:06.300
흥미롭게도, 이런 종류의 모델은 아주 빠르게 학습되어서

0:30:06.300,0:30:09.440
단 3번의 에포크만 수행하면 충분했었습니다.

0:30:09.600,0:30:12.620
물론 평소처럼 학습률 발견자등

0:30:12.620,0:30:15.100
배워온 친숙한 방법들을 사용하셔도 됩니다.

0:30:15.520,0:30:19.980
학습에는 약 2초 정도로 매우 빨랐지만

0:30:19.980,0:30:22.960
미리 학습 같은것은 사용되지 않았고

0:30:22.960,0:30:25.580
모든 것이 랜덤한 상태로 시작된 것입니다.

0:30:26.245,0:30:29.215
이것은 검증 데이터셋에 대한

0:30:29.280,0:30:31.680
RMSE(Root Mean Squared Error)가 아닌

0:30:31.680,0:30:35.920
MSE(Mean Squared Error) 값이기 때문에,
이 값에 루트를 씌워 볼 수 있을겁니다.

0:30:36.160,0:30:40.800
그러면 0,776에 대한 결과는 0.88 이 됩니다.

0:30:40.800,0:30:44.480
이 결과를 이 데이터셋에 대한 
벤치마크와 비교해 볼 수 있습니다.

0:30:44.480,0:30:50.140
아래쪽으로 내려서, 
가장 뛰어난 벤치마크 결과를 찾아보니

0:30:50.140,0:30:53.535
그 값은 0.91 이었습니다.

0:30:53.540,0:31:02.780
즉, 우리가 2초만에 구한 결과가 
이미 더 좋은 것이됩니다.

0:31:03.400,0:31:06.280
지금까지 fastai 라이브러리를 사용해서

0:31:06.280,0:31:11.720
딱히 머리아픈 생각 없이,
어떻게 협업필터링을 할 수 있는지를 보셨습니다.

0:31:11.720,0:31:13.575
지금부터는

0:31:13.575,0:31:16.285
0.776 과 비슷한 결과를 낼 수 있는 코드를

0:31:16.285,0:31:19.005
처음부터 다시 만들어 보는 시간을 가지며

0:31:19.005,0:31:21.735
좀 더 깊이 있게 다뤄보려고 합니다.

0:31:22.485,0:31:25.475
하지만, 깊은 부분을 신경쓰지 않은채

0:31:25.475,0:31:28.595
집에서 이 방법을 사용해 보고 싶으시면

0:31:28.600,0:31:31.780
방금 보신 3줄 코드만 있으시면 충분할 것입니다.

0:31:32.460,0:31:35.600
모델의 predict() 메소드로, 
예측값들을 얻을 수 있을 겁니다.

0:31:35.605,0:31:38.745
그러면, 그 결과를 그래프로 표현해 볼 수 있겠죠.

0:31:38.745,0:31:41.600
sns 는 seabourn 이라는 라이브러리로,

0:31:41.600,0:31:46.180
matplotlib 라이브러리를 아주 잘 활용해서 
만들어진 것입니다.

0:31:46.180,0:31:50.120
따라서, matplitlib 에 대해서 공부하시면, 
그 내용은 seabourn 에서도 도움이 될 것입니다.

0:31:50.120,0:31:53.500
seabourn에는 여기서 사용된 jointplot 같이 
몇가지 멋진 그래프들이 있습니다.

0:31:53.500,0:31:59.340
이 그래프는 예측값(하단)과 실제값(좌측) 간의
관계에 대한 것으로

0:31:59.340,0:32:02.840
그 결과의 형태를 보시면,

0:32:02.840,0:32:06.635
높은 숫자가 예측될 때, 
실제값도 높다는 것을 알 수 있습니다.

0:32:06.640,0:32:12.160
또한, 예측 결과들과 실제값들의 분포가
히스토그램으로도 표현 되고 있습니다.

0:32:12.160,0:32:16.380
약간 흥미로운 시각화의 한 예로써 
보여드리기 위해 그려본 것입니다.

0:32:17.860,0:32:23.735
>> factor 숫자를 왜 50으로 설정해 줬는지
설명해 주실 수 있나요?

0:32:23.740,0:32:27.640
몇 가지 다른 숫자를 시도해본 후, 
50이라고 결정 한 것입니다.

0:32:27.640,0:32:29.715
>> 그게 무슨 말인가요?

0:32:29.715,0:32:31.625
임베딩 행렬의 차원을 의미합니다.

0:32:32.055,0:32:35.345
이렇게 생각 해 보세요.

0:32:35.345,0:32:39.700
엑셀에서 사용한 5개 대신, 50개가 있다는 것입니다.

0:32:42.140,0:32:45.120
>> 질문이 있습니다.

0:32:45.120,0:32:50.300
>> 만드신 추천 시스템이 좀 더 암시적인 것으로

0:32:50.300,0:32:56.320
>> 실제 숫자 대신에 0과 1로만 표현된다면 어떤가요?

0:32:56.500,0:33:01.600
그렇다면, 회귀 알고리즘 보다는 
분류 알고리즘을 사용해야 할 것입니다.

0:33:03.920,0:33:10.840
>> (못 알아듣겠슴)

0:33:11.780,0:33:14.915
말씀하신 부분을 이 수업에서 다룰지 모르겠습니다.

0:33:14.920,0:33:17.720
하지만, 제가 말씀 드릴 수 있는 것은

0:33:17.720,0:33:20.120
회귀 대신 분류를 하라는 것입니다.

0:33:20.260,0:33:23.040
아직 그렇게 할 수 있는 기능을 
라이브러리에 포함하진 않았지만,

0:33:23.040,0:33:26.065
원하시면 이번주 누군가가 
이 기능을 구현해 주셔도 좋을것 같습니다.

0:33:26.065,0:33:28.240
아주 짧은 코드로 가능할 것 같은데,

0:33:28.260,0:33:31.920
기본적으론 단지 활성함수를 sigmoid로 바꿔주고

0:33:31.920,0:33:37.240
손실 함수를 RMSE를 cross entropy로 바꿔줘서

0:33:37.240,0:33:46.560
회귀자 대신 분류자로 만들어 주면 될 것입니다.

0:33:46.580,0:33:49.195
이번주 누군가가 일종의 도전과제로 받아들여서

0:33:49.200,0:33:52.820
다음주에 잘 동작하는 코드가 완성되어 있길 
희망 해 보겠습니다.

0:33:55.660,0:34:01.360
제가 아까 닷( . ) 연산을 한다고 했었습니다.

0:34:01.360,0:34:06.780
닷( . ) 연산이라는 것은 
행렬 곱셉의 벡터 버전쯤으로 이해하시면 됩니다.

0:34:06.780,0:34:13.200
따라서, 왼쪽 행렬의 각각(벡터)과 
윗 행렬의 각각(벡터)을 곱해준 후 더해주는 것입니다

0:34:13.220,0:34:17.920
이 연산을 PyTorch에서 
어떻게 할 수 있는지 한번 알아 봅시다.

0:34:17.980,0:34:22.180
우선, T(…) 를 사용하면 
PyTorch용 텐서를 만들 수 있습니다.

0:34:22.180,0:34:25.580
사실 이 문법은 fastai의 것으로, 
순수 PyTorch 버전은

0:34:25.580,0:34:29.880
torch.from_numpy(…) 같은
형태인 것으로 기억되는군요.

0:34:29.880,0:34:33.500
텐서를 만들 때,
중첩된 리스트를 넘기는 것 또한 가능합니다.

0:34:33.500,0:34:37.120
[[1, 2], [3, 4]] 형태의 2차원 PyTorch 텐서와

0:34:37.120,0:34:40.860
[[2, 2], [10, 10]] 형태의 2차원 PyTorch 텐서를 만들었고

0:34:40.860,0:34:45.440
각각을 출력 해 보았습니다.

0:34:45.860,0:34:48.000
.cuda 관련 명령을 수행하지 않았는데,

0:34:48.000,0:34:51.940
그 말은 GPU가 아니라 
CPU 위에서 돌아간다는 의미가 됩니다.

0:34:51.940,0:34:55.060
이 두 텐서를 곱해줄 수 있습니다.

0:34:55.300,0:35:00.760
numpy 또는 PyTorch에서, 
텐서간의 수학적 연산은

0:35:00.760,0:35:05.080
엘리먼트와이즈하게 동작합니다. 
(각 요소마다 서로 => 행렬의 곱셈같은 법칙이 아닌)

0:35:05.085,0:35:07.855
그렇기 때문에 두 텐서의 차원이 동일해야 합니다.

0:35:07.860,0:35:10.780
여기선 두 텐서는 모두 2차원으로 동일합니다.

0:35:10.780,0:35:12.360
즉

0:35:12.360,0:35:15.240
a*b 의 결과는
2*2 = 4,

0:35:15.240,0:35:18.280
3*10 = 30
등등 처럼 되는 것입니다.

0:35:18.880,0:35:29.460
(갑자기 약간 편집됨)

0:35:29.900,0:35:32.860
(a*b).sum(1) 결과 벡터의
첫 번째 요소는

0:35:33.180,0:35:36.355
1*2 = 2 와, 
2*2 = 4 를 얻어오면

0:35:36.355,0:35:39.375
이 두 값의 합은 6이 되는데

0:35:39.380,0:35:44.600
[1, 2]와 [2, 2] 사이의 
닷( . ) 연산과 동일한 결과입니다.

0:35:44.600,0:35:48.395
마찬가지로, 결과 벡터의 두 번째 요소는

0:35:48.395,0:35:51.065
3*10 = 30 와, 
4*10 = 40 를 얻어오면

0:35:51.065,0:35:53.000
각 결과의 합 = 70가 됩니다.

0:35:53.200,0:35:56.235
(a*b).sum(1) 에서의

0:35:56.235,0:35:59.005
1은 “첫 번째 차원에 대하여” 라는 의미를 가집니다.

0:35:59.005,0:36:02.025
따라서, a*b 결과의 컬럼들의 값을 더하는 것입니다.

0:36:02.025,0:36:05.060
즉, (a*b).sum(1) 이 하는 일은

0:36:05.100,0:36:10.340
각 행렬의 각 열들간의 닷( . ) 연산을 하는것으로 
볼 수도 있는 것입니다.

0:36:11.015,0:36:14.285
이해가 되시나요?

0:36:14.895,0:36:17.445
물론, 행렬 곱셈 같은 방식으로
해볼 수도 있겠지만

0:36:17.445,0:36:22.900
저는 일단 가능한한 간단하게(적은 형태로)
시도하려고 노력해 봤습니다.

0:36:22.940,0:36:25.060
그러니까, 지금부터는

0:36:25.060,0:36:29.300
(a*b).sum(1) 코드를
닷( . ) 연산으로써 사용하게 것입니다.

0:36:29.700,0:36:31.600
기억하시나요?

0:36:31.600,0:36:35.640
기존 데이터는 
크로스 테이블 형태가 아니었습니다.

0:36:35.725,0:36:38.905
그래서 엑셀에선 이를 
크로스 테이블 형태로 만들어 줬었습니다.

0:36:38.905,0:36:44.280
하지만 기존 원본 데이터는 
그렇지 않았던 것입니다.

0:36:44.420,0:36:47.120
그래서, 개념적으로 보면

0:36:47.120,0:36:50.615
userid로 해당 사용자에 대한 50개의 factors를
임베딩 행렬에서 찾게 되는 것입니다.

0:36:50.615,0:36:53.615
마찬가지로, movieid로 해당 영화에 대한 
50개의 factors를 임베딩 행렬로부터 찾게 됩니다.

0:36:53.620,0:36:56.700
그리고나선, 찾아진 두 개의 50개 factors(벡터) 간에

0:36:56.700,0:36:59.900
닷( . ) 연산을 수행하게 됩니다.

0:37:00.540,0:37:03.340
이것을 직접 구현 해 봅시다.

0:37:03.420,0:37:08.640
그러기 위해선, 
진짜 뉴럴넷은 아니지만

0:37:08.645,0:37:11.765
우리만의 한개의 일종의 뉴럴넷 계층을 
만들어야 합니다.

0:37:11.765,0:37:15.840
좀 더 포괄적인 용어로 표현해 보면,

0:37:15.840,0:37:19.220
PyTorch 모듈을 만들게 된다는 것입니다.

0:37:19.720,0:37:23.680
PyTorch 모듈은 매우 구체적인 것으로,

0:37:23.680,0:37:28.380
여러분만의 PyTorch 모듈을 생성하기만 하면,
이것을 뉴럴넷에 전달 해줄수 있게 되고

0:37:28.385,0:37:31.145
뉴럴넷에서의 계층으로써 사용될 수 있게 됩니다.

0:37:31.520,0:37:36.660
그리고, 여기선 model 이라는 모듈을 만들어 뒀는데,

0:37:36.660,0:37:41.780
모듈은 ( ) 괄호로 뭔가를 전달 받고, 
계산하는 방식으로 동작하는 것을 보실 수 있을겁니다.

0:37:41.980,0:37:45.720
저는 이미 DotProduct 라는 모듈을 정의 해 뒀습니다.

0:37:46.200,0:37:49.600
이것을 보시는것 처럼 인스턴스화(객체화) 하여

0:37:49.740,0:37:53.035
model 이라는 DotProduct 객체를 생성했습니다.

0:37:53.040,0:37:56.760
그러고나면, 
이 객체를 마치 함수처럼 사용할 수 있게 됩니다.

0:37:56.760,0:37:58.360
하지만,

0:37:58.360,0:38:02.415
이것에 대한 계산된 미분도 얻을 수 있으며,

0:38:02.415,0:38:05.805
전체 뉴럴넷을 쌓아올리는데 
일 부분으로써도 사용 되는 등

0:38:05.805,0:38:08.945
모듈이란 단순히 함수만이라고 
볼 수는 없습니다.

0:38:08.945,0:38:10.760
일종의 함수지만

0:38:10.760,0:38:14.280
뉴럴넷을 아주 편리하게 구성할 수 있는
방법도 제공해 주는 것입니다.

0:38:14.320,0:38:17.695
여기 코드를 보시면,
어떻게 모듈을 정의 하는지 알 수 있는데,

0:38:17.695,0:38:20.785
이 모듈을 호출하면 닷( . ) 연산 결과를 리턴합니다.

0:38:20.785,0:38:23.825
모듈을 정의 하기 위해선,
우선 Python 클래스를 만들어야만 합니다.

0:38:23.825,0:38:26.855
아직 Python OO (객체지향)을 해본적이 없다면,

0:38:26.860,0:38:30.360
모든 PyTorch 모듈은 Python OO로 작성되어 있기 때문데,

0:38:30.360,0:38:32.725
그 방법을 배우셔야만 할 것입니다.

0:38:32.725,0:38:35.665
이 부분은 제가 PyTorch를 좋아하는 이유중 하나입니다.

0:38:35.665,0:38:40.760
Tensorflow 같이 뭔가 
완전히 새로운 방법을 제안하지 않고

0:38:40.760,0:38:46.100
Python 스타일의 방법을 
사용하려는 경향이 있다는 것입니다.

0:38:46.100,0:38:48.020
여기의 경우에선,

0:38:48.020,0:38:50.675
새로운 종류의 행동방식을 만들어내기 위해선

0:38:50.680,0:38:54.060
Python 클래스를 만들어준 것과 같이 말입니다.

0:38:54.480,0:38:58.920
>> 메모리에 넣을 수 있을 정도의 작은 데이터가 아닌

0:38:58.920,0:39:01.920
>> 아주 많은 데이터가 있다고 가정 해 봅시다.

0:39:01.920,0:39:06.120
>> 이때도 fastai 를 사용한 
협업필터링 문제를 해결 가능한가요?

0:39:06.500,0:39:09.575
네. 물론입니다.

0:39:09.580,0:39:13.700
한번에 특정 크기의 배치만 처리되는

0:39:13.700,0:39:18.320
미니배치 확률적 경사하강이 사용됩니다.

0:39:25.060,0:39:30.200
여기서 사용된 버전은 
pandas의 DataFrame을 생성하게 되는데

0:39:30.200,0:39:34.120
pandas의 DataFrame은 
메모리에 존재해야만 하는 특성을 가집니다.

0:39:34.120,0:39:36.060
그렇긴 해도,

0:39:36.060,0:39:40.080
AWS에서 512기가의 인스턴스를 
손쉽게 구할 수 있습니다.

0:39:40.080,0:39:46.460
그리고, 512기가 보다 큰 CSV가 
있을 경우는 매우 드물 것입니다.

0:39:46.460,0:39:48.500
하지만, 만약 그런 경우라면

0:39:48.500,0:39:51.960
하나의 큰 배열로 저장 해 둔 다음,

0:39:51.960,0:39:56.820
스트리밍과 같은 약간 다른 방식의 
읽기 기능을 만들어야 할지도 모릅니다.

0:39:56.820,0:40:02.600
또는 비슷한 기능을 제공하는 Dask의 
Dataframe을 이용해 볼 수도 있을 겁니다.

0:40:03.020,0:40:07.380
하지만, 저는 아직까지 실세계의 문제 상황에서

0:40:07.380,0:40:13.220
협업 필터링을 위한 512기가 크기의 행렬이 
사용되는 경우를 본 적이 없습니다.

0:40:14.520,0:40:16.660
다시 돌아가서,

0:40:16.660,0:40:20.020
지금 보시는것은 
PyTorch에만 해당되는 내용으로

0:40:20.020,0:40:23.335
코드에서의, (사용자*영화).sum(1) 와 같이

0:40:23.340,0:40:27.900
실제 수행되어야 할 내용의 정의이 정의되어야 할 때

0:40:28.140,0:40:32.180
이 내용은 forward 라는 이름의 
특별한 메소드안에 정의 되어야만 합니다.

0:40:32.180,0:40:37.900
뉴럴넷에서 activation의 계산이 
이루어 지는 부분이

0:40:37.900,0:40:42.900
forward pass 라고 불린다는 개념에서 
채용된 이름입니다.

0:40:42.900,0:40:47.220
즉, forward 계산을 하는 메소드인 것입니다.

0:40:47.220,0:40:49.740
경사도는 backward 계산 이라고 불리는데,

0:40:49.740,0:40:53.340
PyTorch가 이 부분을 자동으로 계산하기 때문에,
직접 구현할 필요는 없습니다.

0:40:53.340,0:40:56.105
따라서, forward만 구현하면 되는것입니다.

0:40:56.105,0:40:58.985
정리해보면, 새로운 클래스를 만들었고
forward 메소드를 정의 했으며

0:40:58.985,0:41:02.365
이 메소드 안에 필요한 
닷( . ) 연산의 정의를 넣어두었습니다.

0:41:02.495,0:41:05.765
이게 전부입니다.

0:41:05.765,0:41:08.735
이렇게 정의된 클래스를 가지고,

0:41:08.735,0:41:11.765
이 클래스의 인스턴스인 
model을 만들 수 있었습니다.

0:41:11.765,0:41:14.785
그리곤, model을 호출해서
원하는 결과를 얻어낸 것입니다.

0:41:15.100,0:41:19.860
보신내용은 어떻게 사용자정의 
PyTorch 계층을 만드는지를 보여줬으며

0:41:19.860,0:41:24.280
다른 라이브러리와 이 방법을 비교해 보시면

0:41:24.280,0:41:26.895
이 방법이 훨씬 쉽다는걸 알 수 있을 겁니다.

0:41:26.900,0:41:31.300
왜냐하면, Python 에서 제공되는 
기본적인 기능을 십분 활용하기 때문입니다.

0:41:31.860,0:41:37.080
이번에는 좀 더 복잡한 모듈을 만들어 볼 것입니다.

0:41:37.440,0:41:40.560
기본적으론, 똑같은 작업을 하게 될 것입니다.

0:41:40.560,0:41:42.400
forward 메소드가 있고,

0:41:42.400,0:41:45.180
이 메소드는 (사용자*영화).sum(1)를 리턴합니다.

0:41:45.180,0:41:48.520
단지, 이 결과를 리턴하기 전에 
한가지 추가 작업이 있는데

0:41:48.525,0:41:51.815
만들게 될 
두 개의 임베딩 행렬로 부터,

0:41:51.820,0:41:56.680
해당 사용자와 영화에 대한 색인을 하는 것입니다.

0:41:56.680,0:41:59.900
한번 구현해 봅시다.

0:42:00.780,0:42:04.300
우선 처음으로 깨닳을 수 있는 것은

0:42:04.300,0:42:07.900
userid들 또는 movieid들끼리의 값은

0:42:07.900,0:42:10.260
인접하지 않을 수도 있다는 것입니다.

0:42:10.265,0:42:13.165
예를 들어서, 하나가 100만번째인데

0:42:13.165,0:42:16.265
다른 하나는 100만 1000번째일 수 있을겁니다.

0:42:16.380,0:42:22.440
만약 이 ID 들을 
임베딩 행렬을 색인하기 위해 그냥 사용하면,

0:42:22.460,0:42:24.925
만들어질 임베딩 행렬의 크기는

0:42:24.925,0:42:29.180
100만 1000이 되어야 할 것인데,
그렇게 되길 원치는 않을 것입니다.

0:42:29.220,0:42:34.380
그래서, 우선 유니크한 userid의 목록을 구합니다.

0:42:34.680,0:42:38.900
그리곤, 각 userid를

0:42:38.900,0:42:42.460
인접한 정수값들로 매핑시켜줍니다.

0:42:42.460,0:42:45.560
이 코드라인이 의미하는 것은

0:42:45.785,0:42:48.775
모든 유니크한 것들을 유니크한 인덱스로

0:42:48.775,0:42:51.795
매핑하는 딕셔너리를 만든 것인데

0:42:51.795,0:42:54.745
매우매우 유용한 문법으로,

0:42:54.745,0:42:57.785
이번 한 주간 공부해 두시는게 좋을 겁니다.

0:42:57.785,0:43:00.895
모든 종류의 머신러닝에서, 
매우 자주 사용하게 될 것입니다.

0:43:00.900,0:43:04.260
문법의 사용법을 이해하는게 어렵진 않으므로,
제가 여기서 설명을 드리진 않을 겁니다.

0:43:04.260,0:43:06.400
만약 이해가 어렵다면, 포럼에 질문을 남겨 주세요.

0:43:06.400,0:43:12.120
어쨋든, 사용자 id와 인접한 인덱스간의 
매핑 데이터를 만들고 나면

0:43:12.200,0:43:20.300
userid 컬럼의 값을 
이 인접한 인덱스값으로 교체할 수 있게됩니다.

0:43:20.880,0:43:24.760
pandas의 apply 메소드는 
사용자 정의 함수를 인자로 받아들이는데,

0:43:24.760,0:43:29.640
Python의 lambda 문법은 
익명의 함수를 만드는 방법으로

0:43:29.740,0:43:33.055
여기서 이 익명의 함수는 
사용자의 인덱스를 리턴합니다.

0:43:33.055,0:43:35.985
똑같은 것을 영화에 대해서도 수행합니다.

0:43:35.985,0:43:39.820
그러고나면,
전과 동일한 평점 데이터 테이블을 가지지만

0:43:39.820,0:43:44.020
id 값들은 
인접한 인덱스값으로 교체된 버전이 됩니다.

0:43:44.020,0:43:48.120
그러면, 이 인덱스 값으로 임베딩 행렬의 
색인을 할 수 있게 되는 것입니다.

0:43:48.500,0:43:52.720
마지막 두 줄의 코드는 
사용자와 영화의 수를 카운트 하는 것입니다.

0:43:53.040,0:43:56.880
다음으론,
엑셀에서 보여드린 예측 관련 내용을

0:43:56.880,0:44:00.600
Python 으로 만들어 볼 차례입니다.

0:44:02.880,0:44:10.740
앞서 만든 간단한 PyTorch 모듈에서는

0:44:10.940,0:44:14.300
상태(state) 정보 같은 것이 없었습니다.

0:44:14.300,0:44:18.660
사용자의 수, 영화의 수, 
얼마나 많은 factor가 필요한지등 정보가

0:44:18.660,0:44:21.840
필요 없었기 때문에
생성자를 정의할 필요가 없었습니다.

0:44:22.320,0:44:28.300
사용자의 수, 영화의 수를 
이용해서 모듈의 인스턴스를 생성하는

0:44:28.300,0:44:33.840
이 코드와 같은 일을 하고 싶을때면,

0:44:33.920,0:44:38.660
작성하는 클래스에 생성자를 만들어 줘야 합니다.

0:44:38.660,0:44:41.460
Python에선, __init__ 라는

0:44:41.460,0:44:48.140
특별한 이름의 메소드를 정의해서 
생성자를 만들 수 있습니다.

0:44:48.300,0:44:52.720
객체지향을 해 보신적이 없다면,

0:44:52.725,0:44:56.065
이번주에 공부해 보셔야 겠지만,

0:44:56.065,0:45:01.700
생성자란 이 객체를 생성할 때 호출되는 것입니다.

0:45:02.160,0:45:05.315
또한 Python에서 사용되는 특별한 개념으로,

0:45:05.315,0:45:07.655
여러분만의 생성자를 만들때,

0:45:07.655,0:45:10.635
부모 클래스의 생성자를 호출해 줘야 합니다.

0:45:10.635,0:45:13.645
그리고, PyTorch에서 제공하는 
모듈의 멋진 기능(행동)을

0:45:13.645,0:45:16.595
모두 여러분의 클래스에 포함시키고 싶다면,

0:45:16.595,0:45:19.285
nn.Module의 상속을 받으면 가능해집니다.

0:45:19.285,0:45:21.855
nn.Module을 상속 받고,

0:45:21.855,0:45:24.895
이 부모 클래스의 생성자를 호출해 줌으로써,

0:45:24.900,0:45:28.360
완전히 동작하는 
PyTorch 계층(모듈)이 만들어 지는 것입니다.

0:45:28.360,0:45:31.680
그리고나선, 
우리들만의 행동을 모듈에 추가하게 될 텐데,

0:45:31.680,0:45:36.080
이 행동은 클래스에 어떤 내용을 
저장해 줘야만 가능한 경우가 있습니다.

0:45:36.080,0:45:39.775
코드를 보시면, self.u 라는 것을 만들어 줬는데

0:45:39.780,0:45:43.680
임베딩 계층을 저장하는 역할을 합니다.

0:45:43.680,0:45:46.380
n_users는 열의 크기로, 
사용자의 수를 넣어줬고

0:45:46.380,0:45:49.900
n_factors는 행(컬럼)의 크기로, 
factor의 수를 넣어줬습니다.

0:45:51.380,0:45:56.800
이렇게 만들어진 것은, 
정확히 엑셀의 왼쪽 임베딩 행렬을 의미하게 됩니다.

0:45:58.120,0:46:02.220
영화에 대해서도 똑같은 일을 해 줘야 하겠죠.

0:46:02.640,0:46:09.300
결국 이 두 줄의 코드는 두 개의 무작위로 초기화된 
배열(임베딩 행렬)을 생성하게 되는 것입니다.

0:46:10.480,0:46:14.460
그런데, 배열을 무작위로 초기화 할 때는

0:46:14.460,0:46:20.220
그 값들을 합리적인 범위로 
초기화 해주는 것이 중요합니다.

0:46:20.660,0:46:24.980
그 범위를 0에서 100만으로 초기화를 해 주게되면,

0:46:24.980,0:46:30.660
이 값들이 수백/수천만의
큰 값들을 가지게 될 것이고

0:46:30.660,0:46:34.400
경사하강이 동작하기에
매우 어려운 상황이 될 것입니다.

0:46:34.640,0:46:38.580
그래서 저는 어떤 범위의 숫자들이

0:46:38.580,0:46:43.200
올바른 평점을 나타낼 수 있을지를 직접 지정해 주었습니다.

0:46:43.300,0:46:47.275
평점의 범위가 0~5인 것을 알고 있기 때문에,

0:46:47.280,0:46:51.560
임베딩 가중치를 0~0.05 범위로 초기화 해 주면

0:46:51.560,0:46:55.200
적절한 수준의 평점을 
얻을 수 있을 것이라고 판단 했습니다.

0:46:55.200,0:46:59.140
이 값을 손쉽게 계산해 볼 수도 있는데요,

0:46:59.140,0:47:05.920
뉴럴넷에선 이를 위한,
일종의 표준적인 알고리즘들이 있습니다.

0:47:06.140,0:47:09.200
그 중 중요한 한가지는

0:47:09.200,0:47:16.940
He(흐어) 초기화 알고리즘으로

0:47:16.940,0:47:27.860
가중치를 정규분포로 설정 하는 것인데,

0:47:27.920,0:47:40.400
이때의 표준편차는 직전 계층 크기에 반비례 하게 됩니다.

0:47:40.560,0:47:46.160
우리의 경우에선,

0:47:46.220,0:47:53.060
0~0.05라는 범위의 
정규분포에서 숫자를 가져와서

0:47:53.160,0:48:03.040
여기에 50개의 숫자들에 곱해 주면 되는데

0:48:03.040,0:48:06.620
그러면, 얼추 원하는 크기의 
숫자를 얻을 수 있게 됩니다.

0:48:07.000,0:48:12.920
PyTorch는 이미 He 초기화 클래스를 제공하기 때문에,

0:48:12.920,0:48:15.735
보통은 이런 방식의 구현을 생각할 필요는 없습니다.

0:48:15.735,0:48:18.935
단순히 제공되는 초기화 기능을 호출하면 됩니다.

0:48:18.935,0:48:21.955
단지, 우리는 특별한 기능을 사용하지 않고,

0:48:21.960,0:48:25.700
스스로 구현해 보려고 하는 것입니다.

0:48:27.315,0:48:30.485
여길 보시면, 
몇가지 PyTorch의 표기법들이 사용됩니다.

0:48:30.485,0:48:36.740
self.u 는 이미 임베딩 클래스의
인스턴스로 할당해 주었는데,

0:48:36.740,0:48:40.580
여기엔 .weight 이라는 속성이 있습니다.

0:48:40.580,0:48:45.720
이 속성은 실질적인 임베딩 행렬을 포함하는 것으로

0:48:45.720,0:48:48.500
엑셀의 왼쪽 행렬 같은 것은 것입니다.

0:48:49.320,0:48:54.760
실제 임베딩 행렬은 텐서가 아니라, 변수 입니다.

0:48:55.400,0:48:59.380
변수는 텐서와 정확히 동일한 것으로,

0:48:59.380,0:49:03.220
텐서에 적용 가능한
동일한 연산들을 지원합니다.

0:49:03.440,0:49:08.800
단, 추가적으로 자동 미분을 수행하는점 
만이 다릅니다.

0:49:10.040,0:49:14.200
변수에서 텐서를 끄집어내기 위해선,

0:49:14.200,0:49:16.135
data 속성에 접근하면 됩니다.

0:49:16.135,0:49:18.505
즉, self.u.weight.data 는

0:49:18.515,0:49:20.655
self.u 라는 임베딩에 있는

0:49:20.660,0:49:23.280
weight 행렬에 대한 텐서가 됩니다.

0:49:24.160,0:49:27.500
이때 한가지 알아두면 아주 유용한 것이 있는데,

0:49:27.500,0:49:30.740
PyTorch에 있는 모든 텐서관련 함수들은 마지막에

0:49:30.740,0:49:33.080
_ (언더스코어)를 붙여줄 수 있다는 것입니다.

0:49:33.080,0:49:36.105
그러면, 텐서 자체에 대해서 
수행을 하라는 의미를 가지게 됩니다.

0:49:36.105,0:49:39.505
이 코드를 예로 들면, data 텐서에 알맞은 크기의

0:49:39.520,0:49:42.600
균등한 무작위 숫자를 만들어 내는데,

0:49:42.600,0:49:45.795
그 숫자를 리턴하지 말고

0:49:45.795,0:49:48.955
그 행렬 자체를 채워 넣으라는 것을 의미합니다.

0:49:48.955,0:49:51.975
알아두면 매우 유용할 것입니다.

0:49:51.980,0:49:55.720
이 기능을 사용하지 않으면

0:49:55.820,0:50:03.780
(코드를 바꾸는중..)

0:50:03.920,0:50:06.800
이런 식으로 코드가 작성 되어야 할 것입니다.

0:50:06.800,0:50:10.900
따라서, 이 기능을 사용하면
좀 더 간결하게 코드 작성이 되겠죠.

0:50:13.380,0:50:15.140
이런식으로

0:50:15.140,0:50:20.560
무작위로 초기화된 
임베딩 가중치 행렬들이 만들어졌습니다.

0:50:20.560,0:50:22.520
forward 에서는

0:50:22.615,0:50:25.675
Rossman 경연에서 사용한

0:50:25.680,0:50:29.340
동일한 ColumnarModelData를 
사용할 생각입니다.

0:50:29.340,0:50:31.605
그래서, forward 메소드가

0:50:31.605,0:50:35.720
범주형과 계속형 변수 모두를 
전달 받도록 정의 되긴 했습니다만,

0:50:35.720,0:50:38.460
현재 다루는 문제의 경우엔
계속형 변수가 없습니다.

0:50:38.460,0:50:43.480
따라서, 계속형 변수는 무시하고
users 에는 범주형 변수의 0번째 컬럼(행)을,

0:50:43.480,0:50:48.140
movies 에는 범주형 변수의 
1번째 컬럼(행)을 넣어주었습니다.

0:50:48.800,0:50:52.940
계속형 변수를 포함하지 않는 
새로운 클래스를 만들어 볼 수도 있겠지만,

0:50:52.940,0:50:56.080
저는 그냥 ColumnarModelData 를 재사용하기 위해서

0:50:56.080,0:50:59.980
대충 얼기설기 만들어본 것입니다.

0:50:59.980,0:51:01.555
이 코드를 통해서,

0:51:01.560,0:51:05.860
사용자와, 영화에 대한 
미니배치를 가져온 것이라고 볼 수 있습니다.

0:51:05.860,0:51:09.280
기억하시죠? 단일 사용자, 영화 데이터가 아니라

0:51:09.300,0:51:12.220
여러개의 데이터가 포함된
하나의 미니배치를 의미합니다.

0:51:12.220,0:51:14.280
그 다음으로는

0:51:14.280,0:51:17.755
미니배치의 사용자들에 대한 
임베딩 행렬(self.u)을 색인하고,

0:51:17.760,0:51:21.140
미니배치의 영화들에 대한 
임베딩 행렬(self.m)을 색인하였습니다.

0:51:21.140,0:51:26.740
단순히 배열을 색인 하는것과 동일하지만,

0:51:26.740,0:51:30.820
여기선 한번에, 미니배치에 포함된 
다중 아이템에 대해서 수행하는 것입니다.

0:51:30.820,0:51:34.600
PyTorch 는 
미니배치 단위로 동작할 수 있기 때문에,

0:51:34.600,0:51:37.760
많은 대부분의 것들에 대해서 
손쉽게 작업 속도를 올릴 수 있습니다.

0:51:37.760,0:51:42.860
미니배치에 반복문(loop)을 적용하는
코드를 작성할 필요가 없는 것입니다.

0:51:43.720,0:51:46.765
만약 미니배치에
반복문을 수동적으로 사용하신다면

0:51:46.765,0:51:49.625
GPU 가속 기능을 사용할 수 없게 됩니다.
매우 중요한 사실이죠.

0:51:49.625,0:51:53.940
따라서, 직접 반복문을 작성해서 
미니배치에 접근해서는 안될 것입니다.

0:51:53.940,0:51:58.420
이 코드에서 처럼, 한번에 미니배치
전체에 대한 작업을 해 주시는 것이 바람직합니다.

0:51:58.420,0:52:02.240
하지만, PyTorch 에서는 거의 모든 것이
이런 식으로 동작할 수 있기 때문에,

0:52:02.240,0:52:05.320
크게 걱정할 필요는 없습니다.

0:52:05.700,0:52:09.660
마지막으로, 
전과 마찬가지로 닷( . ) 연산을 수행합니다.

0:52:09.740,0:52:14.120
이렇게 EmbeddingDot 을 정의하고 나면,

0:52:14.160,0:52:17.100
이번에는

0:52:17.100,0:52:23.080
x 에 rating과 timestamp를 제외한
모든것을 할당해 줍니다.

0:52:23.100,0:52:26.275
그리고, y 에는 rating이 들어갑니다.

0:52:26.280,0:52:28.680
그러면, 이 x, y 를 가지고

0:52:28.680,0:52:34.520
“모델 데이터” 객체를
from_data_frame 으로 만들 수 있습니다.

0:52:34.600,0:52:39.820
이때, 5번째 파라메터는
사용될 범주형 변수들의 이름 목록입니다.

0:52:41.620,0:52:44.340
“모델 데이터” 객체 생성 후,

0:52:44.340,0:52:50.460
방금 만든 EmbeddingDot PyTorch 모듈의
인스턴스를 생성 해 줘야 합니다.

0:52:50.560,0:52:54.360
그 다음으론, optimizer를 생성해 줘야 하는데,

0:52:54.480,0:52:57.485
이 것도 PyTorch에 포함되는 내용입니다.

0:52:57.485,0:53:00.505
현재까지 사용된 fastai 라이브러리는

0:53:00.505,0:53:02.995
ColmunarModelData… 코드 한줄 뿐입니다.

0:53:03.000,0:53:06.340
제 생각엔 데이터를 어떻게 불러오고 설정하는지가

0:53:06.340,0:53:08.880
여러분들에게 별로 흥미롭지 않을것이라고 생각 했습니다.

0:53:08.885,0:53:11.805
이 코스의 두번째 단계에서 배우게 될 지도 모르지만,

0:53:11.805,0:53:15.075
꽤 간단한 내용으로, 많은분들이
이미 포럼에서 토론을 하고 있더군요.

0:53:15.075,0:53:17.985
그래서 지금은 일단 넘어가도록 하겠습니다.

0:53:18.000,0:53:22.280
만약 관심이 있다면, 
포럼에서 관련 내용에 대해서 이야기 해 보세요.

0:53:22.280,0:53:26.980
여기선 단순히, 모델에 주입될
데이터라고만 해 두겠습니다.

0:53:27.200,0:53:29.780
ColumnarModelData는 꽤나 유연한데,

0:53:29.780,0:53:35.720
Data Frame 형태의 데이터만 있으시다면, 
코드의 수정없이 그냥 사용하실 수 있을 겁니다.

0:53:36.780,0:53:39.755
optim.SGD는 PyTorch의 것입니다.

0:53:39.755,0:53:42.745
optim은 PyTorch에 정의된,

0:53:42.745,0:53:47.580
Optimizer에 접근하기 위한 것인데,
잠시 후 배우게 될 것입니다.

0:53:47.580,0:53:51.940
기본적으로는 가중치(weights) 를 
업데이트하는 기능을 하는데,

0:53:51.940,0:53:54.755
PyTorch에서는

0:53:54.755,0:53:57.865
이 가중치들이 모델의 파라메터라고 불립니다.

0:53:57.940,0:54:02.740
바로 윗줄에서, model = EmbeddingDot(…) 를 실행 했었는데,

0:54:02.740,0:54:06.240
EmbeddingDot은 nn.Module 을 상속하기 때문에,

0:54:06.240,0:54:09.975
PyTorch에 정의된 모듈의
모든 종류의 행동을 사용할 수 있습니다.

0:54:09.980,0:54:14.380
그리고, .parameters() 가 그 중 하나 입니다.

0:54:15.020,0:54:19.040
.parameters()는 자동으로 모델에 있는

0:54:19.080,0:54:24.000
업데이트 대상이 되는 모든 
가중치들의 목록을 가져와 주기 때문에,

0:54:24.000,0:54:26.560
꽤나 유용한 기능 입니다.

0:54:26.560,0:54:29.620
그리고 그 가중치 목록은
optimizer에 넘겨져야 합니다.

0:54:29.640,0:54:32.660
optim.SGD의 두 번째 인자는 학습률이고,

0:54:32.660,0:54:35.760
나중에 이야기 될 weight decay는 세 번째 인자,

0:54:35.760,0:54:38.860
나중에 이야기 될 momentum이 네 번째 인자 입니다.

0:54:41.455,0:54:44.485
제가 지금 당장 하지 않겠지만,

0:54:44.485,0:54:47.785
나중에 하게될 한가지 일은
학습 반복문(training loop)을 만드는 것입니다.

0:54:47.840,0:54:51.140
학습 반복문이란 각 미니배치를 반복(loop) 하면서

0:54:51.140,0:54:53.915
가중치를 (경사도*학습률)만큼 감소하여

0:54:53.920,0:54:57.260
매번 업데이트하는 과정입니다 .

0:54:57.760,0:55:01.880
fastai 에서 제공하는 fit이라는 함수가
학습 반복문입니다.

0:55:02.220,0:55:06.860
(??fit 타이핑 중)

0:55:07.220,0:55:10.100
보다시피 그 내용은 아주 간단합니다.

0:55:10.260,0:55:14.560
에포크의 수 동안 반복합니다.

0:55:14.700,0:55:17.775
tqdm은 프로그래스바를 표현하기 위한 것으로,
무시하셔도 됩니다.

0:55:17.780,0:55:24.420
학습 data loader 에서, 
x와 y를 반복적으로 가져와서 손실을 계산하는데,

0:55:24.620,0:55:30.240
손실을 프로그래스바를 통해서 출력하고,

0:55:30.340,0:55:34.980
콜백함수가 있는 경우,
콜백함수도 수행됩니다... 등등..

0:55:34.980,0:55:39.000
마지막엔 검증데이터셋에 대하여
metric 함수들을 호출합니다.

0:55:39.000,0:55:40.820
다시 정리해 보면,

0:55:40.820,0:55:42.600
매 epoch 마다,

0:55:42.600,0:55:45.800
각 미니배치를 가져와서,

0:55:45.800,0:55:48.560
Optimizer의 한 step을 수행하는 것입니다.

0:55:48.560,0:55:52.940
step은 optimizer에 있는 기능이긴 하지만

0:55:52.940,0:55:56.140
잠시 후, 직접 이를 구현해볼 것입니다.

0:55:56.500,0:55:59.860
여기선 fastai의 learner/모델의 
fit()을 사용하지 않았습니다.

0:55:59.860,0:56:02.895
단지 PyTorch 모듈만을 사용했습니다.

0:56:02.900,0:56:06.480
여기의 fit() 또한 fastai의 기능이긴 하지만,

0:56:06.480,0:56:09.680
API의 추상화 수준을 약간 낮춘 것으로

0:56:09.680,0:56:14.620
PyTorch의 모듈 자체를 수용할 수 있도록
만들어져 있습니다.

0:56:14.620,0:56:19.420
PyTorch 모델이나, 인터넷에서 구한 
PyTorch 코드들을 사용하고 싶은 이유로,

0:56:19.420,0:56:24.580
가능한한 fastai 라이브러리의 사용하고 싶지
않은 상황인데

0:56:24.640,0:56:29.040
학습 반복문을 또한 직접 작성하고
싶지 않은 상황이라면,

0:56:29.040,0:56:33.340
이 fastai의 fit() 함수를 사용하면 편리할 것입니다.

0:56:33.460,0:56:37.600
fastai 라이브러리는 
원하시는 어떤 추상 수준에서든지

0:56:37.600,0:56:40.920
사용가능하도록 디자인 되어 있습니다.

0:56:40.920,0:56:43.460
단지, 여기 정도 추상 단계에서는

0:56:43.460,0:56:48.100
재시작하는 확률적 경사하강이나,

0:56:48.100,0:56:50.800
차등 학습률 같이

0:56:50.800,0:56:54.160
learner/모델에 포함된 기능을 사용할 수 없습니다.

0:56:54.160,0:56:57.700
물론 그런 기능을 사용할 순 잇지만, 
여러분이 직접 그 내용을 구현해야 하는 것입니다.

0:56:57.700,0:57:01.460
이 정도의 추상 수준을 사용하는 것의 
단점이라고 볼 수 있을겁니다.

0:57:01.720,0:57:04.180
장점이 있다면,

0:57:04.180,0:57:07.540
단순히 매우 간결한 학습 반복문
코드만이 포함된 것으로

0:57:07.545,0:57:10.605
표준적인 PyTorch 모델이 직접적으로
사용될 수 있다는 점입니다.

0:57:10.605,0:57:14.300
이 경우에선 사용되기 좋은 것입니다.

0:57:14.300,0:57:17.740
그리고 그 결과는 전에 보던것과 
정확히 동일한 형태로 나타납니다.

0:57:17.760,0:57:22.155
검증데이터셋과 학습데이터셋에 대한 손실을

0:57:22.160,0:57:25.160
3번의 에포크에 대해서 보여주고 있습니다..

0:57:27.700,0:57:31.860
아시다시피, 우리는 이 결과가 
약 0.76 정도가 되기를 원했습니다.

0:57:31.965,0:57:35.075
하지만 아직 그 정도의 결과를 얻을수는 없었습니다.

0:57:35.080,0:57:39.220
즉, fastai 의 디폴트 협업 필터링 알고리즘은

0:57:39.240,0:57:42.540
이것보단 더 똑똑한 뭔가를 한다고
추측해 볼 수 있습니다.

0:57:42.540,0:57:45.160
그래서, 이 또한 구현해 보도록 하겠습니다.

0:57:45.160,0:57:47.540
일단 먼저 해 볼 수 있는 것은

0:57:47.540,0:57:50.380
이 낮은 수준의 fit() 함수를 사용할 땐,

0:57:50.380,0:57:54.335
학습률 annealing 같은 것을 하지 않으므로,
저희 스스로가 이것을 해 볼 수 있을 것입니다.

0:57:54.340,0:57:57.880
이 코드라인을 보시면, 
set_lrs() 라는 fastai 함수가 있는데,

0:57:57.880,0:58:00.580
표준적인 PyTorch의 optimizer와,

0:58:00.580,0:58:02.840
새로이 사용될 학습률을 넘겨줄 수 있습니다.

0:58:03.035,0:58:06.015
그리곤 다시 fit() 을 수행 해 볼 수 있을 것입니다.

0:58:06.015,0:58:09.235
이것이 수동적으로 학습률 스케쥴링을 하는 방법입니다.

0:58:09.240,0:58:12.600
보시다시피, 결과가 1.13로 약간 나아졌습니다.

0:58:12.640,0:58:16.640
아직 개선하려면 한참멀긴 하지만 말입니다.

0:58:18.440,0:58:23.200
일단 7분간의 휴식시간을 가집시다

0:58:23.200,0:58:28.560
휴식 후에 이것을 더 향상시켜 보도록 합시다.

0:58:35.840,0:58:37.880
(휴식 후)

0:58:37.960,0:58:41.740
휴식시간동안 받은 질문이 있는데,

0:58:41.820,0:58:45.080
옵셔널한 내용으로
관심 있으신 분들만 보면 될 것입니다.

0:58:45.340,0:58:47.780
fastai 라이브러리를 보시면,

0:58:47.780,0:58:50.960
model.py 라는 파일이 있습니다.

0:58:53.500,0:58:56.700
그리고, 이 파일안에 
fit() 함수가 정의되어 있습니다.

0:58:56.700,0:58:59.575
이 fit() 함수에서는
매 에포크마다

0:58:59.580,0:59:03.800
미니배치에 있는 각 x와 y를 접근해서

0:59:03.800,0:59:08.580
step() 함수를 수행하게 됩니다.

0:59:09.160,0:59:11.060
step() 함수는 ...

0:59:11.200,0:59:15.120
(코드 이동 중)

0:59:15.120,0:59:20.220
여기에 정의되어 있는데,
m (모델)의 출력을 구하는 부분이 있습니다.

0:59:20.220,0:59:22.080
기억하실테지만,

0:59:22.080,0:59:25.580
앞서 구현한 닷( . ) 연산을 위해서

0:59:25.580,0:59:28.175
model.forward 라는 것을 호출하지 않고

0:59:28.175,0:59:30.925
그냥 model(..) 을 직접 호출 했습니다.

0:59:30.925,0:59:34.460
그 이유는 model을 함수인것처럼 호출하면,

0:59:34.460,0:59:39.600
nn.Module은 자동적으로
forward를 수행하라고 해 주기 때문입니다.

0:59:39.600,0:59:43.200
즉, self.m(..) 이 바로 그런식으로 
동작하는 부분인 것입니다.

0:59:43.200,0:59:45.240
그리고, 그 이후의 내용인

0:59:45.240,0:59:47.860
손실 함수,

0:59:47.865,0:59:50.985
backward pass(경사하강)과 같은 것들은

0:59:50.985,0:59:53.755
잠시 후 배우게 될 것입니다.

0:59:53.755,0:59:56.645
관심 있을 분들을 위해서,

0:59:56.645,1:00:00.040
코드가 어떻게 구조화 되어 있는지를
잠시 보여드린 것입니다.

1:00:00.220,1:00:02.000
말씀드린것 처럼,

1:00:02.080,1:00:04.460
fastai의 코드는

1:00:04.460,1:00:07.720
세계적수준의 성능을 얻기 위해서이기도 하지만,

1:00:07.720,1:00:10.620
동시에 가독성이 좋도록 디자인되어 있습니다.

1:00:10.680,1:00:12.360
그러므로,

1:00:12.360,1:00:15.660
내부동작이 궁금하면 직접 코드를 들여다 보실 수 있고,

1:00:15.660,1:00:18.100
만약 의문사항이 생기면,
포럼에 질문도 남겨 주시기 바랍니다.

1:00:18.260,1:00:21.820
그리고 여러분이 코드를 좀 더
현명하게 짤 수 있을것 같으면

1:00:21.900,1:00:23.760
알려주시기 바랍니다.

1:00:24.860,1:00:30.040
그리고, 저희는 앞으로
점점 더 코드 내부를 많이 들여다 보게될 것입니다.

1:00:30.720,1:00:34.375
그러면, 휴식시간 전에 구현한 것을
약간 향상시켜보도록 합시다.

1:00:34.375,1:00:37.825
일단은 엑셀을 사용해서
향상시켜보도록 하겠습니다.

1:00:38.085,1:00:41.035
이렇게 생각해 볼 수 있을 것입니다.

1:00:41.035,1:00:43.715
사용자 72이

1:00:43.720,1:00:50.000
공상과학, 현대적, 특수효과 영화들을
좋아할 것이라고 말입니다.

1:00:50.020,1:00:51.920
또한, 영화 27은

1:00:51.920,1:00:55.900
공상과학, 특수효과가 사용된, 그리고 
대화 주도적이진 않은 영화라고 말입니다.

1:00:55.960,1:01:00.180
근데 한가지 중요한점을 놓치고 있을 수 있습니다.

1:01:01.440,1:01:06.060
사용자 72는 영화라면 대체로 열정적이어서,

1:01:06.060,1:01:10.215
평균적인 평점을 높게 줄 지도 모릅니다.

1:01:10.220,1:01:14.380
마찬가지로, 또 영화 27는 
단지 그냥 인기있는 영화일 수도 있습니다.

1:01:14.380,1:01:17.160
그러면 평균적으로 점수가 높을 것입니다.

1:01:17.160,1:01:20.360
그래서, 이때 뭔가 추가되어야 할 것으로

1:01:20.360,1:01:24.980
사용자와 영화 각각에 대한 상수값을
생각해 볼 수 있습니다.

1:01:24.980,1:01:28.680
그리고, 이 상수값 이라는 것은 뉴럴넷에서
bias라고 불리는 것입니다.

1:01:28.760,1:01:32.640
즉, bias를 추가해볼 수 있는 것이고
이를 아주 쉽게 해볼 수 있습니다.

1:01:32.640,1:01:35.100
bias 스프레드시트 탭으로 이동해 보면,

1:01:35.105,1:01:37.945
전과 동일한 데이터가 있습니다.

1:01:37.945,1:01:42.660
그리고, 전과 동일한 
잠재적(latent) factors가 있는데

1:01:42.660,1:01:47.640
단지, 여기서는 추가적인 한 줄이
영화와 사용자 행렬에 대해서 존재합니다.

1:01:47.960,1:01:50.160
그러면, 별로 놀라운 연산과정이 아닌,

1:01:50.160,1:01:52.780
전과 동일한 행렬의 곱셈을 수행하고

1:01:52.780,1:02:01.040
그 결과에 추가된 한 줄의 숫자들을
더해줘 볼 수 있습니다.

1:02:01.040,1:02:04.600
bias는 이런식으로 추가될 수 있는 것입니다.

1:02:04.600,1:02:08.040
이 부분을 제외하면,
동일한 손실함수를 사용하는 등

1:02:08.040,1:02:13.360
전과 동일하게, solver를 사용해볼 수 있습니다.

1:02:13.780,1:02:18.140
단, 이번엔 variable 내용을 
bias가 포함되도록 변경해줘야 합니다.

1:02:18.140,1:02:21.100
그러면, solve를 수행해볼 수 있고,

1:02:21.100,1:02:24.440
약간 그 과정이 진행되도록 놔둬보면

1:02:24.440,1:02:28.060
더 나은 결과로 도달하게 되는것을
보실 수 있을것입니다.

1:02:28.320,1:02:32.300
이 방법이 모델을 향상시키기 위한
첫 번째 방법입니다.

1:02:32.300,1:02:35.240
코드로 이를 구현 해 보겠습니다.

1:02:35.880,1:02:40.480
약간의 코드 간결화를 위해서,

1:02:40.480,1:02:44.020
get_emb() 라는 함수를 정의 했습니다.

1:02:44.020,1:02:48.040
이 함수는 임베딩 행렬의
열의 크기를 정하기 위한 입력의 갯수와

1:02:48.040,1:02:52.055
임베딩 행렬의 행의 크기를 위한 
factors의 갯수를 인자로 받아들입니다.

1:02:52.060,1:02:54.700
이 인자들을 이용해서
임베딩(nn.Embedding) 을 생성하게 되고

1:02:54.700,1:02:57.480
그 값을 -0.01, 0.01 범위로
무작위 초기화 해 주었는데

1:02:57.480,1:03:00.560
왜 이 범위인지는 솔직히 잘 모르겠습니다만

1:03:00.560,1:03:04.320
어느정도 그 값의 범위가 올바르게만 된다면,
그 이유가 크게 중요하진 않을 것입니다.

1:03:04.320,1:03:08.620
어쨋든, get_emb 함수는 이렇게 초기화된 
nn.Embedding을 리턴해 줍니다.

1:03:09.140,1:03:12.820
이번에는 (사용자 수 x factors) 를 self.u로,

1:03:12.820,1:03:16.195
(영화 수 x factors) 를 self.m로,
임베딩 행렬을 만드는것 이외에

1:03:16.200,1:03:20.280
(사용자 수 x 1) 을 self.ub로,

1:03:20.280,1:03:24.675
(영화 수 x 1)을 self.mb로,
bias 벡터를 만들어 주는것이 추가 되었습니다.

1:03:24.675,1:03:27.365
이 코드는 단순히 리스트 표현법으로

1:03:27.365,1:03:29.925
각 튜플(쌍) 들을 반복하면서

1:03:29.925,1:03:33.940
이들에 대한 nn.Embedding을 만들고,
만든것을 self.u/m/ub/mb 에 넣어주라는 것입니다.

1:03:34.040,1:03:41.780
forward는 전과 완전히 동일하지만,

1:03:41.780,1:03:48.600
두 단계로 나눠서 코딩되었기 때문에, 
약간 혼동스러울 수도 있을 것입니다.

1:03:48.600,1:03:52.020
약간 더 간결하게 코드를 바꿔 보겠보겠습니다.

1:03:52.020,1:03:58.920
세번째 라인의 .sum(1)을 두번째 라인으로 옮기면

1:03:59.040,1:04:01.875
어디선가 본듯한 모양일 것입니다.

1:04:01.880,1:04:06.000
단지 이번에는, 전과 동일한
(사용자 * 영화).sum(1) 을 수행하고 난 후

1:04:06.000,1:04:10.380
단순히 사용자와 영화에 대한 bias를 더해줍니다.

1:04:10.880,1:04:15.300
.squeeze() 함수는  PyTorch의 것으로,

1:04:15.380,1:04:19.240
추가적인 차원축을 추가 해 줍니다.

1:04:19.820,1:04:23.880
브로드캐스팅이란 것을 해본적이 없다면, 
무슨 말인지 전혀 이해가 안될 것입니다.

1:04:23.940,1:04:27.060
머신러닝 코스에서 브로드캐스팅을 다루고 있기 때문에,

1:04:27.065,1:04:30.255
이 코스에서 이를 다루진 않을 것입니다.

1:04:30.260,1:04:34.500
하지만, 기본적으론 브로드캐스팅이란

1:04:34.500,1:04:39.360
이 코드에서 처럼, 
um이 행렬이고 ub는 벡터일때 처럼,

1:04:39.360,1:04:46.240
벡터를 행렬에 더해주고 싶을때
일어나는 일입니다.

1:04:46.600,1:04:50.220
무슨일이 일어나냐 하면, 벡터를 복사합니다.

1:04:50.220,1:04:53.840
그래서 행렬과 동일한 크기가 되도록 만드는 것입니다.

1:04:53.840,1:04:58.440
컬럼단위, 열단위 또는 
어떤 방식의 복사를 사용하던지간에

1:04:58.445,1:05:00.845
브로드캐스팅 이라고 불리는데,

1:05:00.845,1:05:03.740
여기서의 브로드캐스팅 방법은 
Numpy와 동일합니다.

1:05:03.760,1:05:06.755
PyTorch에선 브로드캐스팅이
지원되지 않았던 때가 있었습니다.

1:05:06.760,1:05:11.020
그래서, 제가 이를 PyTorch에
처음으로 추가한 사람이었고

1:05:11.020,1:05:15.660
PyTorch의 개발자들이
이를 지원하기 위한 작업을 했었습니다.

1:05:15.660,1:05:21.360
어쨋든 지금은 PyTorch와 Numpy에서 동일한
브로드캐스팅 연산을 사용할 수 있게 되었습니다.

1:05:21.360,1:05:27.100
전에 접해본 적이 없다면, 매우 중요한 내용입니다.

1:05:27.140,1:05:30.080
왜냐하면 PyTorch와 Numpy에서

1:05:30.080,1:05:32.860
계산을 빨리 하기 위해서 필요한

1:05:32.860,1:05:35.760
가장 중요한 근간이되는 
방법 중 하나이기 때문입니다.

1:05:35.760,1:05:38.700
반복문을 사용할 필요가 없게 해 줍니다.

1:05:38.700,1:05:41.980
만약 여기의 각 행렬의 모든 열을 Loop 하고,

1:05:42.095,1:05:45.165
이 벡터를 각 열에 매번 더해준다고 상상해 보세요.

1:05:45.165,1:05:49.440
그 과정은 매우 느리고, 
코드도 훨씬 더 많이 필요하게 될 것입니다.

1:05:49.740,1:05:52.795
사실상, 브로드캐스팅이란 아이디어는

1:05:52.800,1:05:56.340
1950년대에 Ken Iverson이라는 
아주 특별한분에 의해

1:05:56.340,1:05:59.060
디자인된 APL이라는 언어에 기반합니다.

1:05:59.320,1:06:04.100
APL은 원래 새로운 종류의
수학적 기호로써 디자인된 것으로,

1:06:04.100,1:06:09.420
이분은 관련해서 “notation as a tool for thought” 이라는 
굉장한 수필을 적었었습니다.

1:06:09.420,1:06:11.055
이 아이디어는

1:06:11.060,1:06:14.760
좋은 표기법은 더 나은것을
생각할 수 있게 만들어 준다는 것이었습니다.

1:06:14.760,1:06:16.935
그리고, 이 표기법들 중의 한 부분이

1:06:16.935,1:06:19.965
브로드캐스팅 이라는 아이디어 였습니다.

1:06:19.965,1:06:23.940
저는 이것에 대해서 매우 열정적인 사람으로,
매우 많이 사용할 것입니다.

1:06:23.940,1:06:28.380
추가적인 정보를 위해선, 
머신러닝 코스를 들어보시거나

1:06:28.380,1:06:34.660
Numpy 의 Broadcasting을
검색해 보셔도 좋을 것입니다.

1:06:35.500,1:06:36.800
어쨌든,

1:06:36.800,1:06:40.420
브로드캐스팅은 직관적이면서,
합리적인 방식으로 동작하기 때문에

1:06:40.420,1:06:44.680
이 벡터들을 행렬에 더하는데 사용할 수 있을 것입니다.

1:06:46.960,1:06:48.500
여기까지 하고나면,

1:06:48.540,1:06:52.580
한 가지 더 해야할 것이 있습니다.

1:06:52.900,1:06:55.480
전에 누군가 질문 하셨던것 같은데,

1:06:55.480,1:07:01.240
평점을 1~5 사이로
제한할 수 있는지에 대한 것이었습니다.

1:07:01.640,1:07:03.340
이 것에 대한 대답은,

1:07:03.460,1:07:06.415
그럴 수도 있다는 것입니다.

1:07:06.420,1:07:09.340
좀 더 구체적으로는

1:07:09.660,1:07:13.040
sigmoid 함수를 사용해서
이를 가능하게 할 수 있습니다.

1:07:13.580,1:07:17.580
상기시켜 드리자면, sigmoid 함수는

1:07:18.500,1:07:21.640
이렇게 생긴 그래프입니다.

1:07:21.980,1:07:24.780
상한값이 1 이었습니다.

1:07:24.980,1:07:28.380
이 sigmoid 함수에 값을 넣어주는 것을 의미합니다.

1:07:28.385,1:07:31.495
예를 들어서, 4,96 이라는 평점을

1:07:31.495,1:07:34.365
sigmoid 함수에 넣어주면 
이 평점값은 크기 때문에

1:07:34.365,1:07:37.475
여기 부근쯤이 된다고 볼 수 있을 것입니다.

1:07:37.480,1:07:40.620
그리곤, 이 결과값에 
예를들어서

1:07:40.620,1:07:44.300
5를 곱해줄 수 있을 겁니다.

1:07:45.040,1:07:48.400
우리의 경우는
원하는 범위가 1~5 이기 때문에,

1:07:48.400,1:07:51.880
결과에 4를 곱해주고 
1을 더해줄 수 있을 것입니다.

1:07:51.880,1:07:55.240
아주 단순한 아이디어 입니다.

1:07:55.880,1:07:58.920
이 코드가 그 방법을 나타내고 있습니다.

1:07:58.920,1:08:01.875
엑셀에서 본 것 처럼,

1:08:01.875,1:08:04.985
닷( . ) 연산에서 얻어진 값에 
bias를 더한 결과값을

1:08:04.985,1:08:08.200
sigmoid 함수에 넣어주고 있습니다.

1:08:08.980,1:08:11.700
PyTorch 에서는

1:08:11.700,1:08:17.140
텐서에 적용 가능한 모든 함수들이

1:08:17.140,1:08:20.020
기본적으로 F 라는것 안에 정의되어 있습니다.

1:08:20.495,1:08:23.405
PyTorch 입장에서는
완전히 일반적인 내용으로

1:08:23.405,1:08:26.425
실제로는 torch.nn.functional 이 
대문자 F 의 풀네임이긴 하지만,

1:08:26.425,1:08:29.315
모든 PyTorch 사용자 및
PyTorch의 공식문서 조차

1:08:29.315,1:08:32.385
import torch.nn.functional as F 
의 형태로 사용하고 있습니다.

1:08:32.500,1:08:35.960
따라서, F.sigmoid 라는것은

1:08:35.960,1:08:39.035
torch.nn.functional 이라는 모듈에 있는

1:08:39.035,1:08:41.795
sigmoid 라고 불리는 함수를 의미합니다.

1:08:41.795,1:08:43.965
다시 돌아가서,

1:08:43.965,1:08:47.235
sigmoid 함수가 결과값에 적용되게 돼는데

1:08:47.235,1:08:50.245
그러면, 모든 값들은 0~1 사이로
짜부러지게 됩니다.

1:08:50.245,1:08:55.200
그리곤, 0~1 사이로 변한 값들에
(5-1)=4 에 곱해주고

1:08:55.200,1:08:57.695
다시 1을 더해줍니다.

1:08:57.700,1:09:01.080
그러면, 1~5 사이의 숫자가 만들어지게 됩니다.

1:09:01.260,1:09:04.040
이 코드가 반드시 
수행되어야 할 필요는 전혀 없습니다.

1:09:04.040,1:09:06.515
이 코드를 주석처리해도, 
여전히 동작하는데는 문제가 없습니다.

1:09:06.520,1:09:08.780
단지, 그렇게 되면

1:09:08.780,1:09:12.860
나중에 1~5 범위의 값을 갖게
만들어 주는 계산이 추가로 필요해질 뿐입니다.

1:09:12.865,1:09:15.695
반면에, 이 코드를 주석처리 하지 않으면,

1:09:15.700,1:09:19.240
어떤 영화가 아주 좋다고 생각하면,
매우 높은 숫자를 계산되고

1:09:19.240,1:09:22.280
그렇지 않다면, 매우 낮은 숫자를 계산되고

1:09:22.280,1:09:26.180
그 후에, 그 숫자가 올바른 범위에 있도록 보장해주는 
기능을 손쉽게 가능하게 해 줍니다.

1:09:26.900,1:09:30.200
비록 뉴럴넷에서의 사용의 예를 보여드린 것이지만,

1:09:30.200,1:09:33.340
파라메터 값의 적합화같이

1:09:33.340,1:09:37.980
함수가 원하는 범위의 값으로 리턴하는
모든 경우에 대해서 또한

1:09:37.980,1:09:42.480
좋은 한 예라고 볼 수 있습니다.

1:09:44.140,1:09:47.340
어쨌든, 이렇게 만들어진 클래스를
EmbeddingDotBias 라고 이름 붙였고,

1:09:47.360,1:09:50.735
그 후엔, 전과 동일한 방식으로
인스턴스를 만들 수 있습니다.

1:09:50.735,1:09:53.135
여길 보시면,
 .cuda() 를 호출하고 있는데,

1:09:53.140,1:09:55.160
GPU에 올리기 위한 과정입니다.

1:09:55.160,1:09:58.735
fastai의 learner/모델에서는 자동으로 되지만,
지금은 이를 사용하지 않고 있지 않기 때문에,

1:09:58.740,1:10:01.360
수동적으로 GPU에 올리라고
명령 해 줘야 한 것입니다.

1:10:01.360,1:10:04.160
그 아래 라인도 전과 동일한 것으로
optimizer를 생성하고 있습니다.

1:10:04.160,1:10:06.360
그리곤 fit()또한 전과 동일하게 수행하는데

1:10:06.360,1:10:08.940
이번에 출력된 결과는 좋아 보입니다.

1:10:09.195,1:10:12.245
그리고나선, 학습률에 약간 변화를 줘 봤고

1:10:12.245,1:10:15.285
그러니까 0.80 까지 결과가 좋아졌습니다.

1:10:15.285,1:10:19.260
따라서, 목표 결과에
매우 근접한 수준까지 왔습니다.

1:10:22.700,1:10:25.620
지금까지 보신 것들이

1:10:25.920,1:10:35.160
어떻게 대부분의 협업 필터링이 수행되는지를
보여주는 주된 코드의 단계들 이었습니다.

1:10:35.680,1:10:39.880
그리고 한 학생분이
한가지 중요한 점을 상기시켜 주셨는데,

1:10:39.880,1:10:46.280
엄격히 따져볼때, 
저희가 한 것은 행렬 분해가 아니라는 것입니다.

1:10:46.280,1:10:51.700
왜냐하면 엄격히 따져보면, 행렬 분해라는 것은
왼쪽 행렬과 오른쪽 행렬을 가지고

1:10:51.700,1:10:54.860
중앙의 결과 행렬을 만들어내는 것이기 때문입니다.

1:10:55.760,1:11:00.800
원본 데이터가 어떻게 생겼었는지를
상기 해 보면,

1:11:01.260,1:11:05.040
원본 행렬에서 값이 비어 있는 곳인

1:11:05.140,1:11:09.920
여기와, 여기같은 곳에는
0의 값을 넣어 줬었습니다.

1:11:10.600,1:11:13.595
수식을 확인 해 보면, 
만약 원본값이 비어 있는 경우

1:11:13.595,1:11:16.865
예측값도
0을 넣어주라고 적혀 있습니다.

1:11:17.045,1:11:19.695
전체 행렬을 만들어내는

1:11:19.700,1:11:23.620
일반적인 행렬 분해에서는 
이런 행위를 할 수 없습니다.

1:11:23.620,1:11:28.560
그래서, 사람들이 전통적인 선형대수를 사용해서
협업 필터링 문제를 해결하고자 했을 때,

1:11:28.560,1:11:32.460
꽤나 문제가 되었던 부분입니다.

1:11:32.920,1:11:37.560
이 엑셀에 있는 예제의
원본 행렬에는 빈공간이 많지는 않습니다.

1:11:37.565,1:11:40.385
왜냐하면, 영화를 가장 많이본 사용자들과

1:11:40.385,1:11:43.365
가장 많이 보여진 영화들만을 선정했기 때문입니다.

1:11:43.365,1:11:46.305
하지만, 전체 행렬은
주로 빈 공간이 많이 차지하게 됩니다.

1:11:46.305,1:11:50.120
전통적인 기법은 이 빈 공간들을 0으로 다뤘었습니다.

1:11:50.520,1:11:53.040
따라서, 만약 어떤 영화를 본 적이 없다면,

1:11:53.040,1:11:56.460
이 0이라는 것을 예측해야만 했던 것이고
그 의미는 영화를 “좋아하지 않는다” 라는 것이 됩니다.

1:11:56.460,1:11:58.380
그러면, 최악의 예측결과가 도출되는 것이죠.

1:11:58.600,1:12:03.820
여기서 사용된 확률적 행렬 분해 방식에서는

1:12:03.820,1:12:07.600
사용된 자료구조가 크로스 테이블이 아니라,

1:12:07.600,1:12:11.500
이런식으로 생겼다는 사실로부터
어떤 이점을 취할 수 있습니다.

1:12:11.960,1:12:16.000
그 이점이라는 것은, 오직 실제로 등장한
userid와 movieid의 조합에 대한

1:12:16.000,1:12:18.055
손실만을 계산하게 되는 것입니다.

1:12:18.055,1:12:20.965
예를 들어서, userid가 1이고
movieid가 1029일 때,

1:12:20.965,1:12:23.225
레이블 값은 3이어야 하는데

1:12:23.300,1:12:26.600
만약 예측된 값이 3.5 라면,
0.5의 손실이 발생합니다.

1:12:26.940,1:12:29.800
이 테이블에 실제로 등장한 적이 없는

1:12:29.800,1:12:34.400
userid와 movieid 조합에 대한
손실은 계산될 필요가 없다는 것입니다.

1:12:34.400,1:12:37.415
또한, 당연한 것으로,
미니배치에 존재하는 것들은

1:12:37.420,1:12:41.680
실제 이 테이블에 있는 것들로만 구성이 됩니다.

1:12:44.120,1:12:47.340
흥미롭게도, 
이것과 관련된 많은 내용들은

1:12:47.340,1:12:50.180
Netflix Prize(경연)에서 등장 했었습니다.

1:12:50.380,1:12:53.540
사실상 Netflix Prize 이전에

1:12:53.560,1:12:57.500
이 확률적 행렬 분해가 발명 되었었지만,

1:12:57.500,1:13:01.040
그 누구도 그런게 있다는 사실을 몰랐습니다.

1:13:01.040,1:13:03.755
그러다가 Netflix Prize의 첫 해에서,

1:13:03.760,1:13:07.060
어떤 한 참가자가 이것 관련 
매우 유명한 블로그 글을 적었는데

1:13:07.060,1:13:11.940
그 글의 내용은 “이 기법을 확인해 보세요!”, 
“믿을 수 없을정도로 간단한데, 엄청 잘 동작합니다!”

1:13:11.940,1:13:15.300
같은 것이었고, 갑자기 Netflix 경연의 
리더보드의 모든 참가자들의 결과가

1:13:15.300,1:13:17.000
훨씬 더 좋아져 버렸습니다.

1:13:17.140,1:13:19.980
수 년전에 일어난 것으로,

1:13:19.980,1:13:23.200
현재의 모든 협업 필터링은
이 방법을 사용하고 있습니다.

1:13:23.200,1:13:27.360
단, 모든 협업 필터링이 sigmoid 부분을
채용하는 것은 아닙니다.

1:13:27.700,1:13:30.400
어떤 고도의 지식이 필요한 부분도,

1:13:30.400,1:13:35.100
지난 주에 공부한 
최신예적인 NLP 적인 부분도 아닙니다.

1:13:35.140,1:13:37.940
별로 드물지 않게 사용되긴 하지만,

1:13:37.940,1:13:41.220
여전히 이 방법을 사용하지 않는 사람들도 있습니다.

1:13:41.220,1:13:44.100
단지, 도움이 많이 되는 부분인 것입니다.

1:13:44.380,1:13:46.835
지금쯤 우리가 해 볼 수 있는 것으로,

1:13:46.840,1:13:49.080
이제는 이것에 대한 정의를

1:13:49.080,1:13:52.100
한번 들여다 보기에 좋은 시점일것 같습니다.

1:13:52.780,1:13:55.400
column_data.py 파일을 열어보면,

1:13:55.400,1:13:58.700
ColumnData 모듈에 
이 모든 정의들이 포함되어 있습니다.

1:13:58.700,1:14:04.960
그리고, 이것을 전에 사용하던 
CollabFilterDataset으로부터 리턴된 것과

1:14:04.960,1:14:09.040
비교해볼 수 있습니다.

1:14:09.380,1:14:14.580
코드의 CollabFilterDataset 부분으로
이동 해 보겠습니다.

1:14:14.580,1:14:17.375
여기 있군요.

1:14:17.380,1:14:22.500
그리곤 get_learner를 호출 했는데, 
그 부분으로 이동해 보겠습니다.

1:14:22.720,1:14:27.120
이 메소드는 CollabFilterLearner를
생성해서 리턴하는데,

1:14:27.120,1:14:30.755
이를 생성 할 때, get_model()을 이용해서
얻어진 모델을 넣어주게 됩니다.

1:14:30.760,1:14:34.580
그러면, get_model()에서는
EmbeddingDotBias를 생성해 주는걸 알 수 있습니다.

1:14:34.580,1:14:37.760
다시 EmbeddingDotBias로 이동해 보면,

1:14:37.760,1:14:41.300
우리가 작성한 것과 동일한 코드가
있는 것을 보실 수 있을 겁니다.

1:14:41.300,1:14:44.400
각각에 대한 임베딩들을 만드는 부분이 있고,

1:14:44.400,1:14:46.435
forward 부분에서는

1:14:46.435,1:14:49.145
(사용자 * 아이템).sum(1)과

1:14:49.145,1:14:52.145
bias를 더해주고, sigmoid가 수행됩니다.

1:14:52.145,1:14:57.020
즉, 우리는 fastai에 있는 코드를
똑같이 직접 만들어본 것이라고 볼 수 있습니다.

1:14:59.560,1:15:01.840
라이브러리 코드 에서는

1:15:01.840,1:15:05.800
CollabFilterDataset 이라는
특별한 클래스의 사용 이점을 이용해서

1:15:05.800,1:15:10.100
구현이 더 간결하고, 더 쉽게 가능하도록끔
되어 있습니다.

1:15:10.100,1:15:13.140
users와 items를 forward에 전달해 줄 때,

1:15:13.140,1:15:15.575
계속형과 범주형 변수를 
끄집어낼 필요가 없어지는 것입니다.

1:15:15.580,1:15:17.960
이 부분을 제외하면, 정확히 동일한 코드 입니다.

1:15:17.960,1:15:20.940
희망적으로, 여러분이 fastai 라이브러리라는 것은

1:15:20.945,1:15:23.885
이해하기 매우 어려운 컨셉을 가지는 것이 아닌

1:15:23.885,1:15:27.235
별로 난해하지 않은 코드라는 것을 아셨길 바랍니다.

1:15:27.240,1:15:30.480
저희가 이 부분을 
맨땅에 구현해 봤으니까 말이죠.

1:15:31.940,1:15:36.960
그러면, 
왜 0.8 대신에 0.76 이라는 결과가 나왔을까요?

1:15:36.960,1:15:41.380
제 생각엔, 재시작하는 확률적 경사 하강법 또는

1:15:41.380,1:15:43.960
cycle_mult 또는

1:15:43.960,1:15:46.600
Adam Optimizer 같은

1:15:46.600,1:15:49.920
약간의 학습에 대한 기법들이 
사용됐기 때문인것 같습니다.

1:15:50.520,1:15:54.200
>> 설명하신 것을 보면서 생각한 것인데

1:15:54.200,1:15:57.440
>> 날짜정보에 대해서 어떤 기법을 사용하면,

1:15:57.440,1:16:01.740
>> 확실히 성능 향상을 할 수 있다고 생각합니다.

1:16:01.740,1:16:06.040
>> 왜냐하면, 보여주신건 보통의 모델이기 때문에,

1:16:06.040,1:16:09.300
>> 더 많은 기능들을 추가해 볼 수 있기 때문입니다.

1:16:09.300,1:16:10.815
맞습니다. 맞는 말씀입니다.

1:16:10.815,1:16:13.765
이 코드 내부를 읽어보신 경험으로

1:16:13.765,1:16:17.560
이젠 비록 Notebook에
EmbeddingDotBias같은 것이 없더라도

1:16:17.560,1:16:20.940
fastai에 있는 다른 모델 내용을 참조해서, 
직접 작성해 볼 수 있을 것입니다.

1:16:20.940,1:16:22.860
fastai 어떤 모델에 대한 코드를 보시고,

1:16:22.860,1:16:26.380
“이 모델이 내가 원하는 대부분의 동작을 수행하네!”

1:16:26.380,1:16:28.980
“그런데 날짜정보를 다루지는 않는구나!” 
라고 생각해 볼 수 있습니다.

1:16:28.980,1:16:33.440
그러면, 그 코드를 선택하고 복사해서
Notebook에 붙여넣어준 후,

1:16:33.860,1:16:37.760
좀 더 나은 버전의 모델을 만들어 볼 수 있게되고

1:16:37.760,1:16:40.300
이것저것 시도도 해 볼 수 있을 것입니다.

1:16:40.420,1:16:43.080
즉, 오픈소스 코드로부터

1:16:43.080,1:16:47.040
여러분 자신만의
모델 클래스를 만들 수 있는 것입니다.

1:16:47.380,1:16:49.040
예를 들어서

1:16:49.040,1:16:51.795
두 가지 정도 시도해 볼 수 있을것 같습니다.

1:16:51.800,1:16:54.200
타임스탬프 정보를 활용해서

1:16:54.200,1:16:59.520
특정시간대의 사용자들이 어떤 영화에 대해서

1:16:59.520,1:17:03.675
긍정 또는 덜 긍정적인 평점을 남기는
경향이 있는지를 파악할 수 있을수도 있습니다.

1:17:03.680,1:17:07.520
또는, 각 영화에 대한 장르 목록이 있었는데

1:17:07.520,1:17:09.840
이 정보를 활용해볼 수도 있을 것입니다.

1:17:10.520,1:17:12.820
단지 한가지 문제는

1:17:12.820,1:17:15.800
이 정보들을 EmbeddingDotBias 모델에

1:17:15.800,1:17:18.760
활용하기란 약간 어렵다는 것입니다.

1:17:18.760,1:17:21.895
왜냐하면 이 모델은 꽤나 커스텀한 형태이기 때문입니다.

1:17:21.895,1:17:24.965
그래서, 우리가 다음으로 하게될 것은

1:17:24.965,1:17:28.295
이것의 뉴럴넷 버전을 만드는 것입니다.

1:17:29.900,1:17:32.940
기본적인 아이디어를 말씀드리도록 하겠습니다.

1:17:32.980,1:17:37.300
전과 동일한 것들을 사용하게 될텐데요,

1:17:37.300,1:17:40.120
여기엔 사용자 목록들이 있습니다.

1:17:40.120,1:17:43.620
그리고 이것이 사용자에 대한 임베딩입니다.

1:17:44.940,1:17:46.980
아래엔 영화 목록들이 있고,

1:17:46.980,1:17:50.040
영화에 대한 임베딩이 있습니다.

1:17:50.540,1:17:54.880
전에 있던 영화 임베딩 행렬을
전치(transpose)시켜줘서

1:17:54.880,1:17:57.440
이 두 임베딩들이
동일한 모양을 같도록 해 줬습니다.

1:17:57.980,1:18:01.540
그리고 여기는 
사용자, 영화, 평점 데이터가 있는데

1:18:01.540,1:18:05.120
크로스 테이블 형태가 아니라,
원본 데이터의 형태입니다.

1:18:05.120,1:18:08.840
각 행이 사용자, 영화, 평점을 나타내죠.

1:18:09.880,1:18:12.240
첫 번째로 해야 할 일은

1:18:12.340,1:18:15.020
사용자 14 를

1:18:15.020,1:18:19.940
인접한 사용자 인덱스값으로 교체하는 것입니다.

1:18:19.940,1:18:23.880
엑셀에선 MATCH 라는 함수로 가능한데,

1:18:23.880,1:18:29.080
목록의 길이가 얼마나 되는지 알아내서

1:18:29.080,1:18:32.900
사용자 14이 그 목록의 첫 번째이고,

1:18:32.900,1:18:35.620
사용자 29은 두 번째인등

1:18:35.620,1:18:38.200
순서대로 인덱스를 부여하게 됩니다.

1:18:38.420,1:18:42.660
전에 Python 코드로,
딕셔너리 자료형을 만들어서

1:18:42.660,1:18:46.680
매핑한 것과 동일한 과정이라고 보시면 됩니다.

1:18:47.140,1:18:51.020
이렇게 구해진 인덱스로 교체가 되면,

1:18:51.060,1:18:54.740
(사용자, 영화, 평점) 조합에 대해서

1:18:54.740,1:18:57.940
적절한 임베딩을 색인할 수 있습니다.

1:18:58.200,1:19:02.120
실제로 어떻게 동작하는지를 보면

1:19:02.180,1:19:06.435
사용자 목록의 시작점에서,

1:19:06.440,1:19:10.440
사용자 인덱스만큼 아래로 내려가서,

1:19:10.440,1:19:15.560
각 사용자에 해당하는 
임베딩 벡터를 가져옵니다.

1:19:15.560,1:19:22.220
0.19가 왼쪽의 0.19등 처럼,
참조하고 있는것을 알 수 있습니다.

1:19:22.820,1:19:26.400
임베딩의 동작하는 방식을
보여주고 있습니다.

1:19:26.600,1:19:29.480
한 가지 기억하셔야 할 것은

1:19:29.560,1:19:33.880
이것은 원-핫 인코딩과도 동일하다는 것입니다.

1:19:34.320,1:19:40.600
만약 사용자 인덱스가 (1, 0, 0, ... ) 
형태의 벡터였다면,

1:19:40.960,1:19:44.980
이 벡터와 임베딩 행렬을 곱해주게 되면

1:19:45.040,1:19:49.580
결국엔 첫 번째 열만이
리턴되게 되기 때문입니다.

1:19:51.415,1:19:54.525
기억해두시기에 아주 유용할 것입니다.

1:19:54.525,1:19:57.260
임베딩이란 단순히 행렬의 곱셈인데,

1:19:57.300,1:20:03.295
이 방식의 유일한 존재이유는
최적화의 목적 때문입니다.

1:20:03.295,1:20:06.185
PyTorch로 하여금

1:20:06.185,1:20:09.425
단순 행렬 곱셈이긴 하지만,

1:20:09.425,1:20:13.260
원핫 인코딩 된 것이라고 보장할 수 있게
해주기 때문에

1:20:13.260,1:20:16.480
실제론 행렬 곱셈을 안해줘도 되고
단지 색인만을 해도 된다는 것을

1:20:16.480,1:20:19.100
알 수 있게 해 줍니다.

1:20:19.500,1:20:21.920
이것이 임베딩의 모든 것이고

1:20:21.920,1:20:24.800
특정 종류의 행렬 곱셈에 대한,

1:20:24.800,1:20:27.580
계산적 성능에 관한 것이라고 보면 됩니다.

1:20:27.920,1:20:29.960
다시 돌아가서,

1:20:29.960,1:20:32.645
이것은 사용자 인덱스 1이 색인한 
사용자 임베딩에 대한 것이고,

1:20:32.645,1:20:36.120
오른쪽은 그 사용자가 평점을 매긴
영화의 임베딩을 색인한 내용 입니다.

1:20:36.380,1:20:39.380
movieid가 417인데,

1:20:39.380,1:20:42.280
이 값은 14라는 인덱스 값을 갖는 것입니다.

1:20:42.280,1:20:46.680
즉, 영화 임베딩 값들은 
0.75 / 0.47 / 0.05 / 0.91 / 0.59 가 되어야 하는데

1:20:46.720,1:20:50.580
이 값들이 채워져 있는걸 확인해볼 수 있습니다.

1:20:50.920,1:20:55.260
그래서 이렇게 사용자와 영화에 대한
임베딩을 얻어 왔습니다.

1:20:55.260,1:20:59.180
그런데 일반적으로 해 온 방식인,

1:20:59.180,1:21:04.380
이 두개를 닷( . ) 연산 하는것 대신에,

1:21:04.380,1:21:14.140
만약 이 두개를 이어 붙여서 길이 10인
하나의 벡터로 만들고,

1:21:14.140,1:21:19.040
이것을 뉴럴넷에 입력으로
넣어주면 어떻게 될까요?

1:21:19.880,1:21:22.620
여기선 사실상

1:21:22.660,1:21:27.400
임베딩 계층의 결과이기 때문에,

1:21:27.440,1:21:31.455
출력 activations에 대한 텐서이긴 하지만

1:21:31.460,1:21:34.780
언제든지 입력으로 사용 가능한
activations에 대한 텐서를 가지고 있다면,

1:21:34.780,1:21:37.900
뉴럴넷에 입력으로 넣어줄 수 있습니다.

1:21:38.120,1:21:41.060
왜냐하면 뉴럴넷은 협업 필터링을 포함해서,

1:21:41.060,1:21:43.840
무엇이든지 계산할 수 있는 능력이 있기 때문이죠.

1:21:44.260,1:21:45.980
그러니까, 한번 이렇게 시도를 해봅시다.

1:21:46.060,1:21:51.580
여기를 보시면 EmbeddingNet 이라는것이 있습니다.

1:21:52.080,1:21:58.440
이번서는 
별도의 bias를 만들지는 않았습니다.

1:21:59.000,1:22:01.040
왜냐하면

1:22:01.040,1:22:04.500
PyTorch에서 제공하는 Linear 계층에는

1:22:04.500,1:22:08.340
이미 bias가 포함되어 있기 때문입니다.

1:22:09.420,1:22:16.200
nn.Linear 가 어떤 것인지를 묘사해 보겠습니다.

1:22:18.620,1:22:23.200
u 라는 행렬이 있습니다.

1:22:23.200,1:22:26.140
이 행렬의 열의 갯수는 사용자의 수 이고,

1:22:26.440,1:22:29.720
행의 갯수는 factors의 수 입니다.

1:22:29.840,1:22:34.660
그리고 m 이라는 행렬이 있습니다.

1:22:35.000,1:22:38.135
이 행렬의 열의 갯수는 영화의 수 이고,

1:22:38.140,1:22:41.620
행의 갯수는 factor의 수 입니다.

1:22:42.740,1:22:47.200
u 행렬에서 한명의 사용자를 색인합니다.

1:22:47.200,1:22:50.460
또, m 행렬에서 하나의 영화를 색인합니다.

1:22:50.580,1:22:54.320
이 두 색인된 벡터를 가져와서 이어 붙입니다.

1:22:54.600,1:22:58.620
즉, 이어붙여진 벡터에서 
왼쪽은 u에서, 오른쪽은 m에서 온 것이죠.

1:22:59.300,1:23:03.940
그리곤, 이것에 행렬 곱셈을 수행해 줍니다.

1:23:05.100,1:23:13.140
즉, 곱셈 대상 행렬의 열의 갯수는 
사용자 수 + 영화 수 가 되고,

1:23:14.220,1:23:18.940
행의 갯수는 우리가 정하기 나름입니다.

1:23:21.000,1:23:26.640
이 예에서는 10 이라고 해두었습니다.

1:23:27.160,1:23:30.980
그리곤, 행렬 곱셈 결과를
ReLU에 넣어줍니다.

1:23:32.000,1:23:36.000
그리곤, 또 다른 행렬과 
행렬 곱셈을 수행해 줍니다.

1:23:36.000,1:23:39.040
당연히 이번 행렬의 
열의 갯수는 10이 되어야 합니다.

1:23:39.080,1:23:42.880
그리고 행의 갯수는 1로 해줬는데,

1:23:42.880,1:23:47.360
단일-숫자인 평점을 예측하고자 하기 때문입니다.

1:23:49.720,1:23:51.620
이 그림은

1:23:51.620,1:23:55.300
내부적으로 무슨일이 일어나는지에 대한
일종의 순서도 입니다.

1:23:55.660,1:23:57.960
표준적으로는 아마도

1:23:57.960,1:24:01.535
하나의 은닉 계층을 가진 뉴럴넷이라고
불릴 수 있을것 같습니다.

1:24:01.535,1:24:04.385
사실 어떻게 생각하냐에 달린 문제로,
일종의 임베딩 계층이 있지만

1:24:04.460,1:24:07.220
이것도,
저것도 선형이어서

1:24:07.280,1:24:12.060
두 개를 가져다 놔도, 
결국 하나의 선형 계층이라는 겁니다.

1:24:12.580,1:24:15.720
어쨌든, 딱 하나의 
은닉 계층만이 있는 것입니다.

1:24:15.720,1:24:19.380
ReLU라는 비선형 계층
전에 있는 유일한 계층 입니다.

1:24:20.500,1:24:22.920
코드로 돌아가서,

1:24:22.920,1:24:25.960
선형 계층을 만들기 위해선

1:24:25.960,1:24:29.680
열의 갯수와 행의 갯수를
nn.Linear 에 파라메터로 넣어줘야 합니다.

1:24:30.120,1:24:33.580
이번주의 머신러닝 코스에선

1:24:33.580,1:24:36.520
가중치 행렬과 bias를 직접 만들어서,

1:24:36.520,1:24:40.780
어떻게 선형계층을 기초부터 
만드는법을 배웠습니다.

1:24:40.780,1:24:44.240
그 방법에 관심이 있으시면, 
확인해 보시기 바랍니다.

1:24:48.480,1:24:50.380
코드로 돌아가서,

1:24:50.600,1:24:53.635
임베딩들을 만들고,
두 개의 선형 계층을 만들었는데

1:24:53.635,1:24:56.565
이것들이 시작하기 위해
필요한 모든 것입니다.

1:24:56.565,1:24:59.565
만약 좀 더 일반화 해보고 싶다면,

1:24:59.565,1:25:05.340
nh=10 같은 파라메터를 추가해 보고
(nh: num hidden / 은닉 계층의 factor 수)

1:25:05.480,1:25:12.900
10인 부분을 nh로 치환 하면

1:25:13.100,1:25:17.260
여러 크기의 activations를 손쉽게 
실험을 해 볼 수 있습니다.

1:25:17.260,1:25:19.980
제가 이 정도 갯수의 activations을 가지고

1:25:19.980,1:25:23.560
계층을 만든다고 말할때,
(fully connected 계층인 경우)

1:25:23.560,1:25:26.000
제가 의미하는 것은

1:25:26.020,1:25:29.980
이 선형계층에 대한 가중치 행렬에
얼마나 많은 행이 존재하느냐는 것입니다.

1:25:29.980,1:25:33.820
또는, 얼마나 많은 activation을
생성하고자 하냐는 것과 같은 의미 입니다.

1:25:34.240,1:25:38.020
forward로 가보면,
(미니배치)사용자들과 영화들의 정보를 가져와서

1:25:38.020,1:25:40.795
임베딩 행렬에서 해당 하는 것을 색인하고

1:25:40.795,1:25:43.735
이 두개를 이어 붙입니다.

1:25:43.740,1:25:47.280
torch.cat(..) 이 이어 붙이는것을 
첫번째 차원을 기준으로 수행하고 있습니다.

1:25:47.280,1:25:49.860
즉, 행(컬럼) 끼리 이어 붙여서,

1:25:49.860,1:25:54.300
각 열의 길이는 더 길어지게 됩니다.
첫번째 차원을 기준으로 이어 붙인다는 의미입니다.

1:25:55.960,1:26:01.440
dropout은 잠시 후 돌아와서 설명하겠습니다.

1:26:02.600,1:26:07.140
여기까지 하고나면,
그 결과를 정의된 선형계층에 넣어주고,

1:26:07.180,1:26:10.435
선형계층의 결과를 ReLU에 다시 넣어줍니다.

1:26:10.440,1:26:14.320
근데, ReLU는 F 안에 정의되어 있기 때문에

1:26:14.320,1:26:17.680
단순히 어떤 함수로 취급되는 것을 알 수 있습니다.

1:26:17.725,1:26:20.625
활성함수는 하나의 activation을 입력받아서

1:26:20.625,1:26:23.755
변형된 activation을 출력하는
일을 하는 것이라는걸 기억해 주세요.

1:26:23.860,1:26:28.280
여기서 사용된 ReLU는
양수나 음수인 어떤 숫자를 입력받아서,

1:26:28.280,1:26:31.920
음수를 모두 0으로 만들어 버립니다.

1:26:33.380,1:26:36.500
그리곤, 마지막엔 sigmoid가 있습니다.

1:26:36.600,1:26:40.620
방금 만든 EmbeddingNet은 
참된 뉴럴넷이라고 볼 수 있습니다.

1:26:40.620,1:26:42.740
하나의 은닉 계층만 있기 때문에

1:26:42.740,1:26:45.020
딥(deep) 이라고 불러야 할지는 잘 모르겠습니다만,

1:26:45.020,1:26:47.360
뉴럴넷임에는 의심의 여지가 없습니다.

1:26:47.360,1:26:49.640
그래서, 이 뉴럴넷을 만들어서

1:26:49.640,1:26:52.115
GPU에 올리고

1:26:52.120,1:26:55.320
이를 위한 optimizer를 만들고,
fit(..)을 수행할 수 있게 됩니다.

1:26:55.960,1:26:57.100
그런데,

1:26:57.100,1:27:00.515
fit(..) 수행시 한 가지
추가적인 파라메터를 넣어 줬습니다.

1:27:00.515,1:27:03.405
이 파라메터는 최소화 하고자하는 
손실함수를 명시해준 것입니다.

1:27:03.405,1:27:07.720
mse_loss 는 Mean Squared Error Loss 이고,
이번에도 이 함수는 F 에 정의되어 있습니다.

1:27:07.720,1:27:11.360
대부분의 함수들은 F 에 정의되어 있는 것이지요.

1:27:11.700,1:27:14.980
이 함수는 fit(..) 에 전달 
해줘야만 하는 것 중 하나로,

1:27:14.980,1:27:19.140
얼마나 좋고 나쁜지에 대해
점수를 매기는 방법을 제공합니다.

1:27:19.980,1:27:23.900
>> 그러면 이제 뉴럴넷을 만들게 되었는데,

1:27:23.900,1:27:26.080
>> 사용자와 영화에 대한 임베딩은

1:27:26.080,1:27:28.320
>> 동일한 크기를 가져야만 하나요?

1:27:28.325,1:27:31.475
좋은 질문입니다.
그런데, 그럴 필요는 없습니다.

1:27:31.480,1:27:36.640
우리가 해온 것을 되돌아 보면,
많은 이점을 얻을 수 있었습니다.

1:27:37.080,1:27:39.560
한번 생각 해 보시면,

1:27:39.840,1:27:44.860
(창 띄우는 중 …)

1:27:45.240,1:27:47.720
사용자 임베딩이 있고

1:27:47.720,1:27:50.160
영화 임베딩을 이어 붙여 주는데

1:27:50.160,1:27:53.720
서로 다른 크기가 될 수도 있을 것입니다.

1:27:53.920,1:27:57.720
여기서, 영화 장르 또한 색인할 수 있다고
가정해 보면

1:27:57.720,1:28:01.280
장르에 대한 임베딩 행렬도 있을 수 있을 것입니다.

1:28:01.280,1:28:04.180
이 행렬의 열의 갯수는 장르의 수이고,

1:28:04.200,1:28:07.280
행의 갯수는 정하기 나름인데 3으로 해보겠습니다.

1:28:07.285,1:28:08.285
그러면

1:28:08.285,1:28:11.020
장르 임베딩도 이어 붙여줄 수 있을 겁니다.

1:28:11.020,1:28:13.780
마찬가지로 타임스탬프 또한
이어 붙여줄 수 있을테구요.

1:28:14.160,1:28:22.620
그러면, 이 전체를 뉴럴넷에
입력시켜줄 수 있을 것입니다.

1:28:23.140,1:28:26.680
그리고 뉴럴넷의 마지막엔,

1:28:26.680,1:28:30.335
마지막에 사용된 비선형성은
sigmoid 였었고,

1:28:30.340,1:28:36.460
그러면 이 코드(forward)의 마지막 부분을 
이해할 수 있을텐데요,

1:28:36.460,1:28:39.860
F.sigmoid(…) 의 코드 라인은 단순히

1:28:39.860,1:28:43.180
한 종류의 비선형 활성함수라는 것으로
바라볼 수 있습니다.

1:28:43.340,1:28:45.880
마지막 계층에선
(앞 계층들은 ReLU를 썼지만)

1:28:45.880,1:28:49.760
일반적으로 다른 종류의 활성함수를 사용한다는
사실을 보여주고 있습니다.

1:28:50.160,1:28:52.860
하지만, 앞서 말씀드린것 처럼

1:28:52.860,1:28:55.580
활성함수가 반드시 필요한 것은 아닙니다.

1:28:55.580,1:28:58.080
코드를 이런식으로 만들 수도 있습니다.

1:28:58.080,1:29:00.675
단지, 비선형 활성함수가 없다면

1:29:00.680,1:29:03.840
좀 더 복잡해질 뿐입니다.

1:29:03.900,1:29:07.900
그렇기 때문에, 
여기에 sigmoid를 넣어줬던 것입니다.

1:29:08.400,1:29:12.820
다시 돌아가서,
평소처럼 fit(..) 해볼 수 있습니다.

1:29:12.880,1:29:14.580
그 결과는 흥미롭게도,

1:29:14.580,1:29:21.140
직전 모델보다 더 좋은 결과를 얻을 수 있었습니다.

1:29:21.140,1:29:24.440
그렇기에, 재시작하는 확률적 경사하강을 사용해서

1:29:24.440,1:29:28.620
다시한번 학습시킨 결과를
확인 해 보는 것도 흥미로울 것 같습니다.

1:29:28.620,1:29:31.075
또는 다른 nh 값들이나

1:29:31.080,1:29:34.820
다른 dropout 정도등을 
시도해볼 수도 있을 것입니다.

1:29:34.820,1:29:39.120
그렇게 해서, 0.76보다 좀 더 나은 해답을

1:29:39.120,1:29:46.320
얻을 수 있는지 비교 가능 할 것입니다.

1:29:47.020,1:29:50.540
만약 여러분이 직장에서나 어디서든

1:29:50.540,1:29:54.260
협업 필터링을 좀 더 깊이 공부해보고 싶다면,

1:29:54.260,1:29:57.760
화면에 보고 계신 방법이 나쁘진 않을 것입니다.

1:29:57.960,1:30:02.900
fastai의 CollabFilterDataset을 시작으로,

1:30:02.900,1:30:09.480
get_learner로 learner/모델을
factor의 수 등을 지정 해서 만들고

1:30:09.540,1:30:13.015
한동안 학습을 시킬 수 있고,

1:30:13.020,1:30:16.060
몇 가지 다른 기법들을 시도해 본 후,

1:30:16.280,1:30:20.060
디폴트로 얻을 수 있는
결과를 확인해 보는 것입니다.

1:30:20.060,1:30:22.820
그러고 나서는, 어떻게 더 좋게
만들 수 있는지를 생각해 봅니다.

1:30:22.900,1:30:26.500
코드 내부를 살펴보고, 
코드 저자가 한 일은 무엇인데

1:30:26.500,1:30:30.120
내가 원하는것이 약간 다르다면
수정도 해 볼 수 있을 것입니다.

1:30:30.380,1:30:34.420
뉴럴넷을 사용한 방식이 좋은점은

1:30:34.500,1:30:37.500
한 학생분이 언급하신것 처럼,

1:30:37.500,1:30:41.360
서로다른 크기의 임베딩을 만들 수도 있고,

1:30:41.440,1:30:44.700
nh의 갯수를 고를 수도 있고,

1:30:44.700,1:30:49.120
dropout의 정도를 선택할 수도 있다는 것입니다.

1:30:49.760,1:30:53.215
이 때, dropout이 실제로 하는 일은

1:30:53.220,1:30:56.860
ReLU 활성함수 결과인 activations 중 에서

1:30:56.980,1:31:00.960
(색 선택 중 …)

1:31:01.060,1:31:06.500
무작위로 몇 개를 삭제하는 것입니다.

1:31:07.400,1:31:10.720
이 예제의 경우에선,

1:31:10.880,1:31:15.600
첫 번째 선형 계층의 결과 중
75%를 삭제 하였고,

1:31:15.600,1:31:19.175
두 번째 선형 계층의 결과 중
75%를 삭제 하였습니다.

1:31:19.180,1:31:22.920
dropout을 했다는 것은
일종의 정규화를 추가한 것입니다.

1:31:23.560,1:31:26.980
이 EmbeddingNet 이라는것에 대해서,

1:31:26.980,1:31:30.320
좀 더 유연한 dropout을 넣어줄 수 있는 기능을

1:31:30.320,1:31:36.280
전에 배운것과 가능한 비슷하게 만들고자 한다면,

1:31:36.280,1:31:40.980
생성자의 파라메터로써

1:31:40.980,1:31:46.640
ps=[0.75, 0.75] 같은 것을 추가해 볼 수 있을 것입니다.

1:31:46.940,1:31:49.720
가장 이상적인 형태의 API는 아닐지라도,

1:31:49.720,1:31:51.700
최악은 아닐 것입니다.

1:31:52.320,1:31:57.800
또는, 여기선 두 계층만이 존재하기 때문에

1:31:57.800,1:32:04.360
개별적으로 p1=0.75, p2=0.75 처럼 해 주고,

1:32:04.360,1:32:16.640
droptout 값을 이 변수들로 치환해 줄 수 있을 것입니다.

1:32:18.600,1:32:22.500
여기서 좀 더 나아가보고 싶다면,

1:32:22.500,1:32:28.940
이것을 좀 더 fastai의 구조화된 데이터 모델/learner
처럼 만들어볼 수도 있을 것입니다.

1:32:28.940,1:32:32.680
nh에 숫자가 할당되어 있지만,

1:32:32.680,1:32:35.800
숫자 대신에 리스트를 할당해 줄 수 있고, 그러면

1:32:35.860,1:32:41.160
지금 두 선형계층을 독립된 코드라인으로 적어준 것을

1:32:41.260,1:32:43.500
반복문으로 대체해서

1:32:43.500,1:32:47.000
은닉계층들을 리스트에 명시된 갯수만큼
생성해 볼 수도 있을 것입니다.

1:32:47.000,1:32:50.920
어쨋든 이 코드를 가지고 이것저것 고쳐보면서
가지고 놀아볼 수 있다는 것이지요.

1:32:50.920,1:32:55.360
그리고, 만약 협업 필터링을 위한 
데이터셋이 훨씬 크기가 작은 경우라면,

1:32:55.360,1:32:58.960
좀 더 크고, 많은 정규화 같은 것이 
필요할지도 모릅니다.

1:32:58.960,1:33:01.640
또한, 만약 훨씬 큰 데이터셋이 대상이라면

1:33:01.640,1:33:04.280
계층을 더 많이 쌓는것이 도움이 될지도
모르고 말이지요.

1:33:04.280,1:33:06.400
저도 당장은 잘 모르겠습니다.

1:33:06.400,1:33:10.075
협업 필터링에 대해서 뉴럴넷을 이용한 
이런 종류의 접근에 관련된 토론이

1:33:10.075,1:33:13.175
아직은 많이 활성화 되지 않았고,
저 또한 협업 필터링의 전문가가 아니기 때문입니다.

1:33:13.180,1:33:18.280
그러니까 한번 시도해 보시면, 
흥미로운 결과를 얻으실 수 있을지도 모릅니다.

1:33:21.860,1:33:25.360
이 다음으로는 다루려는 내용은

1:33:25.360,1:33:28.780
학습 반복문에 대산 것입니다.

1:33:28.780,1:33:33.280
학습 반복문에서 실제론
어떤일이 일어나고 있는걸까요?

1:33:33.900,1:33:37.260
일단은 그 전에 살펴봐야 할 것이 있는데

1:33:37.320,1:33:40.100
가중치의 실제 업데이트내용을

1:33:40.100,1:33:44.700
PyTorch의 optimizer로 
전달해 준 부분이 있는데

1:33:44.700,1:33:48.560
특히 그 중,
optimizer라는 것이 실제로

1:33:48.560,1:33:51.480
무엇을 하는지 이해해 보려고 합니다.

1:33:51.620,1:33:56.520
또한, 여기 사용된
momentum이 무엇인지도 이해해 보려고 합니다.

1:33:57.260,1:34:01.700
이를 위해서, 작성된 엑셀 파일이 있는데
graddesc.xlsm 라는 이름입니다.

1:34:01.700,1:34:04.860
이 파일은 경사하강에 대한내용을 담고 있습니다.

1:34:04.860,1:34:09.980
그리고, 이 파일의 워크시트들은
오른쪽에서 왼쪽으로 읽도록 디자인 되어 있습니다.

1:34:10.660,1:34:13.260
우선, 가장 오른쪽의 워크시트로 가 보면,

1:34:13.260,1:34:16.160
몇 개의 데이터가 있습니다. 

1:34:16.160,1:34:21.180
우리는 이 데이터들에 대해서,
엑셀을 이용하여 경사하강을 구현해 볼 것입니다.

1:34:21.180,1:34:25.095
언제나 그렇듯, 협업필터링도 엑셀에서 했었고
CNN도 엑셀에서 한 것 처럼

1:34:25.100,1:34:28.520
이번엔 SGD (Stochastic Gradient Descent, 
확률적 경사하강)도 엑셀로 해보는 것입니다.

1:34:28.520,1:34:32.080
그리고나선, 그 내용을 
Python으로 대체 해 보는 순서로 진행될 것입니다.

1:34:32.080,1:34:35.320
일단 몇 개의 데이터들을 생성해 두었습니다.

1:34:35.320,1:34:37.615
여기 보시면,

1:34:37.620,1:34:41.760
왼쪽 컬럼을 비.의존성 변수인

1:34:41.760,1:34:43.755
x 로서,

1:34:43.760,1:34:47.040
오른쪽 컬럼을 의존성 변수인 
y로서 만들어 두었는데,

1:34:47.040,1:34:50.300
이 두 컬럼의 값들은 선형적으로
직접적인 연관성을 가집니다.

1:34:50.300,1:34:53.700
x의 값들은 랜덤하게 생성된 것이고,

1:34:53.700,1:35:00.080
y의 값들은 (x*2)+30
라는 공식에 의해서 채워집니다.

1:35:01.040,1:35:06.520
그러면 엑셀을 이용해서

1:35:06.620,1:35:10.140
이 데이터들을 가지고, 

1:35:10.180,1:35:14.520
이 const와 slope라는 파라메터들을
학습할 수 있도록 해 봅시다.

1:35:17.620,1:35:22.160
일단은 가장 기초적인 버전의
SGD를 가지고 시작해 봅시다.

1:35:22.160,1:35:25.240
우선은 미리 제가 작성 해 둔 매크로를 수행해서, 

1:35:25.240,1:35:27.155
그 결과를 먼저 확인해 봅시다.

1:35:27.160,1:35:28.620
Run 을 누르면,

1:35:28.620,1:35:31.140
5 번의 에포크가 진행됩니다.

1:35:31.140,1:35:33.920
다시 Run을 눌러서, 추가 5 에포크를 더 수행하고

1:35:33.920,1:35:36.860
또 다시 5 에포크를 추가 수행하겠습니다.

1:35:36.860,1:35:39.515
처음의 결과는 꽤나 좋지 않기 때문에,

1:35:39.520,1:35:44.280
좀 더 그래프를 나이스하게 
그리기 위해서 삭제하도록 하겠습니다.

1:35:44.720,1:35:47.140
보시는 것 처럼

1:35:47.145,1:35:50.445
꽤나 지속적으로
손실이 향상되고 있는 것을 알 수 있습니다.

1:35:50.445,1:35:53.200
이 그래프는 각 에포크 마다의 손실값을 나타냅니다.

1:35:53.200,1:35:55.620
어떻게 이런 결과를 얻었을까요?

1:35:55.620,1:35:58.160
일단 결과를 다시 Reset 시키도록 하겠습니다.

1:35:58.160,1:36:02.460
가장 왼쪽을 보시면, x와 y 값들이 있습니다.

1:36:02.620,1:36:08.880
그리고 초기엔 임의로 
y 절편과 기울기를 각각 1, 1로 가정 하였습니다.

1:36:08.920,1:36:13.260
이 값들은 무작위로 초기화된 가중치 입니다.

1:36:13.260,1:36:17.375
따라서, 무작위로 초기화된
가중치가 1과 1인 것입니다.

1:36:17.380,1:36:22.320
따라서, 다른 랜덤한 숫자를 고르셔도 상관 없습니다.

1:36:22.320,1:36:27.120
저 또한 무작위로 숫자를 골랐다는걸 약속드립니다.

1:36:28.480,1:36:30.680


1:36:32.445,1:36:35.325


1:36:35.325,1:36:38.475


1:36:38.475,1:36:41.665


1:36:41.705,1:36:44.805


1:36:44.805,1:36:48.055


1:36:48.335,1:36:50.535


1:36:51.285,1:36:53.895


1:36:53.895,1:36:56.885


1:36:57.225,1:36:58.225


1:36:58.375,1:36:59.835


1:37:00.385,1:37:03.425


1:37:03.425,1:37:06.325


1:37:06.325,1:37:09.205


1:37:09.205,1:37:11.455


1:37:11.835,1:37:14.875


1:37:14.875,1:37:17.865


1:37:17.865,1:37:21.075


1:37:21.075,1:37:22.755


1:37:23.535,1:37:26.405


1:37:26.405,1:37:29.515


1:37:29.665,1:37:31.035


1:37:32.605,1:37:35.715


1:37:36.035,1:37:37.055


1:37:37.565,1:37:40.505


1:37:40.505,1:37:42.165


1:37:43.395,1:37:46.275


1:37:46.275,1:37:49.155


1:37:49.155,1:37:52.125


1:37:52.505,1:37:55.475


1:37:55.475,1:37:58.445


1:37:58.475,1:38:00.975


1:38:01.835,1:38:03.965


1:38:04.705,1:38:07.945


1:38:08.285,1:38:10.945


1:38:10.945,1:38:12.745


1:38:14.235,1:38:17.095


1:38:17.245,1:38:18.535


1:38:18.895,1:38:21.795


1:38:21.795,1:38:24.745


1:38:24.745,1:38:27.685


1:38:27.685,1:38:30.815


1:38:30.815,1:38:31.815


1:38:32.095,1:38:35.375


1:38:35.805,1:38:38.795


1:38:39.155,1:38:40.225


1:38:40.575,1:38:43.475


1:38:43.495,1:38:46.615


1:38:46.615,1:38:49.635


1:38:49.635,1:38:52.645


1:38:52.645,1:38:55.665


1:38:55.665,1:38:56.665


1:38:58.515,1:39:01.605


1:39:01.605,1:39:04.945


1:39:04.985,1:39:07.835


1:39:07.835,1:39:10.915


1:39:10.915,1:39:13.645


1:39:14.145,1:39:17.045


1:39:17.045,1:39:18.815


1:39:19.395,1:39:22.565


1:39:22.995,1:39:25.805


1:39:26.225,1:39:27.975


1:39:28.285,1:39:31.235


1:39:31.235,1:39:34.085


1:39:34.085,1:39:36.995


1:39:36.995,1:39:40.045


1:39:40.045,1:39:43.155


1:39:43.705,1:39:46.745


1:39:46.745,1:39:49.885


1:39:49.955,1:39:52.995


1:39:52.995,1:39:55.205


1:39:55.205,1:39:57.995


1:39:57.995,1:40:00.825


1:40:00.825,1:40:03.425


1:40:03.425,1:40:06.345


1:40:06.345,1:40:09.355


1:40:09.355,1:40:11.525


1:40:12.395,1:40:15.315


1:40:15.315,1:40:18.135


1:40:18.135,1:40:20.935


1:40:20.935,1:40:23.935


1:40:23.935,1:40:26.775


1:40:26.795,1:40:29.965


1:40:29.965,1:40:33.005


1:40:33.005,1:40:36.285


1:40:36.285,1:40:39.335


1:40:39.335,1:40:42.835


1:40:43.385,1:40:46.205


1:40:46.915,1:40:49.715


1:40:49.715,1:40:51.935


1:40:51.935,1:40:53.385


1:40:54.665,1:40:57.965


1:40:57.965,1:41:00.955


1:41:00.955,1:41:03.995


1:41:03.995,1:41:06.035


1:41:06.615,1:41:09.735


1:41:09.735,1:41:10.985


1:41:11.385,1:41:14.385


1:41:14.385,1:41:17.395


1:41:17.395,1:41:20.285


1:41:20.285,1:41:23.215


1:41:23.215,1:41:24.315


1:41:24.845,1:41:27.885


1:41:27.915,1:41:30.055


1:41:30.405,1:41:33.315


1:41:33.505,1:41:35.105


1:41:35.555,1:41:38.525


1:41:38.525,1:41:40.395


1:41:40.755,1:41:41.755


1:41:42.165,1:41:43.165


1:41:43.545,1:41:45.065


1:41:46.735,1:41:49.685


1:41:49.685,1:41:52.065


1:41:52.235,1:41:55.145


1:41:55.145,1:41:57.125


1:41:59.035,1:42:02.265


1:42:02.265,1:42:05.205


1:42:05.205,1:42:08.095


1:42:08.225,1:42:09.745


1:42:10.945,1:42:13.965


1:42:13.965,1:42:15.525


1:42:17.055,1:42:18.575


1:42:19.435,1:42:22.565


1:42:22.565,1:42:25.965


1:42:25.995,1:42:28.905


1:42:30.515,1:42:33.535


1:42:33.535,1:42:34.535


1:42:34.575,1:42:37.565


1:42:37.565,1:42:39.115


1:42:39.595,1:42:42.565


1:42:42.565,1:42:45.965


1:42:45.965,1:42:46.965


1:42:47.095,1:42:49.885


1:42:49.905,1:42:52.325


1:42:52.325,1:42:55.335


1:42:55.335,1:42:58.605


1:42:58.805,1:43:00.865


1:43:01.575,1:43:04.145


1:43:04.145,1:43:07.205


1:43:07.205,1:43:10.145


1:43:10.145,1:43:13.175


1:43:13.275,1:43:16.465


1:43:16.565,1:43:18.165


1:43:18.625,1:43:21.465


1:43:21.465,1:43:24.525


1:43:24.525,1:43:27.575


1:43:27.575,1:43:30.475


1:43:30.475,1:43:31.475


1:43:31.955,1:43:34.485


1:43:35.285,1:43:38.325


1:43:38.325,1:43:41.435


1:43:41.565,1:43:44.375


1:43:44.375,1:43:47.125


1:43:47.125,1:43:48.885


1:43:48.885,1:43:52.365


1:43:52.665,1:43:55.755


1:43:56.065,1:43:58.815


1:43:58.815,1:44:01.835


1:44:01.835,1:44:02.835


1:44:03.695,1:44:04.695


1:44:05.425,1:44:08.205


1:44:08.845,1:44:11.755


1:44:12.615,1:44:15.645


1:44:15.645,1:44:17.935


1:44:20.305,1:44:21.605


1:44:23.855,1:44:26.755


1:44:26.755,1:44:29.865


1:44:30.155,1:44:32.875


1:44:32.875,1:44:35.725


1:44:35.725,1:44:36.725


1:44:37.295,1:44:38.825


1:44:39.165,1:44:41.745


1:44:41.745,1:44:44.865


1:44:44.865,1:44:46.645


1:44:47.105,1:44:49.395


1:44:51.105,1:44:54.145


1:44:54.145,1:44:57.055


1:44:57.055,1:44:59.975


1:44:59.975,1:45:02.875


1:45:02.875,1:45:05.795


1:45:05.795,1:45:08.875


1:45:08.875,1:45:11.735


1:45:11.865,1:45:14.815


1:45:14.815,1:45:17.685


1:45:17.685,1:45:20.565


1:45:20.565,1:45:22.735


1:45:22.735,1:45:25.025


1:45:25.025,1:45:26.475


1:45:27.695,1:45:30.875


1:45:30.875,1:45:33.765


1:45:33.765,1:45:35.365


1:45:35.705,1:45:38.755


1:45:38.755,1:45:41.705


1:45:41.705,1:45:44.725


1:45:44.725,1:45:47.895


1:45:47.895,1:45:50.255


1:45:50.765,1:45:53.815


1:45:53.855,1:45:57.255


1:45:57.335,1:45:58.555


1:45:59.625,1:46:02.705


1:46:02.985,1:46:04.035


1:46:04.405,1:46:07.255


1:46:07.255,1:46:09.405


1:46:09.925,1:46:11.315


1:46:12.035,1:46:14.985


1:46:14.985,1:46:17.405


1:46:17.765,1:46:20.525


1:46:20.525,1:46:21.525


1:46:21.765,1:46:23.275


1:46:23.695,1:46:26.495


1:46:27.295,1:46:30.535


1:46:30.625,1:46:31.625


1:46:32.805,1:46:35.925


1:46:37.445,1:46:40.215


1:46:41.105,1:46:44.035


1:46:44.035,1:46:47.445


1:46:47.805,1:46:50.445


1:46:52.245,1:46:53.395


1:46:53.995,1:46:54.995


1:46:56.225,1:46:57.225


1:46:57.455,1:46:58.905


1:47:00.655,1:47:03.805


1:47:03.805,1:47:05.905


1:47:06.425,1:47:09.135


1:47:09.135,1:47:11.845


1:47:11.845,1:47:15.055


1:47:15.105,1:47:18.095


1:47:18.095,1:47:21.145


1:47:21.145,1:47:24.505


1:47:24.555,1:47:27.315


1:47:27.315,1:47:29.305


1:47:29.615,1:47:30.895


1:47:32.475,1:47:33.475


1:47:34.085,1:47:35.085


1:47:36.195,1:47:37.195


1:47:38.015,1:47:40.995


1:47:40.995,1:47:41.995


1:47:43.785,1:47:46.815


1:47:46.815,1:47:48.245


1:47:50.045,1:47:51.335


1:47:52.325,1:47:53.665


1:47:54.345,1:47:56.865


1:47:56.865,1:47:57.905


1:47:58.535,1:48:01.225


1:48:01.225,1:48:04.265


1:48:04.265,1:48:06.895


1:48:07.365,1:48:10.005


1:48:10.005,1:48:12.895


1:48:13.305,1:48:14.875


1:48:15.685,1:48:18.555


1:48:18.555,1:48:20.755


1:48:21.135,1:48:24.075


1:48:24.075,1:48:27.175


1:48:27.175,1:48:29.945


1:48:29.945,1:48:32.985


1:48:32.985,1:48:35.925


1:48:35.925,1:48:38.965


1:48:38.965,1:48:41.975


1:48:41.975,1:48:44.555


1:48:44.555,1:48:47.445


1:48:47.445,1:48:50.245


1:48:50.245,1:48:53.305


1:48:53.305,1:48:56.355


1:48:57.175,1:49:00.445


1:49:00.715,1:49:03.455


1:49:03.455,1:49:06.465


1:49:06.465,1:49:09.535


1:49:09.535,1:49:12.615


1:49:12.615,1:49:15.065


1:49:15.065,1:49:17.945


1:49:18.145,1:49:21.085


1:49:21.085,1:49:23.655


1:49:23.655,1:49:25.995


1:49:25.995,1:49:29.245


1:49:29.625,1:49:32.545


1:49:32.545,1:49:35.565


1:49:35.565,1:49:37.525


1:49:37.845,1:49:38.845


1:49:39.095,1:49:42.225


1:49:42.225,1:49:45.215


1:49:45.215,1:49:47.705


1:49:47.705,1:49:50.745


1:49:50.745,1:49:53.905


1:49:53.905,1:49:56.405


1:49:56.405,1:49:59.235


1:49:59.235,1:50:01.965


1:50:02.195,1:50:05.095


1:50:05.095,1:50:07.775


1:50:08.285,1:50:09.285


1:50:09.575,1:50:12.595


1:50:12.595,1:50:14.665


1:50:15.135,1:50:18.485


1:50:18.485,1:50:21.615


1:50:21.615,1:50:24.635


1:50:24.635,1:50:27.535


1:50:27.535,1:50:30.775


1:50:30.885,1:50:34.085


1:50:36.315,1:50:38.645


1:50:39.805,1:50:41.025


1:50:42.375,1:50:45.345


1:50:45.345,1:50:48.505


1:50:49.405,1:50:50.965


1:50:51.425,1:50:54.205


1:50:54.205,1:50:55.205


1:50:55.745,1:50:57.165


1:50:58.615,1:51:01.495


1:51:01.495,1:51:03.685


1:51:03.685,1:51:06.695


1:51:06.695,1:51:08.275


1:51:08.725,1:51:12.105


1:51:12.165,1:51:13.475


1:51:13.805,1:51:16.455


1:51:16.455,1:51:18.145


1:51:18.595,1:51:19.595


1:51:19.775,1:51:20.775


1:51:21.145,1:51:24.375


1:51:25.505,1:51:28.565


1:51:28.565,1:51:29.565


1:51:30.115,1:51:33.435


1:51:34.745,1:51:36.115


1:51:36.705,1:51:39.535


1:51:40.155,1:51:42.565


1:51:42.675,1:51:43.885


1:51:44.865,1:51:47.325


1:51:48.315,1:51:49.545


1:51:50.175,1:51:51.155


1:51:51.155,1:51:52.885


1:51:54.165,1:51:55.165


1:51:55.415,1:51:57.205


1:51:57.755,1:51:59.875


1:52:00.545,1:52:03.945


1:52:04.915,1:52:07.525


1:52:08.135,1:52:09.135


1:52:09.695,1:52:10.695


1:52:11.035,1:52:14.025


1:52:14.025,1:52:17.145


1:52:17.145,1:52:19.675


1:52:19.675,1:52:20.825


1:52:21.585,1:52:24.715


1:52:25.405,1:52:28.355


1:52:28.355,1:52:31.065


1:52:31.065,1:52:34.065


1:52:34.065,1:52:35.715


1:52:36.135,1:52:38.955


1:52:39.055,1:52:42.065


1:52:42.065,1:52:45.055


1:52:45.055,1:52:47.985


1:52:47.985,1:52:50.865


1:52:51.215,1:52:52.215


1:52:52.505,1:52:55.645


1:52:56.585,1:52:59.685


1:52:59.685,1:53:01.475


1:53:02.135,1:53:05.015


1:53:05.015,1:53:07.865


1:53:07.865,1:53:11.305


1:53:11.795,1:53:14.815


1:53:14.815,1:53:17.625


1:53:17.935,1:53:18.935


1:53:19.415,1:53:22.445


1:53:22.445,1:53:23.445


1:53:23.965,1:53:25.625


1:53:29.385,1:53:32.095


1:53:32.095,1:53:35.105


1:53:35.105,1:53:38.345


1:53:38.655,1:53:40.535


1:53:41.755,1:53:44.265


1:53:48.385,1:53:50.805


1:53:51.935,1:53:54.625


1:53:55.265,1:53:58.315


1:53:58.315,1:54:01.305


1:54:01.835,1:54:04.305


1:54:05.065,1:54:06.315


1:54:06.675,1:54:07.705


1:54:08.455,1:54:09.855


1:54:10.855,1:54:13.935


1:54:14.175,1:54:17.155


1:54:17.155,1:54:20.165


1:54:20.165,1:54:22.965


1:54:22.965,1:54:25.575


1:54:25.575,1:54:28.845


1:54:28.845,1:54:32.305


1:54:32.305,1:54:34.765


1:54:35.445,1:54:36.445


1:54:36.745,1:54:37.745


1:54:38.045,1:54:40.975


1:54:44.205,1:54:46.895


1:54:46.895,1:54:49.835


1:54:49.835,1:54:52.675


1:54:53.525,1:54:55.925


1:54:57.165,1:54:59.965


1:54:59.965,1:55:01.435


1:55:02.085,1:55:04.385


1:55:06.485,1:55:09.945


1:55:10.685,1:55:11.685


1:55:13.405,1:55:16.235


1:55:16.235,1:55:17.235


1:55:17.645,1:55:18.965


1:55:19.945,1:55:20.945


1:55:21.315,1:55:23.945


1:55:23.945,1:55:26.665


1:55:26.885,1:55:29.795


1:55:29.795,1:55:32.645


1:55:32.645,1:55:33.775


1:55:34.135,1:55:36.905


1:55:37.805,1:55:40.845


1:55:40.845,1:55:43.795


1:55:43.795,1:55:46.865


1:55:46.865,1:55:49.865


1:55:49.865,1:55:52.485


1:55:53.455,1:55:56.565


1:55:56.845,1:55:58.475


1:55:58.875,1:56:01.805


1:56:01.805,1:56:04.995


1:56:07.355,1:56:10.195


1:56:11.015,1:56:14.085


1:56:14.085,1:56:16.975


1:56:16.975,1:56:20.175


1:56:20.175,1:56:23.095


1:56:23.095,1:56:26.075


1:56:26.075,1:56:28.655


1:56:28.655,1:56:31.685


1:56:31.685,1:56:34.645


1:56:34.645,1:56:37.635


1:56:37.635,1:56:40.635


1:56:40.635,1:56:43.735


1:56:43.735,1:56:46.715


1:56:46.715,1:56:49.725


1:56:49.725,1:56:53.085


1:56:56.575,1:56:59.435


1:56:59.435,1:57:02.465


1:57:03.725,1:57:06.235


1:57:06.235,1:57:08.695


1:57:09.125,1:57:11.605


1:57:11.605,1:57:12.945


1:57:18.195,1:57:19.195


1:57:19.765,1:57:20.885


1:57:26.705,1:57:29.795


1:57:29.795,1:57:32.825


1:57:33.105,1:57:34.725


1:57:36.115,1:57:39.165


1:57:39.165,1:57:42.235


1:57:42.235,1:57:45.085


1:57:45.085,1:57:46.915


1:57:47.615,1:57:50.395


1:57:50.395,1:57:53.505


1:57:53.505,1:57:56.465


1:57:56.465,1:57:59.465


1:57:59.465,1:58:02.305


1:58:02.305,1:58:05.315


1:58:05.315,1:58:08.335


1:58:08.335,1:58:11.515


1:58:11.515,1:58:14.385


1:58:14.385,1:58:17.605


1:58:17.875,1:58:19.585


1:58:19.925,1:58:20.925


1:58:21.025,1:58:23.865


1:58:23.865,1:58:26.855


1:58:26.855,1:58:29.175


1:58:29.925,1:58:30.925


1:58:31.115,1:58:32.115


1:58:34.015,1:58:35.015


1:58:37.825,1:58:38.825


1:58:38.845,1:58:41.965


1:58:41.965,1:58:45.005


1:58:45.005,1:58:48.105


1:58:48.105,1:58:50.675


1:58:51.845,1:58:53.565


1:58:53.895,1:58:54.895


1:58:55.045,1:58:56.735


1:58:57.345,1:58:59.485


1:58:59.635,1:59:01.225


1:59:04.165,1:59:06.805


1:59:11.735,1:59:14.845


1:59:15.125,1:59:16.125


1:59:17.405,1:59:20.315


1:59:20.315,1:59:23.545


1:59:23.545,1:59:25.545


1:59:25.545,1:59:28.485


1:59:28.485,1:59:31.525


1:59:31.525,1:59:32.535


1:59:33.185,1:59:35.855


1:59:35.855,1:59:38.465


1:59:38.465,1:59:41.205


1:59:41.205,1:59:44.255


1:59:44.255,1:59:47.335


1:59:47.335,1:59:49.975


1:59:49.975,1:59:53.045


1:59:53.045,1:59:55.935


1:59:55.935,1:59:58.165


1:59:58.165,2:00:01.245


2:00:01.245,2:00:02.245


2:00:02.825,2:00:05.795


2:00:05.795,2:00:09.205


2:00:09.845,2:00:12.725


2:00:12.725,2:00:14.825


2:00:14.825,2:00:16.995


2:00:16.995,2:00:20.245


2:00:20.245,2:00:23.425


2:00:23.425,2:00:26.325


2:00:26.325,2:00:29.375


2:00:29.375,2:00:31.805


2:00:31.805,2:00:34.695


2:00:34.915,2:00:37.945


2:00:37.945,2:00:40.945


2:00:40.945,2:00:43.995


2:00:43.995,2:00:46.995


2:00:47.075,2:00:50.165


2:00:50.165,2:00:53.185


2:00:53.185,2:00:55.265


2:00:55.995,2:00:56.995


2:00:57.305,2:01:00.155


2:01:00.155,2:01:02.915


2:01:02.915,2:01:06.305


2:01:06.305,2:01:08.315


2:01:08.525,2:01:11.295


2:01:11.655,2:01:14.565


2:01:14.565,2:01:17.645


2:01:17.645,2:01:20.865


2:01:20.865,2:01:24.205


2:01:24.275,2:01:27.135


2:01:27.135,2:01:30.595


2:01:31.055,2:01:34.225


2:01:34.225,2:01:36.925


2:01:38.455,2:01:40.595


2:01:41.025,2:01:42.025


2:01:42.115,2:01:44.315


2:01:45.615,2:01:48.425


2:01:48.425,2:01:50.725


2:01:51.565,2:01:52.915


2:01:53.855,2:01:55.375


2:01:55.905,2:01:58.945


2:01:58.945,2:02:01.975


2:02:01.975,2:02:04.825


2:02:05.015,2:02:07.665


2:02:07.665,2:02:10.065


2:02:11.525,2:02:14.415


2:02:14.415,2:02:15.525


2:02:17.815,2:02:20.735


2:02:20.735,2:02:23.255


2:02:23.255,2:02:26.505


2:02:27.045,2:02:29.565


2:02:30.305,2:02:32.085


2:02:32.605,2:02:35.575


2:02:35.575,2:02:37.595


2:02:39.865,2:02:42.955


2:02:42.955,2:02:43.955


2:02:44.835,2:02:47.785


2:02:47.785,2:02:50.075


2:02:50.515,2:02:53.525


2:02:53.525,2:02:55.995


2:02:57.475,2:02:59.245


2:02:59.565,2:03:02.505


2:03:02.505,2:03:03.505


2:03:04.555,2:03:07.825


2:03:08.225,2:03:11.495


2:03:11.625,2:03:14.635


2:03:14.635,2:03:18.025


2:03:18.635,2:03:21.455


2:03:21.455,2:03:24.395


2:03:24.395,2:03:25.395


2:03:25.815,2:03:28.805


2:03:28.805,2:03:30.615


2:03:30.615,2:03:33.745


2:03:33.745,2:03:36.735


2:03:36.735,2:03:39.945


2:03:39.945,2:03:42.095


2:03:42.095,2:03:45.005


2:03:45.005,2:03:48.245


2:03:48.245,2:03:49.325


2:03:49.695,2:03:51.545


2:04:00.885,2:04:02.525


2:04:03.735,2:04:06.605


2:04:06.605,2:04:09.595


2:04:09.595,2:04:12.165


2:04:12.165,2:04:15.235


2:04:15.235,2:04:18.105


2:04:18.175,2:04:20.795


2:04:20.945,2:04:24.105


2:04:24.105,2:04:27.105


2:04:27.105,2:04:30.565


2:04:30.825,2:04:34.195


2:04:34.395,2:04:37.415


2:04:37.415,2:04:40.295


2:04:40.295,2:04:43.685


2:04:43.725,2:04:46.455


2:04:46.455,2:04:49.605


2:04:49.605,2:04:53.035


2:04:53.245,2:04:55.685


2:04:55.685,2:04:58.605


2:04:58.605,2:05:01.525


2:05:01.525,2:05:04.375


2:05:04.375,2:05:07.055


2:05:07.785,2:05:10.545


2:05:10.545,2:05:13.385


2:05:13.385,2:05:15.925


2:05:16.535,2:05:19.435


2:05:19.435,2:05:22.425


2:05:22.425,2:05:25.335


2:05:25.335,2:05:28.365


2:05:28.365,2:05:31.405


2:05:31.405,2:05:34.515


2:05:34.515,2:05:37.595


2:05:37.595,2:05:40.705


2:05:40.705,2:05:43.735


2:05:43.735,2:05:46.715


2:05:46.875,2:05:49.535


2:05:50.045,2:05:53.495


2:05:54.265,2:05:56.855


2:05:56.855,2:06:00.035


2:06:00.035,2:06:03.015


2:06:03.015,2:06:05.115


2:06:05.485,2:06:08.025


2:06:09.045,2:06:11.815


2:06:11.865,2:06:14.925


2:06:14.925,2:06:17.555


2:06:17.555,2:06:20.595


2:06:20.595,2:06:23.585


2:06:23.735,2:06:27.005


2:06:27.225,2:06:28.225


2:06:28.905,2:06:31.555


2:06:32.595,2:06:36.095


2:06:36.485,2:06:39.475


2:06:39.605,2:06:42.165


2:06:42.165,2:06:45.215


2:06:45.215,2:06:47.195


2:06:47.785,2:06:49.355


2:06:49.865,2:06:50.865


2:06:50.955,2:06:53.925


2:06:53.925,2:06:56.875


2:06:56.875,2:06:59.865


2:06:59.865,2:07:02.425


2:07:02.995,2:07:06.085


2:07:06.085,2:07:08.845


2:07:08.845,2:07:12.225


2:07:12.225,2:07:15.105


2:07:15.105,2:07:17.845


2:07:17.845,2:07:18.845


2:07:19.265,2:07:22.295


2:07:22.295,2:07:24.975


2:07:24.975,2:07:27.935


2:07:27.935,2:07:31.205


2:07:31.205,2:07:33.695


2:07:33.695,2:07:34.825


2:07:35.815,2:07:37.465


2:07:37.805,2:07:40.085


2:07:40.325,2:07:42.165


2:07:42.495,2:07:45.705


2:07:45.705,2:07:49.085


2:07:49.695,2:07:52.855


2:07:52.855,2:07:55.845


2:07:55.845,2:07:58.815


2:07:58.845,2:08:00.545


2:08:00.955,2:08:04.365


2:08:04.365,2:08:05.915


2:08:06.775,2:08:09.635


2:08:09.635,2:08:12.515


2:08:12.515,2:08:15.545


2:08:15.545,2:08:18.195


2:08:18.195,2:08:21.105


2:08:21.105,2:08:24.285


2:08:24.285,2:08:27.675


2:08:27.895,2:08:30.595


2:08:30.595,2:08:33.095


2:08:33.585,2:08:36.885


2:08:37.125,2:08:40.035


2:08:40.035,2:08:43.115


2:08:43.115,2:08:44.695


2:08:45.245,2:08:48.315


2:08:48.315,2:08:51.305


2:08:51.305,2:08:54.395


2:08:54.395,2:08:57.305


2:08:57.305,2:08:59.915


2:08:59.915,2:09:03.415


2:09:03.695,2:09:06.675


2:09:06.675,2:09:09.375


2:09:09.375,2:09:12.365


2:09:12.365,2:09:14.385


2:09:14.825,2:09:16.835


2:09:20.295,2:09:22.275


2:09:22.275,2:09:25.295


2:09:25.295,2:09:27.145


2:09:28.035,2:09:31.005


2:09:31.005,2:09:34.255


2:09:34.265,2:09:37.425


2:09:37.425,2:09:38.475


2:09:38.965,2:09:42.005


2:09:42.005,2:09:43.005


2:09:43.485,2:09:46.435


2:09:46.435,2:09:49.465


2:09:49.465,2:09:51.845


2:09:51.845,2:09:54.715


2:09:55.235,2:09:58.115


2:09:58.115,2:09:59.115


2:09:59.645,2:10:02.615


2:10:02.615,2:10:05.635


2:10:05.635,2:10:06.635


2:10:07.615,2:10:10.655


2:10:10.725,2:10:13.495


2:10:14.655,2:10:18.155


2:10:18.505,2:10:21.575


2:10:21.575,2:10:23.545


2:10:23.875,2:10:26.805


2:10:26.805,2:10:29.025


2:10:29.025,2:10:32.345


2:10:32.345,2:10:35.445


2:10:35.445,2:10:38.115


2:10:38.115,2:10:40.895


2:10:41.505,2:10:43.905


2:10:44.485,2:10:47.425


2:10:47.425,2:10:48.945


2:10:49.435,2:10:50.965


2:10:52.095,2:10:54.515


2:10:54.515,2:10:57.205


2:10:57.205,2:11:00.685


2:11:00.965,2:11:03.825


2:11:04.095,2:11:07.145


2:11:07.145,2:11:08.185


2:11:09.495,2:11:10.495


2:11:12.445,2:11:13.535


2:11:15.335,2:11:17.885


2:11:18.195,2:11:20.875


2:11:20.875,2:11:23.605


2:11:23.605,2:11:25.445


2:11:25.955,2:11:28.835


2:11:28.835,2:11:31.935


2:11:32.295,2:11:35.705


2:11:36.015,2:11:37.015


2:11:37.205,2:11:40.355


2:11:40.445,2:11:43.415


2:11:43.415,2:11:44.615


2:11:45.725,2:11:48.855


2:11:48.855,2:11:51.845


2:11:51.845,2:11:54.835


2:11:54.835,2:11:58.055


2:11:58.055,2:12:01.135


2:12:01.135,2:12:03.475


2:12:03.475,2:12:06.525


2:12:06.525,2:12:09.415


2:12:09.415,2:12:11.605


2:12:12.145,2:12:14.515


2:12:14.515,2:12:17.255


2:12:17.255,2:12:20.145


2:12:20.145,2:12:23.225


2:12:23.225,2:12:25.745


2:12:25.745,2:12:26.745


2:12:27.225,2:12:29.925


2:12:29.925,2:12:32.955


2:12:32.955,2:12:34.295


2:12:34.675,2:12:37.475


2:12:37.475,2:12:40.295


2:12:40.295,2:12:43.365


2:12:43.635,2:12:45.105


2:12:46.265,2:12:47.445


2:12:47.955,2:12:48.955


2:12:50.085,2:12:51.085


2:12:52.015,2:12:53.295


2:12:53.665,2:12:56.915


2:12:56.915,2:12:57.915


2:12:58.445,2:13:00.875


2:13:02.905,2:13:06.155


2:13:06.185,2:13:08.795


2:13:08.795,2:13:09.795


2:13:10.105,2:13:13.305


2:13:13.305,2:13:16.525


2:13:16.525,2:13:19.305


2:13:19.305,2:13:22.565


2:13:22.565,2:13:25.635


2:13:25.635,2:13:28.375


2:13:28.375,2:13:31.255


2:13:31.255,2:13:34.265


2:13:34.265,2:13:37.275


2:13:37.275,2:13:38.275


2:13:38.925,2:13:41.935


2:13:41.935,2:13:43.205


2:13:43.835,2:13:44.835


2:13:45.635,2:13:46.635


2:13:46.635,2:13:49.415


2:13:49.415,2:13:50.945


2:13:51.255,2:13:54.255


2:13:54.255,2:13:57.295


2:13:57.295,2:13:59.735


2:14:00.135,2:14:03.565


2:14:03.585,2:14:06.605


2:14:06.605,2:14:07.555


2:14:07.555,2:14:09.545


2:14:10.175,2:14:13.255


2:14:13.305,2:14:16.085


2:14:16.085,2:14:19.165


2:14:19.165,2:14:22.165


2:14:22.255,2:14:24.865


2:14:24.865,2:14:26.605


2:14:28.045,2:14:30.995


2:14:30.995,2:14:34.045


2:14:34.425,2:14:35.425


2:14:35.455,2:14:38.185


2:14:38.185,2:14:40.675


2:14:40.795,2:14:43.835


2:14:43.835,2:14:44.835


2:14:45.395,2:14:47.965


2:14:48.705,2:14:49.705


2:14:50.485,2:14:53.595


2:14:53.595,2:14:56.605


2:14:56.605,2:14:57.605


2:14:57.795,2:15:00.615


2:15:00.615,2:15:03.435


2:15:03.945,2:15:07.065


2:15:07.105,2:15:09.965


2:15:09.965,2:15:13.265


2:15:13.265,2:15:16.195


2:15:16.195,2:15:18.685


2:15:19.205,2:15:21.575


2:15:21.575,2:15:24.725


2:15:24.725,2:15:28.185


2:15:28.185,2:15:30.945


2:15:30.945,2:15:33.895


2:15:33.895,2:15:36.415


2:15:36.415,2:15:39.385


2:15:39.385,2:15:42.395


2:15:42.395,2:15:45.215


2:15:45.215,2:15:46.315


2:15:47.205,2:15:50.015


2:15:50.015,2:15:51.625


2:15:51.625,2:15:54.155


2:15:54.155,2:15:55.765


2:15:56.215,2:15:59.265


2:15:59.435,2:16:02.345


2:16:02.345,2:16:04.835


2:16:07.445,2:16:10.475


2:16:10.475,2:16:13.365


2:16:13.365,2:16:14.365


2:16:14.835,2:16:17.555


2:16:17.555,2:16:19.345


2:16:19.785,2:16:22.515


2:16:22.515,2:16:25.725


2:16:25.725,2:16:28.995


2:16:29.375,2:16:31.155


2:16:31.645,2:16:34.705


2:16:34.705,2:16:37.505


2:16:37.505,2:16:38.505


2:16:39.045,2:16:42.215


2:16:42.215,2:16:44.415


2:16:44.785,2:16:47.785


2:16:47.785,2:16:49.225


2:16:49.775,2:16:52.705


2:16:52.705,2:16:54.985


2:16:54.985,2:16:57.665


2:16:57.665,2:17:00.805


2:17:00.885,2:17:04.335


2:17:05.185,2:17:06.645


2:17:07.205,2:17:09.925


2:17:09.925,2:17:12.855


2:17:12.855,2:17:15.165


2:17:17.045,2:17:18.575


2:17:19.515,2:17:22.385


2:17:22.695,2:17:25.755


2:17:25.755,2:17:27.375


2:17:27.935,2:17:31.065


2:17:31.185,2:17:33.835


2:17:33.835,2:17:36.815


2:17:36.815,2:17:39.135


2:17:39.765,2:17:42.505


2:17:42.505,2:17:45.385


2:17:45.385,2:17:48.285


2:17:48.285,2:17:51.385


2:17:51.385,2:17:54.045


2:17:54.355,2:17:57.015


2:17:57.015,2:17:59.705


2:17:59.705,2:18:02.845


2:18:02.845,2:18:06.065


2:18:06.065,2:18:08.495


2:18:09.455,2:18:12.055


2:18:12.375,2:18:15.235


2:18:15.235,2:18:18.275


2:18:18.275,2:18:21.415


2:18:21.415,2:18:24.295


2:18:24.515,2:18:27.695


2:18:27.695,2:18:30.755


2:18:30.755,2:18:34.195


2:18:34.265,2:18:37.745

