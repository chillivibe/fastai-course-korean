0:00:00.489,0:00:04.799
코더를 위한 실용적인 딥러닝 코스에 온 것을 환영합니다.

0:00:04.799,0:00:09.209
지금 듣는 내용은 전체과정의 첫 번째 과정입니다.

0:00:09.209,0:00:11.110


0:00:11.110,0:00:14.040
저는 샌프란시스코에 위치한 데이터 기관에서 본 강의를 녹화중입니다.

0:00:14.040,0:00:16.040


0:00:16.119,0:00:19.649
이 첫번째 부분에서, 총 7개의 레슨이 있을 예정입니다.

0:00:20.020,0:00:24.389
대부분 2시간 정도 걸릴것이고, 
지금 듣는 이 레슨은 약간 짧은편 입니다.

0:00:24.389,0:00:26.389


0:00:26.769,0:00:34.229
"코더를위한 실용적인 딥러닝 코스"의 목적은
세계적-수준의 결과를 딥러닝으로 얻는 방법을 다룹니다.

0:00:34.660,0:00:38.399
이름이 말해주듯, 
코딩에 초점을 맞춘 과정입니다.

0:00:38.530,0:00:43.259
하지만, 지나치게 단순화하진 않을 것입니다. 
이 코스가 끝날때즘 되면,  세계적-수준의 모든 결과를

0:00:43.719,0:00:49.649
재 구축에 필요한, 모든 이론바탕을 깊이있게 얻게 될 것 입니다.
일단은 별다른 준비없이 시작을 해봅시다.

0:00:50.829,0:00:54.718
이 비디오는 유투브에 호스팅 되어 있다는걸 알려드립니다.

0:00:55.570,0:01:00.899
하지만, 코스 웹사이트에서 시청하기를 강력히 추천합니다.
course.fast.ai 사이트에 가보면, 똑같은 비디오긴 하지만

0:01:01.930,0:01:09.059
웹사이트에서 시청하면, 최신 라이브러리나

0:01:09.340,0:01:14.189
자주 질문되는 사항에 대한 정보등을 접하면서

0:01:14.619,0:01:17.758
코스 진행이 가능합니다.

0:01:18.640,0:01:23.519
혹시 유투브에서 시청하고 있다면, 
course.fast.ai 로 접속해 보는게 어떨까요?

0:01:23.740,0:01:27.180
코스 웹 사이트에서 시청하려고 생각한다면, 일단
그 페이지에 있는 모든 자료를 읽어보길 바랍니다.

0:01:27.369,0:01:31.469
시청 전에 필요한 모든것을 갖췄는지 확인해 보면 좋겠지요.

0:01:33.220,0:01:39.029
한가지 더 말해두고 싶은건, forums.fast.ai 라는 
매우 뛰어난 커뮤니티가 있다는 점입니다.

0:01:41.380,0:01:44.579
코스 진행 중간중간에 막히는 부분이 생길 텐데요,

0:01:45.159,0:01:49.289
극 초반에 막히거나
한동안 막힘이 없을 수도 있겠지요.

0:01:49.290,0:01:51.779
근데 어느 한 순간에는 이해하는데 있어서 
막히는 부분이 생길 겁니다.

0:01:52.659,0:01:58.169
왜 그건 그렇게 동작해야만 하지? 라던가
컴퓨터 자체에 문제가 발생한다거나 말이죠.

0:01:58.479,0:02:06.059
forums.fast.ai 에 가보면, 수 많은 다른 학생들이 
모든 레슨과 다른 주제에 대해서 토론을 합니다.

0:02:06.310,0:02:12.360
현재까지 딥러닝 관련 가장 활발한 커뮤니티에요.
꼭 커뮤니티에 등록하고, 참여하길 추천합니다.

0:02:12.920,0:02:16.880
그러면, 이 코스에 대해서 정말 많은 것을 얻어갈 수 있을 거에요.

0:02:19.680,0:02:21.680
그러면, 코딩을 하면서 시작해 보도록 합시다.

0:02:21.780,0:02:29.180
이 슬라이드는 접근 방식을 보여줍니다.
나중에 top-down 방식의 학습에 대해서 이야기 할 텐데요,

0:02:30.010,0:02:36.420
일단은 직접 해보면서 배워 봅시다. 그러면,
실제로 뉴럴넷을 학습시켜 보도록 합시다.

0:02:37.090,0:02:42.149
일단, 뉴럴넷을 학습시키기 위해서는
거의 항상 GPU가 필요할 겁니다.

0:02:42.849,0:02:46.499
GPU는 그래픽 처리 유닛이라는 것입니다.

0:02:47.650,0:02:49.650
이게 뭐냐하면

0:02:49.840,0:02:53.220
더 좋은 컴퓨터게임을 할 수 있게 도와주는데 사용됩니다.

0:02:54.459,0:02:59.608
CPU에 비해서, 게임 그래픽을 더 빠르게
그리고 화면에 보여줄 수 있게 해주죠.

0:03:00.609,0:03:07.199
더 자세한걸 잠시후에 이야기 하도록 하고,
일단은 GPU를 사용하기 위한 방법을 보여주겠습니다.

0:03:09.010,0:03:16.260
구체적으로는 NVIDIA GPU가 필요한데, NVIDIA GPU만이 
CUDA라고 불리는 것을 지원하기 때문입니다.

0:03:16.690,0:03:20.849
CUDA는 프로그래밍 언어이자 프레임워크로
거의 모든 딥러닝 라이브러리와

0:03:21.699,0:03:25.199
연구/개발자들이 일을하는데 사용합니다.

0:03:26.829,0:03:33.388
특정 카드 제조사의 환경에 묶이는게 이상적이지 못하죠.
나중에 다른 경쟁 회사들이 나타나길 희망해 봅시다.

0:03:33.389,0:03:35.849
근데 일단 지금, 우리는 NVIDIA GPU가 필요합니다.

0:03:37.090,0:03:43.139
게임에 특화된 노트북을 구매한게 아니라면, 
NVIDIA GPU가 없을 확률이 높습니다.

0:03:44.889,0:03:45.940
그래서

0:03:45.940,0:03:52.109
GPU 하나를 임대할 필요가 있는 것이죠.
좋은 소식은, 이 임대한 GPU의 사용에 대한 비용이

0:03:52.629,0:03:57.839
초당 계산되어 청구되니까, 쉽고, 저렴한 가격으로 
GPU기반의 컴퓨터 이용이 가능하다는 것입니다.

0:03:58.569,0:04:00.719
지금부터 이를 위한 두 가지 옵션을 보여드리겠습니다.

0:04:02.650,0:04:05.669
보여드릴 첫 번째 옵션은

0:04:06.579,0:04:09.239
가장 쉬운 방법으로, Crestle 이라고 불리는 것이죠.

0:04:09.879,0:04:13.319
crestle.com 에 접속해서 

0:04:14.620,0:04:17.250
sign up 버튼을 클릭 하거나, 
전에 sign in 한 적이 있다면

0:04:17.889,0:04:20.038
지금 보고계신 화면이 열리는 것을 
확인하실 수 있을 겁니다.

0:04:20.229,0:04:23.369
이 화면에 보면, 큰 버튼이 하나 있는데
Start Jupyter 라고 쓰여 있죠. 

0:04:23.620,0:04:29.429
그리고 enable GPU라는 스위치 버튼도 있습니다.
이 버튼을 켜면, GPU가 가용 상태로 전환됩니다.

0:04:29.560,0:04:31.560
Start Jupyter를 눌러봅시다.

0:04:32.110,0:04:33.940
그리고,

0:04:33.940,0:04:35.940
Start Jupyter를 눌러봅시다.

0:04:37.540,0:04:40.619
그러면, Jupyter notebook 이라는 것을
실행하게 됩니다.

0:04:41.139,0:04:42.730
수 많은 데이터 과학자들을 대상으로한 최근 조사에서

0:04:42.730,0:04:50.730
Jupyter Notebook은 데이터 과학자에게 있어서
세번째로 가장 중요한 툴이라고 평가 되었습니다.

0:04:51.190,0:04:55.799
이 툴을 배우는건 꽤나 중요합니다.
모든 코스의 내용은 Jupyter를 통해서 실행됩니다.

0:04:56.320,0:04:58.260
네, 레이첼. 질문이나 덧붙일 말이 있나요?

0:04:58.260,0:05:04.439
Crestle 을 처음 사용하면, 
무료 10시간이 제공된다는 사실을 이야기 해두고 싶었어요. 
(1시간임)

0:05:08.530,0:05:14.250
맞아요. 최근 그것 보다는 양이 줄어든것 같은데요, 
 FAQ나 pricing을 확인해봐야 겠지만, 
몇 시간은 확실히 무료로 제공될 거에요.

0:05:15.190,0:05:21.719
실제로는 AWS(아마존 웹 서비스)에서 돌아가기 때문에,
가격이 달리질 수 있는데, 일단 시간단 60센트군요.

0:05:23.080,0:05:29.250
근데 좋은게 뭐냐하면, 언제든지 GPU 없이 
Jupyter를 키거나 끄거나 할 수 있다는 겁니다.

0:05:29.250,0:05:31.979
GPU에 비하면, 10분의1 가격이니까 꽤 괜찮죠.

0:05:34.229,0:05:39.149
Jupyter Notebook은 코스 전반에서 사용됩니다.
여기서 시작하면 되는데요,

0:05:39.150,0:05:42.630
코스에 대한 내용을 찾아볼 겁니다. 
그러니까.. courses라는 폴더로 가봅시다.

0:05:43.570,0:05:45.570
그리고 fastai 라는 폴더로 이동해 봅시다.

0:05:46.210,0:05:47.890
그리고..

0:05:47.890,0:05:49.450
여기에 있군요.

0:05:49.450,0:05:53.399
몇 내용이 이동하거나, 약간의 변경이 있을 수 있어요.
이 비디오를 볼때면,  다른 폴더를 확인해 봐야할지도 모릅니다.

0:05:54.039,0:05:57.719
현재 상황에 대한 정보를 웹사이트에 올려둘테니
확인해 보시길 바랍니다.

0:05:59.590,0:06:03.510
보다시피 Crestle 방식이 즉각적이고 쉬운걸

0:06:04.360,0:06:08.039
알 수 있겠지요.

0:06:09.460,0:06:11.080
근데, 좀 더 시간이 허락된다면

0:06:11.080,0:06:18.660
paperspace 라는 좀 더 나은 옵션을
확인해 봅시다.

0:06:22.690,0:06:27.809
Crestle과는 다르게 Paperspace에는 AWS 사용 대신에
자체적으로 컴퓨터를 보유하고 있습니다.

0:06:32.889,0:06:34.330
그리고

0:06:34.330,0:06:38.429
여기가 paperspace 인데요, 
New Machine을 클릭해 보면

0:06:39.459,0:06:45.299
세 군데의 데이터센터 중 하나를 선택할 수 있습니다.
가장 가까운 것을 선택하면 되는데,
제 경우에는 West Coast가 되겠네요.

0:06:46.569,0:06:50.549
그리고, Linux를 선택하고
Ubuntu 16을 선택하겠습니다.

0:06:52.269,0:06:56.999
그러면, 머신을 선택하라고 하는데,
선택 가능한 많은 종류의 머신을 확인해 볼 수 있습니다.

0:06:57.789,0:06:58.989
그리고

0:06:58.989,0:07:00.279
시간당 지불 방식도 확인 가능합니다.

0:07:00.279,0:07:06.898
이 머신이 시간당 40센트로 좋아 보이는군요.
Crestle 보다도 저렴합니다. 이 머신은 사실상

0:07:06.899,0:07:14.099
Crestle 의 60센트짜리 머신 보다 훨씬 빠릅니다.
시간당 65센트 짜리는 그것 보다도 더 빠르구요.

0:07:14.499,0:07:18.148
좋습니다. 그러면, paperspace 방식은 
준비없이 시작하는 방식이기 때문에,

0:07:18.729,0:07:24.658
 어떻게 시작해야 하는지를
실제로 보여드리도록 하겠습니다.

0:07:25.419,0:07:32.549
시간당 65센트 옵션을 선택하면, 
paperspace에 컨택 해야한다고 할 수도 있습니다.  

0:07:32.860,0:07:38.429
왜 사용하고 싶은지등을 말해주면 됩니다.
단순히 사기방지를 위한 것으로, fast.ai 라고 말하면

0:07:40.899,0:07:45.389
바로 사용가능하게 해 줄 겁니다.
저는 일단 시간당 40센트 옵션을 사용할 겁니다.

0:07:48.939,0:07:51.689
필요한 용량의 크기를 선택할 수 있습니다.

0:07:52.749,0:07:57.449
머신이 실행되면, 일단 선택된 용량에 대한 
월단위의 금액을 지불해야 합니다.

0:07:57.449,0:08:01.378
그러니까 많은 머신을 실행하고 끄고 하지 마세요.
왜냐하면 그럴때마다 돈을 지불해야 하니까요.

0:08:01.990,0:08:05.639
월당 250기가의 용량이 $7이면 괜찮아 보입니다.

0:08:06.159,0:08:09.569
하지만, 가격을 최소화 하고 싶다면 50기가로도 충분하죠. 

0:08:11.649,0:08:17.549
달리 알아둬야 할 것은 Public IP를 켜야 한다는 것인데,
외부에서 이 머신에 접속할 수 있는 수단이 됩니다. 

0:08:17.919,0:08:21.718
자동 snapshot 기능을 끄면, 돈을 절약할 수 있지만
백업은 이루어지지 않습니다.

0:08:27.950,0:08:33.970
좋습니다. create 버튼을 클릭하면,
약 1분 후,

0:08:35.360,0:08:40.390
생성된 머신이 나타난 것을 확인할 수 있습니다.
보시다 시피, Ubuntu 16.04 머신이 보이는군요.

0:08:41.810,0:08:43.810
이메일을 확인해 봅시다.

0:08:45.230,0:08:50.349
패스워드에 대한 내용이 있을 거에요.
패스워드를 복사한 후,

0:08:51.920,0:08:54.370
머신으로 들어가서, 그 패스워드를 입력하세요.

0:08:54.950,0:09:01.960
패스워드를 붙여넣으려면, Ctrl +Shift + V 를 누르거나
맥에서는  + Shift + V 를 누르면 됩니다.

0:09:03.170,0:09:07.479
일반적인 붙여넣기 과정과는 약간 다르죠.
또는 그냥 키보드 타이핑 하셔도 됩니다.

0:09:10.550,0:09:14.710
그러면 접속이 되었습니다. 
여기 화살표를 누르면, 화면 공간이 커집니다.

0:09:15.770,0:09:17.750
그리고 약간 확대해 보겠습니다.

0:09:17.750,0:09:21.969
보이는것 처럼, 터미널같은 화면이 있습니다.
브라우져 안에 존재하는데 

0:09:23.420,0:09:26.020
꽤나 편리합니다.

0:09:26.170,0:09:32.979
자, 그러면 코스에 대한 설정을 좀 더 해야 합니다.
 그 방법으로 다음을 수행하면 됩니다.

0:09:34.790,0:09:36.790
curl

0:09:37.490,0:09:44.289
curl http:

0:09:47.210,0:09:48.500
curl http://files.fast.ai/setup/paperspace

0:09:48.500,0:09:49.670
curl http://files.fast.ai/setup/paperspace | bash

0:09:49.670,0:09:56.560
이 명령은 스크립트를 실행하는데, 
CUDA 드라이버와 

0:09:58.160,0:09:59.930


0:09:59.930,0:10:06.460
파이썬의 한 방식인 Anaconda와 코스에 필요한
다른 모든 라이브러리들을 설정해 줍니다.

0:10:07.100,0:10:14.950
코스의 첫 번째 부분에서 사용되는 데이터도 만들어줍니다.
약 1시간정도 걸리는데, 이 작업이 끝나면

0:10:15.290,0:10:17.560
컴퓨터를 재부팅 해줘야 합니다.

0:10:18.170,0:10:20.260
물리적으로 보유하신 컴퓨터 말고,

0:10:20.260,0:10:24.219
paperspace의 컴퓨터 입니다. 
여기 보이는 작은 원형태의 버튼을 클릭해서

0:10:24.620,0:10:30.969
재부팅 하실 수 있습니다. 재부팅이 완료되면, 
다음으로 진행할 준비가 된 것입니다. 

0:10:32.870,0:10:35.710
anaconda3 디렉토리가 있는걸 보실 수 있는데요,

0:10:35.710,0:10:42.199
파이썬이 있는 장소 입니다. 그리고 data 디렉토리는 코스의 
첫 번째 부분에 대한 데이터가 들어 있습니다.

0:10:42.629,0:10:44.629
dogs and cats는 첫 번째 레슨을 위한 겁니다.

0:10:45.269,0:10:47.628
fastai 디렉토리도 있는데요,

0:10:50.369,0:10:52.369
코스에 대한 모든것이 들어 있습니다.

0:10:52.920,0:10:54.920
그러면, 뭘 해야만 할까요?

0:10:57.389,0:11:03.229
cd fastai 로 디렉토리 이동 하시고,
때때로 git pull 해서 fastai 관련 내용물이

0:11:03.990,0:11:05.990
최신상태인지 확인해 봐야 합니다.

0:11:06.480,0:11:10.759
또, 때때로 파이썬 라이브러리들도
최신상태인지 확인해 봐야 하죠.

0:11:10.769,0:11:13.339
이를 위해서 conda env update 를 수행하면 됩니다.

0:11:16.050,0:11:21.800
fastai 디렉토리 안으로 들어와 있는지 확인하시고,
jupyter notebook 이라고 타이핑 

0:11:22.679,0:11:24.679
해보시기 바랍니다.

0:11:26.910,0:11:31.249
이제 Jupyter Notebook 서버가 동작하게 됩니다.

0:11:31.249,0:11:36.289
그리고, 그 서버에 접속해서 사용해야 하는데요
보시는것처럼 이 URL을 브라우져 주소창에

0:11:37.350,0:11:41.029
복사&붙여넣기 하라는 군요.
URL을 더블클릭 해 보세요.

0:11:41.759,0:11:43.759
그러면

0:11:46.230,0:11:48.230
URL이 복사될 겁니다.

0:11:48.689,0:11:52.879
그리고나면, 이 URL을 붙여넣기 해보세요.
그런데, localhost라는 부분을

0:11:53.730,0:12:01.429
paperspace 머신의 IP 주소로 바꿔야 합니다.
이 화살표를 눌러서 전체화면을 해제하면,
이곳에서 IP 주소를 확인하실 수 있습니다.

0:12:02.009,0:12:04.009
그러면 이것을 복사해서,

0:12:04.799,0:12:06.799
localhost 라는 부분에다가 대신

0:12:06.809,0:12:13.489
붙여넣어 보죠. 보시는것 처럼 이제는
http 다음에 제 IP 주소가 있고 전에 복사한 모든게 있게 됩니다.

0:12:14.369,0:12:16.369
자, 접속 되었군요.

0:12:16.529,0:12:18.529
보시는것이 fast.ai 의 

0:12:19.410,0:12:23.269
git 저장소 내용이고,
코스에 대한 모든 내용은 courses에 들어 있습니다.

0:12:23.939,0:12:27.349
거기서 딥러닝 첫 번째 부분은 dl1 이고,

0:12:28.170,0:12:30.170
거기서 

0:12:30.660,0:12:34.100
레슨1에 대한 IPython 노트북이 있는걸 보실 수 있을 거에요.

0:12:39.400,0:12:45.440
crestle이든 paperspace든 뭐를 사용하던지간에
이제 시작할 준비가 되었어요. 

0:12:45.460,0:12:56.000
course.fast.ai를 확인해 보시면, 소개된 방법 이외에
설정할 수 있는 다른 방법에 대한 비디오하고 링크가 있습니다.

0:12:57.500,0:13:03.160
Jupyter Notebook에서 셀을 실행하려면
셀을 선택하고,

0:13:03.740,0:13:06.140
Shift + Enter 키를 입력하거나

0:13:06.740,0:13:14.290
툴바를 나타나게 해서, 여기 보이는 Run 버튼을 누르면 됩니다.
보이는 셀들이 코드를 포함하거나, 단순 글자나 그림

0:13:15.080,0:13:22.869
또는 비디오를 포함한다는 것을 눈치 채셨을텐데요,
이 환경은 기본적으로 

0:13:23.870,0:13:29.679
실험할 수 있는 환경을 제공해 주면서, 동시에

0:13:30.050,0:13:33.669
이에대한 설명 또한 같이 확인해 볼 수 있게 해줍니다.

0:13:34.390,0:13:36.400
이 환경이 데이터 과학에서 왜 인기있는 툴인지

0:13:36.920,0:13:42.550
알 수 있을 겁니다. 
데이터 과학은 실험을 하는 과정 이라고 생각하시면 됩니다.

0:13:44.090,0:13:46.899
그러면, Run 버튼을 클릭 해 봅시다.

0:13:47.210,0:13:53.259
셀 왼쪽에 보시면, * 표시가 잠시동안 생기는걸 보실 수 있는데,
실행이 끝나면 사라집니다.

0:13:53.260,0:13:58.119
다음 셀도 한번 실행해 봅시다. 이번는 툴바 버튼 대신에
Shift + Enter를 사용해 볼게요.

0:13:58.970,0:14:01.749
앞서 본 것 처럼 * 로 변하고,
끝나면 숫자 2로 바뀌는걸 보실 수 있습니다.

0:14:02.030,0:14:06.639
Shift 키를 누른채, 계속 Enter 키를 누르면
각각의 셀이 계속해서 실행되게 됩니다.

0:14:06.830,0:14:10.900
그리고 원하는 뭐든지 여기에 입력할 수 있어요.
예를 들어서 1+1는 실행해보면

0:14:11.480,0:14:13.220
2가 되죠.

0:14:15.560,0:14:17.739
이제부터 우리가 해야할 것은...

0:14:18.590,0:14:24.849
네, 레이첼?
>> 사이드 노트로, 여기서 사용되는 파이썬은 
버전이 3.x 라는걸 알려드리고 싶었어요.

0:14:24.850,0:14:29.590
맞아요, 고마워요. 

0:14:30.770,0:14:33.189
버전 2를 사용하신다면, 버전 3으로 변경해야 합니다.

0:14:38.660,0:14:40.749
아시다시피, 많은 라이브러리들이

0:14:42.080,0:14:44.080
버전 2에 대한 지원을 지워나가고 있습니다.

0:14:45.440,0:14:47.440
고마워요 레이첼!

0:14:48.499,0:14:53.988
자, 여기를 확인해 보시면, 이 레슨에 사용되는 
데이터셋이 다운로드 가능한 링크가 있습니다.

0:14:55.379,0:14:57.119
Crestle이나 Paperspace를 

0:14:57.119,0:15:03.709
사용하면,  이 데이터셋은 이미 가용상태 입니다.
아까 보셨던것 처럼 말이죠.

0:15:03.709,0:15:06.138
만약 이 둘을 사용 안한다면,
wget 명령으로 이 데이터셋을 얻어야 합니다.

0:15:08.339,0:15:10.339
Crestle은 

0:15:10.520,0:15:14.060
paperspace에 비해서 꽤 느린편이고,

0:15:15.060,0:15:17.420
몇가지 지원되지 않는 것들이 있습니다.

0:15:17.640,0:15:23.720
그래서, 몇가지 추가적인 단계가 필요합니다.
다음 두 셀을 실행 해야만 하는데요,

0:15:24.029,0:15:27.319
보시는것 처럼 시작부분에 # 문자로
주석처리 되어 있습니다.

0:15:27.320,0:15:31.480
# 문자를 제고하고, 이 두개의 셀을 실행하면,

0:15:31.680,0:15:37.220
Crestle을 사용할때 필요한 추가 작업이 수행될 겁니다.
저는 paperspace를 사용하니까 필요 없겠네요.

0:15:38.640,0:15:40.640
자, 그러면

0:15:41.609,0:15:43.609
데이터를 한번 

0:15:43.769,0:15:47.928
들여다 봅시다. 위에 보시면
PATH를 data/dogscats 로 설정 했는데요

0:15:47.929,0:15:52.579
이 PATH 안에 보시면, 
아! 여기 보이시는 ! 문자는

0:15:53.849,0:15:55.849
기본적으로

0:15:56.639,0:16:00.139
파이썬을 실행하기 싫다는 말입니다.
대신에 bash를 실행 하겠다는 거죠.

0:16:00.139,0:16:04.938
이 셀은 bash 명령어하고 { }괄호 안의 내용을 실행하는데,

0:16:05.759,0:16:11.058
{ } 괄효는 사실상 파이썬 변수를 참조하게 됩니다.
파이썬 변수를 bash 명령어에 넣어주는거죠.

0:16:11.339,0:16:13.848
PATH 폴더의 안을 보시면,

0:16:14.099,0:16:21.079
학습, 검증에 대한 데이터셋이 있군요.
이 데이터셋을 잘 모르신다면,

0:16:21.539,0:16:24.109
"실용적인 머신러닝" 코스를 확인해 보시길

0:16:25.080,0:16:27.080
추천 합니다.

0:16:27.140,0:16:32.980
그 코스에서는 이런 종류의 내용을 많이 다루고,
머신러닝 프로젝트에 대해서, 좀 더 일반화된 

0:16:33.260,0:16:35.440
설정과 실행방법의 기본을 배울 수 있습니다.

0:16:36.659,0:16:40.369
이 코스 전에, 들어보는걸 추천합니다.

0:16:41.009,0:16:46.908
많은 학생들이 그러는데, 이 코스를 들으면서 
머신러닝 코스도 같이 진행해보니 좋았다는군요.

0:16:47.759,0:16:50.178
그러니까 한번 머신러닝 코스도 확인해

0:16:51.269,0:16:53.269
보시길 바랍니다.

0:16:55.799,0:16:59.148
비슷한 내용을 다룰지도 모르지만,
완전히 다른 접근방식을 사용합니다.

0:16:59.149,0:17:05.689
두 코스를 모두 한 사람들이 그러는데
각 코스의 내용이 상호보완적인 부분이 있다고 합니다. 
선수 과목은 아니지만,

0:17:06.659,0:17:08.778
제가 이게 학습 데이터셋이고, 

0:17:08.779,0:17:11.149
저게 검증 데이터셋이라고 말할때

0:17:11.150,0:17:15.709
그 의미를 모르면, 최소한 구글링해서 빠르게 한번 읽어 보세요.

0:17:16.470,0:17:23.270
머신러닝이 뭐고 다른 기타 기본적인 내용을 
알고 있다고 가정하니까요.

0:17:23.279,0:17:26.719
이 주제에 대해서 블로그 글도 많이 적어놨습니다.

0:17:26.720,0:17:29.809
그 블로그 링코도 course.fast.ai에서 확인 가능합니다.

0:17:29.990,0:17:36.289
>> fast.ai의 철학은 필요에 의해서 기본을 배워나가는 
것이라는 것을 말씀드리고 싶습니다.

0:17:36.809,0:17:42.019
맞아요. 모든것을 시작할때부터
배우고 시도하려고 하지 마세요.

0:17:42.020,0:17:47.029
그러기 쉽지 않을 겁니다. 
>> 맞아요. 딥러닝에서도

0:17:47.789,0:17:49.789
>> 특히나 마찬가지 구요 ㅎㅎ

0:17:51.720,0:17:53.570
valid 폴더 안을 보시면,

0:17:53.570,0:18:00.409
cats 하고 dogs라는 폴더가 있고요,
cats 폴더 안에는 많은 JPEG 이미지가 존재합니다.

0:18:01.400,0:18:06.940
이렇게 설정된 이유는
이미지 분류에 대한 데이터셋을 공유하고 제공하는데 

0:18:07.860,0:18:15.080
가장 보편적인 표준화된 방식 이라서 입니다.
각 폴더의 이름은 레이블을 말해 주는데,

0:18:15.420,0:18:17.420
여기 각각의 이미지들은

0:18:17.460,0:18:23.560
고양이로 레이블되어 있고, dogs 폴더 안에는 
강아지라고 레이블된 이미지들이 있게 되는 것입니다. 

0:18:23.600,0:18:26.140
예를 들어서, Keras도 이 방식대로 동작합니다.

0:18:28.560,0:18:33.260
그러니까 이 방식이 이미지 분류를 위한 파일을
공유하는 표준적인 방법이라는 거죠.

0:18:36.960,0:18:44.000
그리고, plt.imshow() 를 이용해서 이미지를 볼 수 있는데,
지금 보고 있는건 첫 번째 고양이 이미지 입니다.

0:18:45.929,0:18:47.929
이 문자열 형태를 처음 보신다면,

0:18:47.940,0:18:49.940
이것은 파이썬 버전 3.6의

0:18:50.549,0:18:54.199
형식의 문자열 입니다.
좀 더 자세한건 구글링 해보세요!

0:18:54.200,0:18:57.049
문자열의 포맷을 만드는 아주 편리한 방식이라서,
이 코스에서 많이 사용될 것입니다.

0:18:58.590,0:19:05.149
보시는것 처럼 고양이 그림이 있네요. 하지만,
그보다는 이 그림을 이루는 내부 데이터에 관심을 가져야 합니다.

0:19:05.789,0:19:07.789
좀 더 구체적으로 말하자면

0:19:08.000,0:19:15.820
이 이미지의 형태는 198 x 179 x 3 크기의 배열이고,
3차원의 배열입니다.

0:19:15.980,0:19:18.520
3차원(등급)의 텐서라고도 합니다.

0:19:19.100,0:19:23.860
그리고, 이 셀이 의미하는 건 그 이미지 배열의 
처음 4개의 열과 4개의 행을 보여주는 것입니다.

0:19:24.930,0:19:26.850
보다시피

0:19:26.850,0:19:28.650
각각의 

0:19:28.650,0:19:30.650
열에는 세 개의

0:19:31.530,0:19:32.450
요소가 들어 있습니다.

0:19:32.450,0:19:39.739
이 값은 RGB 픽셀값으로 0에서 255의 값으로 이루어져 있죠.
여기 보이는게 컴퓨터가 사진을 실제로 

0:19:40.050,0:19:42.349
바라보는 값의 일부 입니다.

0:19:43.320,0:19:48.529
이 숫자들을 사용해서, 이 숫자들이 고양이 또는 개를
표현한다는 것을

0:19:49.020,0:19:53.629
예측해 보는게 우리가 하게될 일입니다.

0:19:54.270,0:19:56.629
물론 많은 개와 고양이 사진을 사용해야 합니다.

0:19:56.630,0:20:02.479
쉬운 일은 아닙니다. 
사실, 이 데이터셋은 

0:20:03.540,0:20:08.300
Kaggle 경연에서 가져온 것인데요,

0:20:08.550,0:20:10.699
공개 됐을 때가 아마도.. 2012년 입니다.

0:20:11.700,0:20:17.150
최신예 기술이 80%의 정확도를 보여줬습니다.
컴퓨터가 정말로 모든고양이와 개 사진을 정확히

0:20:17.370,0:20:20.060
분류하지는 못했다는 것입니다.

0:20:21.180,0:20:23.960
일단 모델을 학습시켜 봅시다.

0:20:28.700,0:20:34.540
여기 보시면 세 줄의 코드가 있는데,
모델을 학습시키기 위해서 반드시 필요합니다.

0:20:35.240,0:20:39.920
일단 이 셀을 실행해 봅시다. 기억하시죠?
Shift + Enter를 누르면 실행 됩니다.

0:20:42.260,0:20:47.240
그러면, 수 초 정도 기다리면, 
학습 진행을 보여주는 뭔가가 나타날 겁니다.

0:20:48.330,0:20:50.780
네, 학습이 진행되고 있군요.

0:20:51.690,0:20:55.489
3번의 에포크를 진행하라고 요청했습니다.
이 말의 의미는 전체 이미지에 대한 학습을

0:20:55.620,0:20:59.630
총 세번 수행하라는 것입니다.

0:20:59.660,0:21:04.260
그게 에포크가 의미하는 것입니다.
그리고 학습을 수행하면, 뭔가를 출력하는데요,

0:21:05.560,0:21:11.320
마지막 행의 세 개의 열이 의미하는건 
검증 데이터셋에 대한 모델의 예측 정확도 입니다.

0:21:12.180,0:21:15.340
나머지 숫자들의 의미는 나중에 설명 하겠습니다.

0:21:15.350,0:21:18.560
이들은 손실함수의 값인데,
여기서는 크로스 엔트로피 손실값이

0:21:18.660,0:21:23.359
학습과 검증 데이터셋에 대해서 출력 되었고,
맨 앞에 있는 숫자는 에포크의 숫자 입니다.

0:21:23.700,0:21:26.390
보시다시피 약 90%의 

0:21:27.960,0:21:29.960
정확도가 나오는군요.

0:21:30.030,0:21:34.310
그리고 약 17초 걸렸습니다.
2012년의 결과와 비교해 보면

0:21:35.160,0:21:38.450
큰 차이가 나는걸 알 수 있습니다. 
사실상

0:21:39.420,0:21:47.239
이 결과는  당시 Kaggle 경연대회에서의 우승 수준입니다.
Kaggle 경연에서 최고점이 98.9 정도 인데, 우리는 약 99%

0:21:47.850,0:21:49.020
결과니까요.

0:21:49.020,0:21:52.700
2012~13에 있던 Kaggle 경연에서 우승할 수준의 결과를

0:21:53.480,0:21:57.460
약 17초만에 얻어서

0:21:58.300,0:22:03.160
이 결과에 약간 놀랐나요? 

0:22:05.250,0:22:07.250
불과 세 줄의 코드로 말입니다.

0:22:08.340,0:22:14.779
많은 사람들이 딥러닝은 엄청난 시간이 
걸리는 작업이거나 엄청나게 큰 데이터가 필요하다거나

0:22:15.420,0:22:20.359
생각해서 놀랐을 거에요. 
이 코스를 진행하면서, 알게 되겠지만

0:22:21.060,0:22:23.389
일반적으로 사실이 아닙니다.

0:22:24.330,0:22:31.460
우리가 딥러닝 과정을 훨씬 간결하게 만들었고,
여기 보시는 코드는 우리가 개발한 fast.ai 라고 불리는 

0:22:32.520,0:22:34.520
라이브러리 위에서 작성된 것입니다.

0:22:34.890,0:22:39.979
fastai 라이브러리는 기본적으로 
가장 현실적인 모든 방식을 포함합니다.

0:22:40.740,0:22:47.990
논문이 발표되고, 우리 생각에 흥미롭다고 판단되면,
그 논문을 테스트 해보고

0:22:48.300,0:22:53.749
다양한 데이터에 대해서 잘 동작하면서
어떻게 튜닝해야 하는지 알고나면, 
이를 fastai의 일부로 구현합니다.

0:22:53.790,0:22:59.359
fastai는 이 모든것을 관리하고,
사용자들을 위해서 패키징 됩니다.

0:22:59.360,0:23:01.230
사용하는 대부분의 상황에서 

0:23:01.230,0:23:03.380
자동으로 문제를 다루는 최고의 방법을 알아냅니다.

0:23:04.050,0:23:07.430
그러니까, fastai 라이브러리를 사용했기 때문에,
단 세줄로 학습이 가능 했습니다.

0:23:08.040,0:23:14.359
그리고, fastai 라이브러리가 잘 동작하게 만들 수 있었던 
이유는 이 라이브러리는 PyTorch 

0:23:14.360,0:23:16.360
라는 것을 내부적으로 사용하기 때문입니다.

0:23:17.240,0:23:20.120
PyTorch는 페이스북에서 개발된 

0:23:20.130,0:23:25.420
매우 유연한 딥러닝, 머신러닝, 
그리고 GPU 계산용 라이브러리 입니다.

0:23:27.630,0:23:34.310
많은 사람들이 PyTorch 보다는 TensorFlow를 
더 친숙해 합니다. 구글이 마케팅을 열심히 해서 그래요.

0:23:34.680,0:23:40.800
제가 아는 구글에 있는 많은 상위권 연구자들이 
요즘 PyTorch로 갈아타고 있습니다.

0:23:40.800,0:23:42.500
네 레이첼?

0:23:42.500,0:23:47.480
코스 나중에 PyTorch를 다룰 것이라고 언급하려고요.
>> 네. 맞아요

0:23:48.929,0:23:55.639
학생들이 fastai 를 좋아하길 희망해 봐야 겠군요.
현실적으로 최고인 것들을 포함하면서도

0:23:55.740,0:24:02.089
원하는게 있으면, 스스로의 코드를 추가하기도 쉬운
매우 유연한 라이브러리 입니다.

0:24:02.700,0:24:09.680
예를 들어서, 커스텀 데이터 강화라던지
손실 함수, 네트워크 구조등을 말이죠.

0:24:11.060,0:24:14.400
자, 그러면
이 모델이 어떻게 생겼는지 한번 볼까요?

0:24:15.080,0:24:17.920
확인해 보는 방법으로는

0:24:18.480,0:24:22.099
검증 데이터셋에 대한 변수 Y가 어떻게 

0:24:22.620,0:24:27.169
생겼나 확인해 볼 수 있습니다. 
많은 0과 1로 채워져 있군요.

0:24:27.570,0:24:34.039
data.classes 를 출력해 보면, 
0은 고양이, 1은 개를 나타낸다는걸 알 수 있습니다.

0:24:34.039,0:24:35.240
기본적으로 두 개의 객체만 있는데요,

0:24:35.240,0:24:39.620
하나는 학습과 검증 데이터를 뭉친 
data라고 불리고 것이고

0:24:39.620,0:24:44.209
다른 하나는 learn 이라는 것으로,
모델이 저장됩니다. 아시겠죠?

0:24:44.669,0:24:47.809
그리고 데이터에서 뭔가 찾고 싶으면, 
언제든지 데이터 내부를 들여다 보세요

0:24:49.340,0:24:55.400
검증 데이터셋을 가지고 예측을 해볼겁니다.
그러려면 learn.predict 함수를 호출하면 됩니다.

0:24:56.580,0:25:05.260
출력 결과는 처음 10개의 예측 값인데
의미하는건 예측 결과가 개냐 고양이냐에 대한 것입니다.

0:25:05.540,0:25:12.020
PyTorch하고 fastai가 동작하는 일반적인 방식은 
대부분의 모델이

0:25:12.990,0:25:14.309
확률값 자체가 아니라

0:25:14.309,0:25:19.908
예측에 대한 로그를 반환한다는 겁니다.
왜 그런지 나중에 알려 드리도록 할게요.

0:25:20.100,0:25:23.630
그래서, 확률값을 얻고 싶으면
np.exp 라고 되어 있는 

0:25:24.179,0:25:26.179
이 코드를 수행해야 하는 겁니다.

0:25:27.030,0:25:28.830
여기 보시면 np라고 되어 있는

0:25:28.830,0:25:32.720
Numpy를 사용합니다. 
Numpy는 이 코스에서 우리가

0:25:32.720,0:25:36.289
학생들이 친숙할 것이라고 생각하는 것 중 하나입니다.

0:25:36.870,0:25:43.039
course.fast.ai 웹에서 이에대한 기본을 
배울 수 있으니까 확인해 보시기 바랍니다.

0:25:43.440,0:25:47.900
Numpy는 파이썬이 수를 다루는 프로그래밍을

0:25:48.900,0:25:52.340
빠르게 할 수 있도록 해줍니다.

0:25:52.559,0:25:54.859
배열 계산같은 것들 말이죠.

0:25:56.080,0:25:59.280
쨋든, np.exp 라는걸 사용해서 

0:25:59.780,0:26:02.100
확률을 구할 수 있었습니다.

0:26:02.380,0:26:06.160
여기에 몇 선언된 함수들이 있는데
관심 있으면 한번 분석해 보세요.

0:26:06.420,0:26:08.420
그림을 그리는 함수에요.

0:26:08.740,0:26:13.740
이걸 사용해서, 무작위로 선택된
옳바르게 예측된 이미지를 그려볼 수 있습니다.

0:26:15.860,0:26:20.139
보시는 바와 같이 말이죠.

0:26:20.140,0:26:24.910
다시 상기시켜 드릴게요.
1은 강아지 이고,

0:26:25.460,0:26:29.400
0은 고양이 입니다. 
여기 이 값은 10의 -5승이니까 확실히 고양이죠.

0:26:30.320,0:26:32.320
여기는 몇 가지

0:26:32.500,0:26:37.660
옳바르지 않게 예측된 결과를 보여줍니다.

0:26:38.060,0:26:40.299
이 그림은 확실히 아니란걸 알겠죠

0:26:41.480,0:26:48.399
여기 이 사진은 개라고 예측됐는데, 개가 아닙니다.
확실히 보이는 실수들이 있습니다.

0:26:51.230,0:26:53.230
여기서는

0:26:54.770,0:27:02.379
가장 자신있게 고양이라고 예측된 결과를 보여줍니다.
아래에는 강아지에 대산 것이구요.

0:27:03.860,0:27:08.949
더 흥미로운 내용으로, 여기서는
자신있게 개라고 예측했는데, 고양이인 것을 보여줍니다 

0:27:09.050,0:27:11.980
어떤게 제일 잘못된 결과인지를 보여주죠

0:27:12.830,0:27:14.360
똑같이 개에 대해서도 

0:27:14.360,0:27:18.579
고양이라고 자신있게 예측했는데,
사실은 개인 경우입니다.

0:27:19.120,0:27:22.160
여기에 개가 있다고 생각되는데 재미 있군요.

0:27:22.160,0:27:22.820
네, 레이첼

0:27:22.510,0:27:26.710
왜 데이터를 들여다 봐야 하는지에 대해서 
좀 더 이야기 해주실 수 있나요?

0:27:27.950,0:27:29.950
물론이죠.

0:27:31.820,0:27:38.590
일단 마지막으로 보시는것에 대한 설명을 마무리하죠
확률이 0.5에 가까운 것들에 대한 것입니다.

0:27:38.590,0:27:40.340
이 이미지들은

0:27:40.340,0:27:44.169
모델이 어떻게 분류해야 할지 모르는 것들입니다.
보시는대로, 그럴만한 이미지 들이죠?

0:27:46.310,0:27:48.759
지금까지 한 것들에 대해서 잠시 설명 드리자면

0:27:50.720,0:27:56.649
모델을 만든 후, 첫 번째로 항상 하는게 있는데
결과를 시각화 하는 것입니다.

0:27:57.800,0:27:59.800
모델을 좀 더 좋게 만들고 싶으면

0:27:59.840,0:28:05.769
현재 잘 동작하는 데이터를 이해하고,
잘 동작 못하는 데이터에 대해서 뭔가 수정을 해야 하거든요

0:28:07.640,0:28:11.270
그리고 이렇게 해서 
데이터 자체에 대한 것을 배울 수 있곤 합니다

0:28:11.270,0:28:14.500
 이 이미지는 여기에 있으면 곤란한 녀석인걸
알게 되는것 처럼 말이죠

0:28:16.460,0:28:18.460
그리고, 이 모델이 

0:28:18.480,0:28:20.660
향상될 만한 공간이 있는지

0:28:20.670,0:28:22.760
 확실히 해두고 싶고 말이죠.

0:28:23.310,0:28:25.850
저에게 이 사진은 명확하게

0:28:26.400,0:28:29.449
개지만, 한가지 의심이 가는건

0:28:30.030,0:28:32.030
이 이미지는 매우

0:28:32.610,0:28:34.610
폭이 두껍고

0:28:35.100,0:28:36.570
높이가 짧습니다.

0:28:36.570,0:28:38.570
나중에 배우겠지만

0:28:39.240,0:28:44.150
알고리즘이 동작하는 방식은 
정사각형 만큼의 부분을 검사하는 것입니다.

0:28:45.320,0:28:50.390
그래서 제가 약간 의심스럽다는 것입니다.
이 상황에서 데이터강화 라는걸 사용해야 하는데요,

0:28:50.670,0:28:53.180
무엇이고 어떻게 사용하는지 나중에 배우게 됩니다.

0:28:59.190,0:29:01.250
일단 여기까지 됐군요.

0:29:04.470,0:29:12.199
이미지 분류기를 만들었고, 여러분이 시도해 볼만한 것은
스스로 두개나 그 이상의 종류로 이루어진

0:29:13.710,0:29:15.710
사진들의 데이터를

0:29:16.110,0:29:22.789
구해서, 각각 폴더에 넣고, 앞에서 본
세 줄의 동일한 코드를 돌려보는 것입니다.

0:29:23.250,0:29:26.150
데이터가 평범하게 찍힌 사진들 이라면

0:29:27.000,0:29:37.960
스스로 구한 데이터에 대해서도 
잘 동작할 것입니다. 

0:29:38.880,0:29:41.900
현미경 사진, 병리학적인 사진, 또는

0:29:42.990,0:29:48.140
CT 사진같은 것들에 대해서는 잘 동작하지 않는데, 
왜 그런지 나중에 배울 겁니다.

0:29:48.140,0:29:53.089
이 사진들에 대해서도 잘 동작하게 하려면,
추가 작업이 필요합니다. 하지만, 보통의 사진 이라면

0:29:54.810,0:29:57.440
이 세 줄과 동일한 코드를 실행해 보실 수 있습니다.

0:29:58.080,0:30:00.080
단지, PATH 변수가 스스로 구한 이미지를

0:30:00.510,0:30:02.510
보관한 폴더를 가리키게 해주세요.

0:30:03.360,0:30:06.469
예를 들어서,

0:30:07.230,0:30:09.150
한 학생이

0:30:09.150,0:30:11.150
아까 세 줄의 코드를 수행하는데,

0:30:11.280,0:30:17.779
크리켓 하는 사람에 대한 사진 10장과
야구 하는 사람에 대한 사진 10장을
구글 이미지에서 다운로드 했습니다.

0:30:18.510,0:30:20.220
그리고, 그 이미지에 대한

0:30:20.220,0:30:23.270
분류 모델을 만드는데, 
그 과정은 거의 완벽하게 올바르다고 볼 수 있죠.

0:30:23.970,0:30:25.410
그 학생이

0:30:25.410,0:30:29.390
7장의 캐나다 통화 사진과, 

0:30:30.150,0:30:34.880
7장의 미국 통화 사진을 다운로드 하려고 한다면

0:30:35.400,0:30:40.640
역시나 모델은 100% 정확도를 보여줄 겁니다.
그러니까, 구글 이미지로 가서 

0:30:40.860,0:30:46.010
다른 종류의 원하는 사진 몇 장을 다운로드하고
그 결과에 대해서 커뮤니티 포럼에서 

0:30:46.320,0:30:48.320
성공이냐 실패냐에 대해서 알려주세요.

0:30:52.320,0:30:54.500
지금까지 우리가 한 것은

0:30:55.800,0:31:02.180
뉴럴넷을 학습시키는 것이었습니다. 하지만, 
먼저 뉴럴넷이 뭔지 학습이 뭔지에 대한 이야기가 없었죠.

0:31:03.600,0:31:08.540
왜 그랬을까요?
이건 top down 학습 방식의

0:31:09.060,0:31:11.140
시작 이었습니다.

0:31:11.580,0:31:17.630
그러니까, 기본적으로 수학이나 기술적인
주제에 대해서 작은 한 부분 부분을 모두 배우고

0:31:17.790,0:31:23.480
그 부분 부분을 한데 모으지 않는 형식과는 다릅니다. 
예를 들어서 대학원 3년차에 이르러서야

0:31:24.420,0:31:30.289
이미지 분류 모델을 만들어 내야 하는 것 처럼 말이죠.
우리가 사용하는 접근 방식은

0:31:30.630,0:31:35.089
시작과 동시에,  어떻게 이미지 분류 모델을 
학습 시키는지 보여주고

0:31:35.089,0:31:40.360
점진적으로 더 깊이 더 깊이 있게 
알아가게 되는 것입니다. 

0:31:42.000,0:31:44.740
그러니까 이 방식은

0:31:46.170,0:31:53.330
코스를 통해서, 해결하고자 하는 새로운 문제를 
마주하게 됩니다. 예를 들어서, 다음 레슨에서는

0:31:56.100,0:32:01.189
일반적인 사진이 아닌 경인 위성 사진에 대해서
다루게 되는데요,

0:32:01.190,0:32:06.830
우리가 배우는 접근 방식이 "왜" 
잘 동작하지 않는지를 알게 되고,

0:32:07.020,0:32:13.520
어떤 부분을 수정해야 하는지를 배우게 됩니다.
무슨일이 일어나는지 이론적인 부분을 충분히 배울 것이고

0:32:13.620,0:32:17.599
라이브러리와, 그 라이브러리를 사용해서 
더 좋은 결과를 위한 변화를 주는 방법을 배우게 됩니다. 

0:32:20.460,0:32:26.539
코스를 진행하면서, 점진적으로 더 많은 종류의 문제의 
해결 방법을 배우게 되는데, 그러면서 

0:32:26.580,0:32:31.730
라이브러리의 더 많은 부분과
더 많은 이론적인 내용을 알게 됩니다.

0:32:32.310,0:32:35.060
실제로 배우게될 내용은 아무런 준비 없이 

0:32:35.950,0:32:38.099
"어떻게" 세계적 수준의 

0:32:38.710,0:32:45.000
뉴럴넷 구조를 만드는지, 그리고 
학습을 위한 Loop을 만드는지에 대한 것입니다.
기본적으로 우리 스스로가 만들게 되는거죠.

0:32:45.850,0:32:47.680
설명드린 것이 이 방식을 

0:32:47.680,0:32:48.730
표현해 줍니다.

0:32:48.730,0:32:55.200
네 레이첼?
>> 이 방식은 "whole game"이라고도 하는데,
 하버드 교수인 "David Perkins"로 부터 

0:32:55.750,0:32:57.450
영감을 받은 거죠.

0:32:57.450,0:33:01.769
"whole game"은 어떻게 야구나 음악
같은 것을 배우는 방법에 더 가까운데,

0:33:01.770,0:33:05.129
야구 게임을 일단 접하고,

0:33:05.130,0:33:09.450
야구가 뭔지 배우게 되고,

0:33:09.700,0:33:15.569
몇 년 후에, 변화구가 어떻게 가능한지에 대한
물리적인 것을 배우게 되는 과정입니다.

0:33:15.610,0:33:22.170
음악에 비유하자면, 일단 악기를 손에 쥐고
드럼을 두들기거나 실로폰을 치고

0:33:22.170,0:33:29.159
몇 년 후에는 음의 5도를 배우고,
어떻게 마침표를 그리는지를 배우게 되는 겁니다.

0:33:30.460,0:33:34.050
그러니까, 우리가 사용하는 이 방식은 
"David Perkins"나 

0:33:34.690,0:33:37.679
다른 교육분야의 작가들로 부터 
영감을 받았습니다.

0:33:38.710,0:33:43.470
이 방식으로 뭔가 배움을 얻어가려면,
점차 배움의 단계를 하나씩 벗겨 나가고

0:33:44.170,0:33:48.599
단계마다 숨어 있는 의미를 스스로 살펴 봐야 합니다.
이 코스는 매우 코딩이 주도하는 방식이라서

0:33:48.910,0:33:52.800
많은 실험등을 수행해야 합니다.

0:33:53.380,0:33:57.030
여기 보시면, 뭘 하게 될지 보이실 겁니다.
일단 오늘은 

0:33:57.910,0:34:01.859
이미지를 위한 컨볼루션 뉴럴넷을 살펴보고,
뒤의 두 레슨까지 가면서

0:34:01.860,0:34:07.709
어떻게 구조화된 데이터를 뉴럴넷이 이용하는지,
그리고 언어 데이터,

0:34:07.710,0:34:11.189
그리고 추천 시스템 데이터,

0:34:12.280,0:34:17.969
그리고 여기까지 배운 깊이있는 지식을 가지고
반대방향으로 거슬러 올라 갈 것입니다.

0:34:17.970,0:34:22.140
네번째 부분까지 도달하면, 

0:34:23.500,0:34:29.159
세계적 수준의 이미지 분류 모델,

0:34:30.250,0:34:32.250
세계적 수준의 구조화된 데이터의 분석 프로그램

0:34:32.770,0:34:36.659
세계적 수준의 언어 분류 모델,
세계적 수준의 추천 시스템을 생성하는걸 알게 될 겁니다.

0:34:37.090,0:34:41.279
그러고나면, 거꾸로 가면서 깊이 있는 공부를 하게 되는데, 

0:34:41.280,0:34:47.550
정확히 뭘 했고, 어떻게 해서 동작했고,
변화를 줄 수 있는 방법과, 다른 상황에서 어떻게 사용 가능한지

0:34:48.130,0:34:51.009
추천 시스템에서 시작해서,
구조화된 데이터

0:34:51.770,0:34:56.739
이미지와 마지막으로 언어관련 순서로 
배우게 될 것입니다.

0:34:56.740,0:35:04.360
많은 학생들이 비디오 강의를 두번, 세번씩 시청하곤 하는데

0:35:06.110,0:35:11.229
두세번 시청하고, 두세번 들어보고 하는게 아니라

0:35:11.230,0:35:18.310
일단 전체 비디오 레슨을 다 들어보고, 
다시 처음으로 돌아가서 듣는 방식으로 말입니다.

0:35:19.460,0:35:23.740
다시 돌아가서 모든 상세내용을 이해하고자 할때, 
많은 사람들이 이 방식대로 진행합니다.

0:35:24.020,0:35:30.249
꽤 좋은 방식입니다. 
레슨 7의 마지막까지 듣는걸 목표로 하고

0:35:32.210,0:35:39.070
모든걸 다 이해하려고 하지 말고,
가능한한 빠르게 진행하세요.

0:35:41.510,0:35:46.570
그래서 말인데,
오늘 레슨에서 배우게될 내용은

0:35:47.780,0:35:52.269
가능한한 짧은 라인의 코드와
가능한한 적은 상세 내용입니다.

0:35:52.270,0:35:57.820
딥러닝을 이용해서, 보시는 그림처럼 
고양이 그림과 상반되는 개 그림 몇장을 쥐어주고

0:35:57.820,0:36:01.749
이미지를 분류하는 모델을 
실제 어떻게 만드는지를 배웁니다.

0:36:03.200,0:36:05.200
그리고나서, 다른 종류의 이미지를

0:36:05.810,0:36:08.199
 어떻게 관찰하는지도 배우고요.

0:36:08.570,0:36:13.809
여기서는 위성사진 이미지를 한번 관찰해 보고,
이 위성사진에서 어떤 종류의

0:36:14.450,0:36:21.370
내용이 있는지 보게 될 거고, 사진에 여러개의 내용이 
담겨 있을 수 있기 때문에 
다중-레이블(multi-label) 분류에 대해서도 살펴볼 겁니다.

0:36:23.420,0:36:26.440
그 다음으로는 많은 사람들에게

0:36:26.960,0:36:32.320
가장 폭 넓게 적용 가능한 것으로
구조화된 데이터 라는걸 배웁니다.

0:36:33.080,0:36:37.479
구조화된 데이터라는건 데이터가
데이터베이스나, 스프레드시트 같은데

0:36:38.180,0:36:39.610
있는걸 의미합니다.

0:36:39.610,0:36:46.629
여기 보이는 데이터를 보게 될 겁니다.
여러 가게에서 팔린 물건들을 날짜별로 팔린 갯수를

0:36:47.150,0:36:48.800 
예측해 보는 거에요.

0:36:48.800,0:36:55.600
다른 종류의 휴일이나 다른 카테고리에 의해서
판매량을 예측해보는 연습입니다.

0:36:56.510,0:37:00.639
이 내용 다음으로는, 언어(말)를 다뤄볼 거에요.
예를 들어서

0:37:01.400,0:37:02.990
이 글을 적은 사람이

0:37:02.990,0:37:05.140
영화에 대해서 어떻게 생각하는지와 같은거에요

0:37:05.360,0:37:09.099
앞에서 이미지 분류 모델을 만든것과 비슷하게

0:37:09.200,0:37:16.659
여기서는 NLP 모델을 만들어서, 다양한 방법으로
다양한 언어를 분류하는법을 배우게 됩니다.

0:37:19.100,0:37:23.860
그 다음으로는 협업필터링 이라는걸 배울 거에요.
이건 보통 추천 시스템을 위한 겁니다.

0:37:24.320,0:37:27.249
여기 보시는 이 데이터를 사용하게 됩니다.
4명의 다른 사람이 매긴

0:37:27.410,0:37:32.769
4개의 영화에 대한 평점에 대한 것입니다.
여기에 영화 정보가 있군요.

0:37:33.440,0:37:35.559
이 차트를 보면 좀 더 쉽게 이해 될 거에요.

0:37:35.560,0:37:37.560
많은 사용자(리뷰어)들이 있는지

0:37:37.640,0:37:39.320
많은 영화 종류가 있는지.

0:37:39.320,0:37:45.340
사용자들이 얼마나 각각의 영화를 좋아하는지 말이죠.
당연히 우리의 목적은

0:37:45.340,0:37:47.860
사용자와 영화를 결합해서

0:37:47.860,0:37:55.690
아직 보지 못한 영화에 대해서, 좋아할지의 예측입니다.
홈페이지에 어떤 내용을 넣을지

0:37:55.970,0:37:59.470
결정하는데 사용되는 꽤나 일반적인 방법입니다.

0:37:59.540,0:38:03.910
어떤 책을 좋아할지, 어떤 영화를 좋아할지 
같은 내용입니다.

0:38:05.840,0:38:11.140
그리고 나서, 이 언어(말)에 대한 내용을
좀 더 깊이 파볼 예정 입니다.

0:38:11.720,0:38:17.139
철학자 니체의 글을 이용해서
글자 하나하나로 부터, 우리만의 글을

0:38:17.750,0:38:20.890
생성하는 방법을 배울 거에요.

0:38:21.320,0:38:27.309
지금 제가 읽어드리는 글은

0:38:27.310,0:38:31.239
니체가 적은 글이 아니에요

0:38:31.790,0:38:34.360
이 내용은 순환 뉴럴넷을 이용한

0:38:35.060,0:38:39.160
문자 단위로 생성된 글입니다.

0:38:41.060,0:38:44.680
여기까지 배우고 나면,  처음으로 돌아가서
컴퓨터 비전을 다시 볼 거에요.

0:38:45.230,0:38:50.680
이번에는 단순히 고양이 개를 분류하는게 아니라
고양이가 사진 어디에 있는지를 찾아낼 거에요.

0:38:50.810,0:38:56.799
여기 보시는 히트맵을 사용해서 말이죠.
또, 우리 스스로의 모델을 설계하는 방법을 배울 겁니다.

0:38:57.080,0:38:59.080
여기 보시는 코드는 ResNet으로

0:38:59.300,0:39:04.870
오늘 레슨 (컴퓨터 비전)에서 사용한 
뉴럴넷의 한 종류입니다.

0:39:04.870,0:39:09.910
이 코드처럼 뉴럴넷을 만들고, 
어떻게 학습시키는지를 알게 될 겁니다.

0:39:09.910,0:39:13.720
기본적으로 여기서 부터 진행되는 과정은

0:39:14.410,0:39:16.410
각 과정마다 상세한 내용의 

0:39:17.299,0:39:21.099
비중을 늘려나가고, 여러분 스스로 
이것들을 할 수 있게 도와줄 겁니다.

0:39:24.559,0:39:27.669
이 글은 과거에 이 과정을 들은 학생의

0:39:28.369,0:39:30.369
생각입니다.

0:39:31.060,0:39:38.940
많은 학생들이 공통적으로 말하는건
이론과 연구에 너무 많은 시간이 든다는 것이에요.

0:39:40.260,0:39:43.120
반면에 코드를 실행 해보는 시간은 없고요.

0:39:43.490,0:39:49.719
오늘 아침에도 코스 막바지 까지 들은
몇 사람들이 찾아와서 

0:39:49.720,0:39:51.720
계속 코드를 실행해보라는 조언을

0:39:52.400,0:39:55.119
좀 더 심각하게  받아들였어야 했다고 후회 하더군요.

0:39:55.670,0:39:59.319
이 글은 커뮤니티 포럼에서 가져온 건데요
이렇게 말하고 있어요

0:39:59.319,0:40:03.819
실제 코딩과 노트북 실행에서
입력과 그 결과를 이해하는데

0:40:05.450,0:40:07.750
대부분의 시간을 보냈어야 했다고 후회 하는군요

0:40:10.430,0:40:14.169
우리가 접근하는 방식은

0:40:14.720,0:40:21.579
세계적 수준의 모델의 코딩을 먼저 한 후,
진행하면서 더 깊이 있게 배우는 건데

0:40:21.579,0:40:23.619
다른 사람들의 조언과는 많이 다릅니다.

0:40:24.920,0:40:32.589
해커뉴스 포럼에 올라온 글입니다. 이 사람이 그러는데, 
머신러닝 엔진니어가 되기 위한 최고의 방법은

0:40:32.869,0:40:35.949
수학, C, C++, 병렬 프로그래밍,

0:40:36.500,0:40:41.979
머신러닝 알고리즘을 공부하고
C언어로 모든것을 구현해 보고

0:40:42.500,0:40:50.260
그리고 나서, 머신러닝을 시작하라는 군요.
효과적인 전문가가 되려면, 이것과 "반대로" 하라고 
말씀드리고 싶군요.

0:40:50.900,0:40:51.880
네, 레이첼?

0:40:51.880,0:40:53.880
>> 이 글에서 말하는 방식은

0:40:54.820,0:40:59.980
나쁜 조언이고, 많은 사람들이 
포기하게 되는 방법이라고 말씀드리고 싶어요.

0:41:00.040,0:41:06.220
맞아요. 우리는 이미 이 코스를 마친
많은 학생들을 보았고

0:41:07.760,0:41:12.120
그 학생들 중에서 연구실이나, 
구글 브레인팀에 들어가거나

0:41:13.069,0:41:15.069
딥러닝 관련 특허를 만드는등을

0:41:15.140,0:41:22.869
한 사례를 많이 봐 왔습니다.

0:41:23.450,0:41:25.450
그러니까 "top down" 접근법이 꽤나 

0:41:25.730,0:41:27.730
효과가 있는 것입니다.

0:41:27.859,0:41:33.679
우리는 이미 세계적 수준의 이미지 분류 모델을 
17초만에 학습 시키는 법을 배웠습니다.

0:41:34.079,0:41:41.599
처음으로 코드를 실행하면, 17초 보다 
더 걸리는 두 가지 요소가 

0:41:41.970,0:41:47.419
있다는걸 알려 드려야 겠습니다.
첫 번째로는 "미리 학습된" 모델을 인터넷에서 

0:41:47.849,0:41:53.179
다운로드 하는 것입니다. 처음 코드를 실행하면, "downloading model" 메세지를 보게 될거에요.

0:41:53.819,0:41:55.819
이 과정이 약 1~2분 걸립니다.

0:41:56.069,0:41:57.359
그리고

0:41:57.359,0:42:02.869
처음 코드 실행시에, 몇 중간 단계의 정보를 
미리 계산한 후 cach에 저장하는 과정이 있습니다.

0:42:03.269,0:42:08.569
이것도 약 1~2분 걸립니다.
그러니까, 처음으로 코드를 실행하면 

0:42:09.180,0:42:10.920
3~4분정도 다운로드와 사전 계산 과정등에

0:42:10.920,0:42:17.840
소요됩니다. 나중에 다시 실행해 보면, 
그때는 약 20초 정도 걸리는걸 확인할 수 있을 거에요.

0:42:18.680,0:42:21.960
이미지 분류에 대해서 

0:42:21.960,0:42:23.960
고양이하고 강아지를

0:42:24.450,0:42:31.730
컴퓨터가 인식할 필요가 있는지를 생각할 수 있습니다.
직접 해보면 더 잘 분류하는데 말이죠.

0:42:32.339,0:42:33.839
근데, 흥미로운건

0:42:33.839,0:42:38.058
이미지 분류 알고리즘이 다른 일에도
매우 유용할 수 있다는 거에요.

0:42:39.720,0:42:41.720
예를 들어보죠.

0:42:42.029,0:42:48.498
바둑 세계챔피언을 이긴 알파고가 
동작하는 원리의 코어에는

0:42:49.499,0:42:56.359
우리가 만든 개/고양이 이미지 분류 알고리즘과
매우 비슷한 것이 있습니다.

0:42:56.880,0:43:00.829
알고리즘이 수 많은 과거에 플레이된 바둑판을 보고

0:43:01.710,0:43:07.220
각 바둑판에는, 그 바둑을 둔 사람이 
이겼는지 졌는지를 레이블링 해 뒀거든요.

0:43:08.100,0:43:10.380
제가 하고 싶은 말은, 알파고가

0:43:10.560,0:43:13.100
기본적으로 이미지 분류를 학습 했다는 겁니다.

0:43:13.200,0:43:17.450
바둑판을 볼줄 알고, 그 바둑판이 
좋은지 나쁜지를 판단한 것이죠.

0:43:17.730,0:43:25.730
이게 바둑을 두는데 가장 중요한 단계로
다음 수의 퀄리티를 판단하는 겁니다.

0:43:27.809,0:43:31.969
또 다른 예를 들어볼게요.
이것과 관련해서 

0:43:33.329,0:43:35.329
몇 가지 특허를 보유한 수강생이 있는데

0:43:36.150,0:43:39.980
사기꾼 방지에 대한 겁니다.

0:43:40.880,0:43:45.200
사기꾼을 피하기 위한 손님의 입모양을 

0:43:46.380,0:43:52.549
추적하는 소프트웨어를 사용해서
입이 움직이는 방향을 얻었습니다.

0:43:53.339,0:43:56.659
고객 사이트에 있는 사용자들에 대해서

0:43:57.359,0:44:01.788
입이 움직이는지, 움직이면 얼마나 빨리 
움직이는지를 그림으로 바꿨고

0:44:02.579,0:44:06.379
이미지 분류 모델을 만든 후,
그 이미지를 입력 데이터로 사용해서

0:44:07.049,0:44:11.959
거래 내역에 사기꾼이 있는지에 대한
예측을 출력했습니다.

0:44:12.480,0:44:18.889
결과적으로 매우 좋은 결과를 얻었습니다.
이미지 분류라는게

0:44:20.130,0:44:22.789
상상한것 보다도 그 사용 범위가 
더 유연하다는 것이죠.

0:44:26.519,0:44:32.479
지금까지 설명드린게, 딥러닝
특히나 이미지 인식을 사용하는 방법들 이에요.

0:44:33.839,0:44:36.139
이걸 이해 하는게 중요합니다.

0:44:37.109,0:44:39.109
딥러닝은

0:44:39.500,0:44:45.420
머신러닝의 의미와는 다릅니다.

0:44:47.400,0:44:50.380
딥러닝은 머신러닝의 일종 입니다.

0:44:50.819,0:44:56.778
여기 보이는 이 사람(아서 사무엘) 이 50년대 말에
머신러닝을 발명했는데

0:44:57.089,0:45:00.739
그 당시, IBM 메인 프레임으로
자신 보다 체커게임을

0:45:01.170,0:45:06.980
더 잘 두도록 했는데, 그 방식이
머신러닝의 시작이었습니다.

0:45:07.859,0:45:15.679
메인프레임이 스스로 대전하도록 해서
어떻게 하는게 승리하고, 그렇지 않은지를
알아가게 했습니다.

0:45:16.049,0:45:19.309
스스로 프로그램을 만들어 나갔던 것입니다.

0:45:19.710,0:45:24.109
아서 사무엘은 1962년에 이렇게 말했는데요
언젠가

0:45:24.630,0:45:28.130
대부분의 컴퓨터 소프트웨어가

0:45:28.680,0:45:34.639
손으로 직접 코드를 짜는게 아니라, 머신러닝 방식을 
사용해서 만들어질 것이라고 했습니다.

0:45:35.400,0:45:41.180
제 생각엔 아직 그렇게 되지는 않았지만
그렇게 되어가는 도중이라고 생각 되는군요.

0:45:41.789,0:45:47.929
오랫동안 아직 그렇게 되지 못한 이유는
전통적인 머신러닝은 매우 어렵고

0:45:48.599,0:45:50.599
매우 지식이 많이 필요 했고

0:45:51.630,0:45:57.559
시간이 오래 걸렸기 때문입니다. 예를 들어서
여기 보시는건 "컴퓨터 병리학자" 라는건데

0:45:58.560,0:46:02.120
앤드류 백 스탠포드에 있을 때
제안한 내용입니다.

0:46:03.120,0:46:05.299
지금 그는 아마도 

0:46:06.240,0:46:08.419
하버드에서 일하고 있을 거에요.

0:46:09.360,0:46:14.239
그가 한 일은 여기 보시는 
유방암 조직에 대한 병리학적인 

0:46:15.000,0:46:17.000
사진을 가지고

0:46:17.010,0:46:19.310
수 많은 병리학자들과 함께

0:46:20.010,0:46:23.300
 "어떤" 패턴이나 특징이  

0:46:24.030,0:46:29.959
장기적-생존과 단기적-죽음에
연관이 있을지에 대해서 연구해 왔습니다.

0:46:30.720,0:46:35.840
그리곤, 이 컴퓨터 병리학자라는
아이디어를 생각해 냈습니다.

0:46:35.840,0:46:39.350
이 아이디어는 
상피조직에 있는 인접한 핵의 관계나

0:46:40.080,0:46:45.890
상피조직과 기질 종양간의 관계 같은것에
관한것인데, 이것에 대한 특징을 생각해 냈습니다. 

0:46:45.890,0:46:50.119
수 백개의 특징 중, 여기 보시는건 
몇 가지만 나와 있습니다.

0:46:50.700,0:46:52.910
그리곤 똑똑한 수 많은 컴퓨터 프로그래머들이 

0:46:53.700,0:46:59.660
이것에 특화된 알고리즘을 개발해서
이 모든 특징에 대한 계산을 하도록 했습니다.

0:47:00.390,0:47:06.949
특징들이 로지스틱-회귀의 입력값으로 주어지고
생존성을 예측하는 것인데, 결과적으로 
아주 잘 동작 했습니다.

0:47:07.170,0:47:15.110
생존에 대한 예측이 병리학자들이 하는 
예측보다 더 정확 했던 것이죠.

0:47:15.660,0:47:19.729
머신러닝이 매우 잘 동작할 수 있습니다.
단지, 이 예를 통해서

0:47:20.580,0:47:25.610
 알아두셔야 할 것은
수 많은 그 분야 전문가와, 컴퓨터 전문가들이

0:47:26.010,0:47:31.130
수년 동안 이 문제에 매진 했다는 것입니다.

0:47:31.920,0:47:33.920
그런데

0:47:35.460,0:47:37.460
우리가 진짜 원하는건

0:47:38.010,0:47:39.780
좀더 나은 무언가에요.

0:47:39.780,0:47:45.590
제가 지금부터 보여드릴 내용은
매우 특정 분야에 특화된 특징을

0:47:46.110,0:47:49.099
집어내는

0:47:51.030,0:47:53.010
매우 구체적인 함수가 아니라

0:47:53.010,0:47:58.040
무한대로 유연한 함수로
어떤 문제든지 해결 할 수 있는 함수 입니다.

0:47:58.830,0:48:00.860
어떤 문제든지, 그 함수의 파라메터만

0:48:00.860,0:48:03.259
잘 설정해 주면, 해결할 수 있습니다.

0:48:03.260,0:48:08.780
그리고 나서, 그 함수의 파라메터를 
설정하는 만능의 방법이 필요합니다.

0:48:08.780,0:48:11.210
그 다음은 빠르고, 확장성도
있어야 합니다.

0:48:11.610,0:48:14.329
이 세가지를 모두 가지고 있다면,

0:48:14.330,0:48:20.509
이것처럼 매우 시간이 들고, 특정 분야의 지식이 
필요한 접근 방식이 필요 없게 됩니다.

0:48:20.880,0:48:26.680
이 세가지에 대한 알고리즘을 
배우면 되는거죠.

0:48:27.320,0:48:29.320
이미 눈치채셨 겠지만

0:48:29.430,0:48:34.279
이 세가지 속성에 대한 알고리즘을
"딥러닝" 이라고 합니다.

0:48:34.950,0:48:39.019
알고리즘보다는 알고리즘의 한 종류라고
불러야 할지도 모르지만요.

0:48:40.560,0:48:42.590
이 세가지의 순서대로 살펴봅시다

0:48:43.560,0:48:49.519
딥러닝이 사용에 사용되는 근본적인 함수는
뉴럴넷이라고 불리는 것입니다.

0:48:50.460,0:48:56.359
코스를 통해서 뉴럴넷이 뭔지, 그리고
우리 스스로 어떻게 구현할지 배우게 될 겁니다.

0:48:56.850,0:49:03.229
근데 지금은 일단, 이것이 "단순 선형적인 계층"들과 
"단순 비선형적인 계층"들이 배치되어

0:49:04.230,0:49:07.010
이루어졌다는 사실만 아시면 됩니다.

0:49:08.790,0:49:12.290
레이어를 보시는것 처럼 배치하면

0:49:12.900,0:49:19.789
"Universal Approximation Theorem" 이라는걸
얻게 되는데, 이게 의미하는건

0:49:20.010,0:49:22.010
이 함수가 어떤종류의 문제든지

0:49:22.410,0:49:24.739
충분한 파라메터만 주어진다면

0:49:25.440,0:49:31.909
알아서 근사한 정확도로 문제를 
해결할 수 있는지를 말해줍니다.

0:49:32.400,0:49:37.160
증명 가능한 방식으로 
무한대로 유연한 함수를 보여주는 것입니다.

0:49:38.520,0:49:46.520
어떤 방식으로 파라메터가 들어 맞혀서 무한대로-
유연한 뉴럴넷이 특정 문제를 해결하게 하고 싶은거죠.

0:49:47.070,0:49:50.029
그러려면 경사하강법(Gradient Descent)라는 기법을

0:49:50.700,0:49:56.689
사용해야 합니다. 아마도 전에 어디선가 
들어 보셨을 거에요. 경사 하강법은

0:49:56.690,0:49:59.959
우리가 가진 여러 파라메터들의 값이

0:50:00.960,0:50:04.220
문제 해결에 얼마나 좋은지, 그리고

0:50:04.740,0:50:11.719
약간 더 좋은 값의 어떻게 찾아내는지에 대한 것으로,
여기 보시는 손실함수 표면의 선을 따라서 

0:50:12.150,0:50:17.959
내려가는 과정입니다.
이것은 마치 구슬이 내려가는 것과 비슷한데

0:50:18.530,0:50:20.220
최소값을 찾기 위한 것입니다.

0:50:20.220,0:50:24.949
여기 보시면, 어디서 시작하냐에 따라서
다른 장소에 도달하게 됩니다.

0:50:26.970,0:50:34.040
이 다른 장소들은 지역-최소값들 이라고 불리는데
뉴럴넷에서는 흥미롭게도

0:50:35.820,0:50:38.840
여러개의 지역-최소값들이 존재하지

0:50:40.530,0:50:42.560
않습니다. 기본적으로 단 한 곳 만

0:50:43.200,0:50:50.480
존재한다는 겁니다. 다르게 말해보면
한 공간에 여러 다른 장소들이 존재하는데,
그들 모두가 동등하게 좋은 장소인 것이죠.

0:50:52.660,0:50:57.680
경사하강법은 뉴럴넷에 들어맞는 
파라메터를 찾는 문제를 

0:50:58.820,0:51:02.740
해결하기 위한 
매우좋은 방법입니다.

0:51:04.830,0:51:09.620
한가지 문제가 있는데, 

0:51:10.290,0:51:14.239


0:51:15.060,0:51:17.570


0:51:19.260,0:51:23.929


0:51:25.230,0:51:31.459


0:51:31.770,0:51:33.770


0:51:34.080,0:51:36.080


0:51:36.540,0:51:40.489


0:51:41.370,0:51:42.540


0:51:42.540,0:51:46.519


0:51:46.980,0:51:51.890


0:51:52.260,0:51:58.370


0:51:59.310,0:52:01.310


0:52:02.190,0:52:06.890


0:52:08.340,0:52:11.809


0:52:13.410,0:52:15.980


0:52:17.100,0:52:18.600


0:52:18.600,0:52:25.610


0:52:27.750,0:52:29.670


0:52:29.670,0:52:37.000


0:52:39.650,0:52:42.220


0:52:42.220,0:52:46.599


0:52:47.089,0:52:52.689


0:52:53.210,0:52:59.530


0:53:00.109,0:53:02.199


0:53:02.839,0:53:10.059


0:53:11.359,0:53:16.629


0:53:17.270,0:53:23.560


0:53:24.200,0:53:25.609


0:53:25.609,0:53:29.259


0:53:29.990,0:53:36.490


0:53:41.119,0:53:45.129


0:53:47.180,0:53:51.220


0:53:53.060,0:53:59.799


0:54:00.710,0:54:07.300


0:54:08.660,0:54:09.680


0:54:09.680,0:54:15.339


0:54:15.770,0:54:20.349


0:54:21.170,0:54:23.170


0:54:23.480,0:54:26.709


0:54:27.050,0:54:30.939


0:54:31.460,0:54:37.480


0:54:38.180,0:54:41.290


0:54:43.910,0:54:51.040


0:54:51.500,0:54:53.500


0:54:53.990,0:54:57.520


0:54:58.280,0:55:01.660


0:55:03.200,0:55:07.659


0:55:08.360,0:55:12.910


0:55:13.820,0:55:15.820


0:55:16.370,0:55:24.249


0:55:24.530,0:55:26.530


0:55:26.570,0:55:30.789


0:55:31.610,0:55:33.610


0:55:34.610,0:55:39.219


0:55:39.500,0:55:42.969


0:55:43.820,0:55:49.900


0:55:50.210,0:55:57.880


0:55:58.340,0:56:03.340


0:56:04.190,0:56:09.129


0:56:12.020,0:56:16.420


0:56:17.090,0:56:18.680


0:56:18.680,0:56:26.139


0:56:26.140,0:56:33.339


0:56:34.040,0:56:36.219


0:56:37.490,0:56:42.729


0:56:43.970,0:56:50.560


0:56:54.340,0:57:00.690


0:57:02.170,0:57:08.190


0:57:08.740,0:57:12.689


0:57:13.720,0:57:15.720


0:57:16.600,0:57:20.309


0:57:20.860,0:57:27.510


0:57:28.330,0:57:31.019


0:57:32.230,0:57:34.319


0:57:35.020,0:57:38.670


0:57:39.490,0:57:41.490


0:57:42.370,0:57:44.370


0:57:44.470,0:57:46.470


0:57:46.510,0:57:50.669


0:57:50.860,0:57:55.769


0:57:56.080,0:58:03.840


0:58:04.750,0:58:09.449


0:58:09.880,0:58:13.380


0:58:14.380,0:58:18.690


0:58:20.800,0:58:27.390


0:58:27.390,0:58:30.660


0:58:30.880,0:58:37.260


0:58:37.260,0:58:41.309


0:58:41.860,0:58:45.299


0:58:46.300,0:58:51.360


0:58:52.120,0:58:55.380


0:58:56.350,0:58:57.700


0:58:57.700,0:59:03.839


0:59:04.150,0:59:06.150


0:59:06.180,0:59:11.460


0:59:13.269,0:59:15.869


0:59:16.569,0:59:22.139


0:59:22.539,0:59:29.609


0:59:30.069,0:59:34.679


0:59:36.009,0:59:38.699


0:59:40.230,0:59:42.230


0:59:42.730,0:59:44.319


0:59:44.319,0:59:50.459


0:59:50.829,0:59:56.699


0:59:57.489,1:00:03.779


1:00:04.620,1:00:08.120


1:00:10.920,1:00:16.760


1:00:17.460,1:00:23.540


1:00:23.549,1:00:28.379


1:00:28.809,1:00:31.619


1:00:32.140,1:00:37.349


1:00:38.680,1:00:40.420


1:00:40.420,1:00:44.460


1:00:45.190,1:00:48.839


1:00:48.839,1:00:52.768


1:00:53.410,1:00:58.710


1:00:59.980,1:01:07.319


1:01:10.690,1:01:17.730


1:01:18.510,1:01:20.580


1:01:21.580,1:01:23.969


1:01:24.820,1:01:28.889


1:01:29.950,1:01:32.820


1:01:33.640,1:01:39.540


1:01:39.940,1:01:43.380


1:01:43.990,1:01:51.120


1:01:51.880,1:01:58.409


1:01:58.570,1:02:01.229


1:02:01.570,1:02:04.229


1:02:04.600,1:02:10.829


1:02:12.070,1:02:14.639


1:02:15.280,1:02:17.170


1:02:17.170,1:02:24.060


1:02:25.450,1:02:31.590


1:02:32.200,1:02:33.610


1:02:33.610,1:02:36.120


1:02:36.840,1:02:41.260


1:02:41.260,1:02:46.020


1:02:46.800,1:02:51.440


1:02:51.440,1:02:52.060


1:02:52.060,1:02:58.350


1:02:58.660,1:03:04.020


1:03:05.560,1:03:12.600


1:03:12.960,1:03:15.480


1:03:19.080,1:03:24.020


1:03:24.820,1:03:31.260


1:03:31.660,1:03:38.430


1:03:38.890,1:03:46.799


1:03:47.079,1:03:53.279


1:03:54.220,1:03:56.549


1:03:57.099,1:04:02.279


1:04:03.040,1:04:10.439


1:04:10.839,1:04:15.629


1:04:16.180,1:04:22.589


1:04:24.849,1:04:28.558


1:04:30.849,1:04:36.539


1:04:37.510,1:04:38.980


1:04:38.980,1:04:43.230


1:04:43.540,1:04:48.269


1:04:48.910,1:04:53.639


1:04:54.400,1:04:58.770


1:04:59.530,1:05:01.559


1:05:02.980,1:05:07.559


1:05:10.809,1:05:12.839


1:05:13.839,1:05:15.839


1:05:15.849,1:05:23.129


1:05:23.290,1:05:25.919


1:05:26.200,1:05:31.659


1:05:32.330,1:05:35.919


1:05:36.080,1:05:42.850


1:05:43.100,1:05:48.400


1:05:49.100,1:05:56.220


1:05:56.920,1:05:59.100


1:06:02.090,1:06:04.809


1:06:04.810,1:06:12.759


1:06:13.820,1:06:18.699


1:06:18.700,1:06:26.260


1:06:27.080,1:06:29.080


1:06:29.600,1:06:34.479


1:06:34.670,1:06:41.080


1:06:41.660,1:06:47.530


1:06:48.440,1:06:49.730


1:06:49.730,1:06:51.730


1:06:56.870,1:06:58.870


1:07:00.230,1:07:02.919


1:07:03.830,1:07:07.360


1:07:07.970,1:07:12.939


1:07:12.940,1:07:14.940


1:07:15.530,1:07:18.549


1:07:19.580,1:07:21.610


1:07:22.730,1:07:24.730


1:07:26.000,1:07:27.410


1:07:27.410,1:07:32.950


1:07:32.950,1:07:35.079


1:07:35.080,1:07:40.930


1:07:46.430,1:07:48.430


1:07:49.250,1:07:50.650


1:07:50.650,1:07:58.569


1:07:58.570,1:07:59.960


1:07:59.960,1:08:04.990


1:08:05.330,1:08:12.460


1:08:12.740,1:08:14.979


1:08:15.500,1:08:17.500


1:08:18.200,1:08:21.130


1:08:21.980,1:08:23.980


1:08:24.950,1:08:26.950


1:08:29.900,1:08:36.279


1:08:36.950,1:08:43.990


1:08:44.810,1:08:49.780


1:08:50.150,1:08:53.979


1:08:54.710,1:08:58.660


1:08:59.270,1:09:05.830


1:09:06.320,1:09:09.039


1:09:09.560,1:09:16.580


1:09:17.100,1:09:22.729


1:09:22.730,1:09:27.649


1:09:29.310,1:09:32.959


1:09:37.050,1:09:41.449


1:09:43.080,1:09:44.670


1:09:44.670,1:09:45.960


1:09:45.960,1:09:52.700


1:09:52.830,1:09:56.990


1:09:57.390,1:10:02.780


1:10:03.660,1:10:10.789


1:10:11.310,1:10:12.900


1:10:12.900,1:10:15.290


1:10:16.230,1:10:20.300


1:10:20.910,1:10:24.649


1:10:25.800,1:10:28.819


1:10:29.940,1:10:34.129


1:10:34.170,1:10:38.390


1:10:39.030,1:10:42.199


1:10:43.080,1:10:50.450


1:10:51.060,1:10:54.800


1:10:55.170,1:11:01.399


1:11:02.670,1:11:08.120


1:11:09.450,1:11:11.450


1:11:12.030,1:11:14.449


1:11:15.030,1:11:20.780


1:11:21.810,1:11:24.919


1:11:25.560,1:11:27.120


1:11:27.120,1:11:30.289


1:11:32.760,1:11:36.409


1:11:39.240,1:11:41.240


1:11:41.240,1:11:45.080


1:11:45.200,1:11:50.860


1:11:50.870,1:11:54.950


1:11:56.250,1:11:59.419


1:11:59.420,1:12:03.350


1:12:03.350,1:12:09.950


1:12:10.530,1:12:13.519


1:12:13.980,1:12:19.430


1:12:20.040,1:12:22.160


1:12:23.400,1:12:26.450


1:12:27.120,1:12:33.530


1:12:34.830,1:12:39.109


1:12:39.110,1:12:44.839


1:12:45.900,1:12:53.150


1:12:53.790,1:12:57.560


1:12:58.500,1:13:00.589


1:13:01.620,1:13:05.329


1:13:05.940,1:13:07.560


1:13:07.560,1:13:10.850


1:13:12.210,1:13:19.700


1:13:20.640,1:13:23.569


1:13:23.570,1:13:31.430


1:13:31.680,1:13:35.359


1:13:36.150,1:13:41.429


1:13:42.310,1:13:44.310


1:13:48.130,1:13:50.969


1:13:51.520,1:13:57.629


1:13:58.480,1:14:00.480


1:14:00.760,1:14:05.760


1:14:07.780,1:14:14.429


1:14:14.530,1:14:21.600


1:14:21.940,1:14:23.650


1:14:23.650,1:14:26.609


1:14:27.550,1:14:29.550


1:14:29.679,1:14:31.679


1:14:33.460,1:14:36.779


1:14:37.449,1:14:43.709


1:14:44.260,1:14:47.130


1:14:48.370,1:14:50.640


1:14:52.929,1:14:56.279


1:14:59.530,1:15:03.239


1:15:04.179,1:15:06.040


1:15:06.040,1:15:08.040


1:15:08.080,1:15:10.409


1:15:14.590,1:15:20.580


1:15:25.090,1:15:27.090


1:15:27.100,1:15:31.769


1:15:32.500,1:15:36.089


1:15:37.510,1:15:39.510


1:15:40.540,1:15:42.540


1:15:43.090,1:15:45.090


1:15:45.340,1:15:47.050


1:15:47.050,1:15:50.489


1:15:51.130,1:15:53.130


1:15:53.530,1:15:54.670


1:15:54.670,1:15:56.130


1:15:56.130,1:16:03.210


1:16:03.720,1:16:05.600


1:16:05.620,1:16:08.309


1:16:08.309,1:16:15.080


1:16:15.610,1:16:16.630


1:16:16.630,1:16:20.219


1:16:20.770,1:16:22.770


1:16:23.739,1:16:29.099


1:16:30.010,1:16:33.030


1:16:33.030,1:16:38.460


1:16:38.619,1:16:42.059


1:16:44.320,1:16:46.320


1:16:48.190,1:16:53.730


1:16:54.400,1:17:00.480


1:17:00.880,1:17:07.859


1:17:08.679,1:17:12.658


1:17:13.179,1:17:15.959


1:17:15.960,1:17:22.710


1:17:22.710,1:17:29.669


1:17:29.670,1:17:36.659


1:17:38.500,1:17:46.409


1:17:47.560,1:17:51.509


1:17:52.599,1:17:57.239


1:17:58.210,1:18:04.019


1:18:04.599,1:18:06.010


1:18:06.010,1:18:12.479


1:18:13.119,1:18:15.119


1:18:16.269,1:18:20.429


1:18:21.309,1:18:25.139


1:18:25.809,1:18:31.018


1:18:31.019,1:18:34.589


1:18:34.989,1:18:38.908


1:18:39.519,1:18:41.519


1:18:41.620,1:18:46.620


1:18:49.560,1:18:55.040


1:18:56.320,1:19:02.130


1:19:03.070,1:19:10.349


1:19:11.889,1:19:17.399


1:19:17.949,1:19:22.978


1:19:23.409,1:19:27.478


1:19:28.360,1:19:32.969


1:19:33.940,1:19:36.509


1:19:37.989,1:19:39.880


1:19:39.880,1:19:41.829


1:19:41.829,1:19:43.809


1:19:43.809,1:19:48.869


1:19:49.510,1:19:51.929


1:19:53.380,1:19:55.329


1:19:55.329,1:20:02.669


1:20:02.670,1:20:06.929


1:20:07.479,1:20:09.779


1:20:10.780,1:20:12.380


1:20:12.380,1:20:18.000


1:20:18.000,1:20:23.540


1:20:24.219,1:20:29.609


1:20:30.639,1:20:32.939


1:20:33.000,1:20:39.029


1:20:39.460,1:20:43.679


1:20:44.619,1:20:51.689


1:20:51.690,1:20:53.690


1:20:53.920,1:20:59.790


1:21:00.639,1:21:02.819


1:21:04.480,1:21:08.669


1:21:08.670,1:21:16.440


1:21:17.020,1:21:21.810


1:21:21.810,1:21:27.359


1:21:27.360,1:21:32.279


1:21:33.929,1:21:38.969


1:21:39.940,1:21:44.730


1:21:45.790,1:21:47.790


1:21:48.730,1:21:55.589


1:21:56.710,1:22:03.960


1:22:05.050,1:22:10.139


1:22:10.960,1:22:14.849


1:22:15.119,1:22:20.369


1:22:21.150,1:22:21.510


1:22:21.510,1:22:23.500


1:22:23.500,1:22:29.720


1:22:31.740,1:22:35.100


1:22:35.860,1:22:39.690


1:22:39.690,1:22:44.669


1:22:45.520,1:22:47.969


1:22:48.700,1:22:50.700


1:22:51.130,1:22:52.990


1:22:52.990,1:22:54.990


1:22:55.420,1:22:58.379


1:22:59.470,1:23:05.699


1:23:07.390,1:23:12.569


1:23:14.170,1:23:16.589


1:23:18.880,1:23:22.560


1:23:22.560,1:23:29.189


1:23:29.740,1:23:33.719


1:23:34.480,1:23:35.760


1:23:35.760,1:23:39.960


1:23:39.960,1:23:42.270


1:23:44.800,1:23:50.190


1:23:50.490,1:23:57.570


1:23:58.150,1:24:04.170


1:24:04.420,1:24:07.350


1:24:09.280,1:24:10.930


1:24:10.930,1:24:16.380


1:24:16.840,1:24:17.850


1:24:17.850,1:24:23.489


1:24:24.070,1:24:26.070


1:24:26.800,1:24:33.179


1:24:34.170,1:24:34.900


1:24:34.900,1:24:38.340


1:24:39.010,1:24:41.010


1:24:41.680,1:24:46.200


1:24:46.450,1:24:50.099


1:24:50.890,1:24:54.360


1:24:54.940,1:25:00.450


1:25:00.580,1:25:03.660


1:25:03.970,1:25:08.939


1:25:09.910,1:25:14.760


1:25:15.700,1:25:20.669


1:25:20.890,1:25:23.490


1:25:24.100,1:25:27.510


1:25:27.630,1:25:32.069


1:25:32.230,1:25:37.080


1:25:37.870,1:25:43.019


1:25:43.210,1:25:49.950


1:25:50.680,1:25:52.680


1:25:53.230,1:25:57.930


1:25:58.390,1:26:04.499


1:26:04.500,1:26:10.830


1:26:11.260,1:26:17.939


1:26:19.330,1:26:20.890


1:26:20.890,1:26:24.780


1:26:24.940,1:26:28.770


1:26:29.470,1:26:33.899

