0:00:00.250,0:00:04.350
딥러닝 레슨2에 돌아오신걸 환영합니다.

0:00:06.940,0:00:08.940
지난주에

0:00:09.360,0:00:12.800
꽤나 정확한 이미지 분류 모델을 
성공적으로

0:00:13.180,0:00:15.740
학습시키는 것까지 진행하였습니다.

0:00:16.000,0:00:18.719
어떻게 했었는지 
리마인드 해드리도록 하겠습니다.

0:00:19.280,0:01:01.220
(백그라운드 사운드)

0:01:02.440,0:01:08.240
리마인드 차원에서, 이미지 분류 모델을 만든 
방법은 단순히 

0:01:09.300,0:01:12.620
세 줄의 코드를 사용한 것이었습니다.

0:01:12.820,0:01:16.049
그리고 이 세 줄의 코드는 
특정 경로(PATH)를 가리켰는데, 

0:01:16.990,0:01:22.259
이 경로에는 이미 데이터가 들어 있었죠.
모델을 어떻게 학습스키는지를 알기위해서

0:01:23.049,0:01:30.479
개와 고양이 이미지 데이터를 가진 이 경로가
특별한 구조를 가진다는게 중요합니다.

0:01:31.270,0:01:33.570
이 경로에는 학습(train)과 검증(valid) 폴더가 있고

0:01:33.850,0:01:37.949
각 폴더에는 고양이(cats)와 개(dogs)폴더가
들어 있습니다.

0:01:37.949,0:01:42.400
그리고 각 고양이(cats)와 개(dogs)폴더에는
고양이와 개들의 사진이 들어 있습니다. 

0:01:42.400,0:01:44.400
꽤나 표준적인 방식의 구조로

0:01:45.560,0:01:48.120
주로 사용되는 두 가지 구조 중

0:01:48.480,0:01:50.120
하나로 

0:01:50.530,0:01:55.470
이미지 모델의 학습에 필요한 데이터가 이 폴더에 
있다는걸 알려줍니다. 지난 한 주 동안

0:01:55.780,0:01:59.549
다른 종류의 이미지를 가진 폴더에 저장된

0:02:00.340,0:02:03.899
자신만의 다른 데이터를 가지고
자신만의 이미지 모델을 만들어 봤을 겁니다. 

0:02:04.300,0:02:09.140
커뮤니티 포럼에 게시된 글을 보면
대부분 잘 동작하는것 같더군요.

0:02:09.760,0:02:12.080
스스로 해 보신것이

0:02:12.140,0:02:13.880
코스를 시작하는데 있어서

0:02:13.890,0:02:15.890
알아야할 전부입니다.

0:02:16.150,0:02:21.660
다른 종류의 이미지 수백장 또는 수천장이 포함된
폴더를 스스로 만들어 보셨다면, 

0:02:22.180,0:02:24.959
이 동일한 세 줄의 코드의 수행 결과가

0:02:25.569,0:02:31.679
이미지 분류 모델을 생성해 주고,
마지막 세 번째 행을 보고

0:02:31.680,0:02:35.760
모델이 얼마나 정확한지를 
알 수 있을 것입니다.

0:02:37.420,0:02:39.280
그리고

0:02:39.280,0:02:41.280
몇 간단한

0:02:42.010,0:02:44.640
시각화를 수행해서 

0:02:45.640,0:02:49.340
불확실한게 무엇이고, 잘못된 결과가 
무엇인지등을 살펴봤습니다.

0:02:49.340,0:02:51.340
이 시각화는 매우 좋은 접근 방법 입니다.

0:02:52.800,0:02:54.180


0:02:54.190,0:02:58.889
그리고나서, 선택해야 하는 한가지 중요한 숫자에 대해서 
배웠습니다. 

0:02:58.890,0:03:05.970
여기 이게 그 숫자로 값이 0.01 인데,
이 숫자는 학습률(learning rate)이라고 불립니다.

0:03:07.120,0:03:09.959
그리고 학습률이 의미하는게 무엇인지
코스의 나머지를 진행하는 동안

0:03:10.600,0:03:15.509
이론적으로 상세하게 많이 배우게 될겁니다.
단지, 지금은 방법에 대해서 설명드린 것입니다.

0:03:18.780,0:03:27.040
네. Yannet?

0:03:30.760,0:03:35.910
전에 다른 세가지 숫자가 나빠지는 것에 대해서
말씀하였죠??

0:03:36.640,0:03:38.230
여기 이 세가지 숫자요? 

0:03:38.230,0:03:43.170
세번째 것은 나중에 말씀 드리겠습니다.
일단 마지막에 있는 숫자가 주된 것으로

0:03:43.170,0:03:45.959
정확도를 의미합니다.

0:03:47.110,0:03:53.429
첫번째 숫자는 에포크 숫자고, 더 나은 분류 모델을 
학습하기 위해서 몇번이나 전체 데이터셋을

0:03:54.610,0:04:00.239
훑어봤는지를 말해주죠.
두번째, 세번째 숫자는 손실이라는 것인데

0:04:01.209,0:04:05.369
오늘 나중이나,  다음주에 배우게 될 겁니다.
그 중 첫번째는 학습 데이터셋에 대한 손실로

0:04:05.530,0:04:09.179
더 나은 분류 모델을 만들기 위해서
살펴봐야 하는 이미지들에 대한 것입니다.

0:04:09.180,0:04:14.459
두번째는 검증 데이터셋에 대한 손실로,
학습단계에서는 사용되지 않는 이미지로

0:04:14.459,0:04:19.679
아무 관여 없이, 얼마나 정확힌지 확인해 보는것입니다.
쨋든 나중에 좀 더 자세히 배우게 될거에요.

0:04:25.400,0:04:27.400


0:04:29.300,0:04:33.789
요약해 보면, 에포크의 수가 있고
학습 데이터셋에 대한 손실,

0:04:34.700,0:04:39.430
검증 데이터셋에 대한 손실, 그리고 
마지막에 정확도가 있습니다.

0:04:45.399,0:04:48.599
학습률의 기본적인 개념은

0:04:54.520,0:05:01.469
학습률의 기본적인 개념은 얼마나 빠르게
해결책으로 다가갈지를 결정하는 것입니다.

0:05:01.599,0:05:08.218
학습률이 뭔지를 생각해보기에 좋은 방법으로

0:05:08.219,0:05:10.219
어떤함수에 맞아 들어가길 

0:05:10.539,0:05:12.539
원하는 상황을 사용해 봅시다.

0:05:13.240,0:05:18.449
여기 보시는것 처럼 말이에요. 
이함수의 최소 지점이 어디쯤인지를 알고 싶은거에요.

0:05:19.029,0:05:24.509
딥러닝을 할때, 보통 우리가 원하는 것은
함수의 최소지점을 찾는 것입니다.

0:05:26.349,0:05:28.349
만약 우리의 함수가 수백만개의 

0:05:28.869,0:05:33.329
파라메터로 이루어져 있더라도, 동일한 방식으로 동작합니다. 
이 함수를 보면

0:05:33.330,0:05:35.319
즉시 최소 지점이 여기라는걸

0:05:35.319,0:05:37.319
아실 수 있을 거에요.

0:05:38.680,0:05:44.459
하지만, 사람이 아니라 컴퓨터 알고리즘이라면
어떻게 여길 알아낼 수 있을까요?

0:05:44.619,0:05:48.869
여기처럼 시작시점에는 무작위 지점을 선택합니다.
그리고 그 지점에 대해

0:05:48.870,0:05:51.569
손실 또는 에러값이 무엇인지 계산하게 되죠

0:05:51.999,0:05:56.009
그리고 나서, 어느방향이 위고 아래인지를 말해주는
경사도를 구하게 됩니다.

0:05:56.529,0:06:02.729
경사도가, 아래로 내려가기 위한 방향이 왼쪽이라는걸
알려주고, 얼마나 빠르게 내려갈 수 있을지도 알려줍니다.

0:06:03.339,0:06:05.759
이 지점에서는 꽤나 빠르게 내려가겠군요.

0:06:06.519,0:06:09.029
그러면 그 방향으로 한번의 보폭만큼

0:06:09.309,0:06:13.829
내려가고, 움직인 거리는

0:06:13.990,0:06:20.909
얼마나 가파른지를 나타내는 경사도에 비례하게 됩니다. 
더 가파르면, 더 멀리 움직이게 되는거죠.

0:06:21.219,0:06:25.889
설명드린 내용이 기본적인 개념입니다.
이 예제에서 보시면, 이 지점에서 

0:06:25.889,0:06:30.718
얼마나 가파른지에 대한 경사도를 구하고,
경사도에 학습률이라는 값을 곱해줍니다.

0:06:31.209,0:06:34.349
만약 이 학습률의 값을 아주 작게

0:06:35.289,0:06:36.550
설정하면

0:06:36.550,0:06:41.339
아주아주 조금씩 매번 최소지점으로 가까워
지는것을 보장할 수 있습니다.

0:06:41.680,0:06:48.419
하지만 그러면 너무 오랜 시간이 걸리게 되죠.
만약 아주아주 큰 값으로 설정하면

0:06:48.459,0:06:54.959
방향은 맞지만, 너무나 멀리 움직이게 되어 버리는데
이만큼 반대편으로 날아갈 수 있습니다.

0:06:56.169,0:07:00.929
마지막엔 결국 시작점보다도 훨씬 더 
멀리 떨어져 버릴 수 있고, 점점 더 

0:07:01.300,0:07:08.699
나빠지게 됩니다. 만약 뉴럴넷의 학습을 시작하는데,
정확도나 손실값이 

0:07:09.130,0:07:12.930
무한대로 빠져버린다면, 거의 확실하게
학습률이 너무 크다고 보시면 됩니다.

0:07:14.050,0:07:18.120
그러니까 아주 작은 학습률이 
아주 클때보다는

0:07:18.550,0:07:22.590
오랜 시간 기다리기만 하면 되서, 더 나은 문제입니다.
하지만, 만약 최고의 학습률을

0:07:22.590,0:07:24.590
생각해낼 방법이 있다면 좋겠지요?

0:07:24.640,0:07:30.640
아주 빠르게 최소지점으로 도달 가능하니까요.

0:07:32.220,0:07:36.080
그러기 위해서 우리는 
학습률 발견자(lr_find)라는 것을 사용합니다.

0:07:36.090,0:07:42.479
학습률 발견자가 하는일은 매번
미니배치에 대해서, 아 미니배치란

0:07:42.880,0:07:50.010
한번에 확인 대상이 되는 적응양의 이미지로,
GPU를 이용한 병렬 처리의 장점을 효과적으로

0:07:50.350,0:07:52.350
용하기 위한 것입니다. 일반적으로

0:07:52.420,0:07:57.900
각 미니배치에는 64 또는 128장의 이미지가 있습니다.
여기 iterations 으로 표시되어 있습니다.

0:07:58.840,0:08:01.500
학습률을 서서히 배수 단위로

0:08:01.540,0:08:02.850
증가시킵니다.

0:08:02.850,0:08:09.329
처음에는 아주 작은 학습률로 시작해서,
점진적으로 값을

0:08:09.640,0:08:12.180
증가시킵니다. 

0:08:12.670,0:08:18.870
마침내 학습률은 너무 커져서, 손실값이 
점점 나빠지게 됩니다. 

0:08:18.870,0:08:20.870
그러면 이때는 

0:08:21.970,0:08:26.249
학습률에 대한 손실값의 변화에 대한 그래프를
살펴봐야 합니다. 보시다시피 학습률이 아주 작으면

0:08:27.250,0:08:30.510
손실변화가 천천히 증가하고, 
손실변화가 더 빨이 증가하게 되고,

0:08:30.510,0:08:35.100
마지막으로 더이상 증가하지 않기 시작합니다.
사실상 좋아지는게 아니라 나빠지고 있는 시점이죠.

0:08:35.380,0:08:40.840
그리고 과학적 표기와 친숙해 져야

0:08:41.140,0:08:44.100
한다는걸 알아주세요. 
예를 들어서 10의 -1승은 0.1이고

0:08:45.370,0:08:51.359
10의 0승은 1이고, 10의 -2승은 0.01 같은것들 말이죠.

0:08:52.120,0:08:58.140
파이썬에서 이 표기들을 코딩할때는 
10-1이나 10-2처럼 적는게 아니라

0:08:58.240,0:09:00.240
1e-1, 1e-2  처럼

0:09:00.700,0:09:06.809
적으면 됩니다. 좌우가 동일한 값이에요.
이 표기를 자주 접하게 될겁니다.

0:09:07.780,0:09:12.299
첫번째가 0.1, 두번째가 0.01인 것도 알아두세요.

0:09:15.100,0:09:17.100


0:09:18.460,0:09:22.769
여기에 출력되는 텍스트로 인해서 혼동하지 마세요.
여기 있는 손실(loss)는

0:09:23.020,0:09:27.380
제일 마지막에 대한 최종 손실값입니다.
일단 지금은 무시하고 넘어가도  됩니다.

0:09:28.260,0:09:32.160
학습률 발견자를 사용하는게 아닌
기본적인 학습을 할때만, 의미 있는 값이에요.

0:09:32.160,0:09:38.999
학습률 발견자를 사용할때 관심을 가져야하는건
learn.sched.plot() 인데요,

0:09:39.670,0:09:45.959
이 그래프에서 가장 낮은 지점이 중요한게 아닙니다.
최저점은 더이상 향상되지 않는 부분이니까요.

0:09:45.960,0:09:47.380
학습률이 너무 높은거죠.

0:09:47.380,0:09:53.489
그러니까 중요한 지점은 최저점이 아니라
최저점의 한단계 직전 지점이 됩니다.

0:09:54.250,0:09:56.110
여기서는 1e-2 정도가 

0:09:56.110,0:09:58.110
좋은 선택이 되겠군요.

0:09:58.140,0:10:04.220
learn.fit() 함수를 수행할때, 
첫번째 인자값으로 0.01(=1e-2) 을

0:10:04.560,0:10:08.460
선택한 이유입니다.

0:10:10.180,0:10:15.450
이 숫자는 우리가 조절할 수 있는 
중요한 숫자라고 배웠다는

0:10:16.090,0:10:17.980
점이 중요합니다.

0:10:17.980,0:10:19.240
그리고

0:10:19.240,0:10:20.260
만약

0:10:20.260,0:10:24.660
다른 부분은 그대로 놔둔채, 이 숫자만 다르게 
조절 해보면 대부분 좋은 결과를 얻을 수 있는데

0:10:24.820,0:10:32.489
보통 교과서나 다른 강의들이 말하는 내용과는 
매우 다른 이야기가 됩니다.

0:10:33.100,0:10:34.660
왜냐하면

0:10:34.660,0:10:36.100
지금까지는

0:10:36.100,0:10:41.790
설정해야하는 하이퍼-파라메터가 수십개씩
존재해 왔고,

0:10:41.950,0:10:47.249
매우 민감하고, 설정하기가 매우 어렵다고 생각되었습니다. 
Fast AI 라이브러리에서는

0:10:47.340,0:10:50.360
가능한한 이 모든것을 자동으로 할 수 있도록

0:10:50.500,0:10:52.150
도와주고,

0:10:52.150,0:10:56.400
코스 동안에는 좀 더나은 결과를 얻기위한 
다른 무언가를 배우게 될 것입니다.

0:10:57.760,0:10:59.580
약간

0:10:59.580,0:11:02.140
재밌는 상황이긴 한데요

0:11:02.140,0:11:06.869
아직 아무것도 배우지 않은 학생들에게는
그 다른 무언가가 전부가 됩니다.

0:11:07.630,0:11:11.999
매우 쉽죠. 그리고 이 수업을 듣지 않은
다른 사람들과 이야기할때

0:11:12.000,0:11:17.100
딥러닝이 매우 어렵고, 진정한 예술적인 형태라고
말할지도 모릅니다.

0:11:17.320,0:11:22.229
진실은 이 학습률을 설정하는게 
엄청나게 중요하다는 것이죠.

0:11:22.300,0:11:27.409
학습률을 어떻게 설정 잘 할지를 알아내기 위해
고안된 이 논문에 소개된 방법은

0:11:28.170,0:11:30.170
약 18개월 정도 되었지만

0:11:30.449,0:11:33.919
거의 아무도 이 논문에 대해서 모른단 말이죠.
별로 유명하지 않은

0:11:33.920,0:11:39.769
연구실 소속의 학생이 적어서, 대부분의 사람들이
이를 무시했어요. 사실 소개된 기법은 전체 논문의

0:11:39.990,0:11:41.990
일부분으로 소개된 내용입니다.

0:11:42.829,0:11:46.099
그러니까 이 방법이 학습률을 어떻게
설정해야 하는 방법이에요.

0:11:47.189,0:11:53.689
이 수업을 듣지 않으면 모를 일이죠. 논문의 저자
Leslie Smith는 물론 알겠지만요 ㅎ

0:11:55.259,0:12:00.589
동료들에게 학습률을 설정하는 매우 좋은
방법이 있다는걸 알려주길 바랍니다.

0:12:01.709,0:12:06.438
또 다른 유명한 논문은 제목이
"no more pesky learning rates"로
(더이상 성가시지 않은 학습률)

0:12:06.899,0:12:10.008
제가 소개드린 것 보다 "덜" 효과적인 기법 입니다.

0:12:10.009,0:12:12.049
학습률을 설정하는 것이

0:12:12.209,0:12:17.899
매우 어렵고 환장할 일이라는게 대부분의 
딥러닝 역사를 통해서 사실로 여겨져 왔죠

0:12:19.019,0:12:21.409
여기 그래프를 보세요

0:12:22.050,0:12:27.079
최저지점 같아보이는 곳을 찾고, 거기서부터
약 1/10만큼 앞의 값을 학습률로 시도해보세요

0:12:27.080,0:12:31.160
이 값으로 잘 동작하지 않으면, 계속해서
1/10만큼 작은걸 시도해 보면 됩니다.

0:12:31.160,0:12:34.300
제 경우는 그럴필요가 항상 없었지만요.

0:12:40.160,0:12:41.279
>> 질문이요

0:12:41.279,0:12:46.578
>> 학습률 방법이 momentum 등 다른 방법과
비교해서 어떤가요?

0:12:46.579,0:12:52.428
>> 학습률 설정 기법의 장점과 단점 같은 것들 말예요

0:12:58.559,0:13:00.559
아주 좋은 질문이에요.

0:13:00.689,0:13:03.498
경사하강법의 향상을 위한 말씀하신 momentum이나

0:13:03.920,0:13:07.700
Adam 같은 많은 방법을 코스동안에 배우게 될 겁니다.

0:13:08.129,0:13:14.029
사실 Adam같은 기법들은 fastai 라이브러리가
하려는 일과 직교하는데요,

0:13:14.610,0:13:17.149
fastai가 올바른 경사하강을 알아내려고 할때,
내부적으로는 

0:13:17.149,0:13:22.849
Adam 이라는 것을 사용합니다. 이 학습률의
기법은 최고로 좋은 학습률을

0:13:24.749,0:13:28.399
Adam optimizer라는걸 사용해서 알려주는 것이죠.

0:13:28.529,0:13:34.489
그러니까 각 기법들이 상호 타협적인것이 
아니라는 겁니다.

0:13:34.490,0:13:37.669
다른 기법들을 사용하더라도
여전히 학습률을 설정 해야만 하는 것이죠.

0:13:37.679,0:13:39.600
어떤 문제에 대해서

0:13:39.600,0:13:43.129
최고의 optimizer를 발견했다고 해도,
여전히 학습률을 설정 해야 합니다.

0:13:43.129,0:13:47.629
여기 그래프가 그 방법을 보여주죠.
사실 이 학습률 기법을, 논문의 발표당시

0:13:48.209,0:13:50.899
없었던 더 진보된 

0:13:51.029,0:13:54.049
Adam 같은 optimizer와 함께 
사용할 수도 있습니다.

0:13:54.050,0:13:57.199
엄청난 임팩트가 있는건 아니지만,
아직 이 기법을 시도한 사람이 

0:13:57.990,0:14:00.589
많지 않으니까 좋은 결과를 보여줄 거에요.

0:14:05.800,0:14:07.010
>> 잘 안들림

0:14:07.010,0:14:14.229
(Adam Optimizer 저자가 그러길, 학습률이 
에포크 동안 변화해야 하는것이고 하지 않았냐?)

0:14:14.420,0:14:16.420
라고 들리는군요...)

0:14:19.040,0:14:20.990
음..

0:14:20.990,0:14:25.570
Adam이나 더 자세한 내용을
클래스 나중에 배우게 될것입니다.

0:14:25.570,0:14:29.640
일단 간단히 대답해 드리면, "아니오" 구요.
Adam 또한 학습률이 있는데

0:14:31.320,0:14:37.740
간단히 말해보자면... 학습률은 앞서 계산된
경사도의 평균으로 나눠(divide) 진 후

0:14:38.690,0:14:43.179
가장 최근의 경사도들의 
루트값들의 합입니다.

0:14:43.180,0:14:48.700
여전히 학습률이라고 불리는 숫자가 있어요.
동적인 학습률이라고 언급하더라도

0:14:49.250,0:14:51.080
여전히

0:14:51.080,0:14:53.080
학습률이 있습니다.

0:14:55.640,0:14:57.640


0:14:59.839,0:15:01.839
모델을 더 좋게

0:15:02.690,0:15:08.649
만들기 위해서 가장 중요한 것은 
더 많은 데이터를 던져 주는 겁니다.

0:15:09.290,0:15:14.589
이 모델에는 수백만개의 파라메터가 있는데,
한동안 이 모델을 학습 시키면

0:15:15.320,0:15:18.640
과적합(overfitting) 이라는 것이 

0:15:19.070,0:15:26.679
시작된다는 난제가 있습니다. 과적합이란 
모델이 모든 이미지의 일반적인 부분이 아니라, 
주어진 이미지의 아주 특정 부분만을

0:15:26.740,0:15:28.740
바라보기 시작한다는 것입니다.

0:15:29.320,0:15:32.820
일반적인 학습이 되어야,
검증 데이터셋에서도 잘 동작할 것입니다.

0:15:33.529,0:15:38.199
과적합을 피하기 위해서 할 수 있는 것은
더 많은 데이터를 구하는 것입니다.

0:15:38.329,0:15:43.329
어디서든지 데이터를 더 많이 구해서
레이블링하는것이 확실한 방법입니다. 

0:15:43.760,0:15:49.119
그러나 "data augmentation"이라고 불리는 
기법을 사용하는게 더 쉬운 방법이고,
항상 사용해야 하는 방법이에요.

0:15:50.540,0:15:55.599
많은 코스들은 이 방법에 대해서 언급조차
하지 않거나

0:15:55.600,0:15:59.560
심화 주제로 코스 마지막에 다루거나 합니다만

0:15:59.560,0:16:04.779
사실상 모델을 더 좋게 만들기 위해서
가장 중요한 방법중 하나입니다.

0:16:04.779,0:16:09.849
이 방법은 fastai 라이브러리에 내장되어 있고,
아주 사용법이 쉽습니다. 관련 코드를 상세히

0:16:09.849,0:16:13.260
잠시후 보게 될 텐데요,

0:16:14.080,0:16:17.320
일단 작성된 최초의 코드를 보면

0:16:18.340,0:16:22.620
ImageClassfierData.from_path 라는 
코드 한 줄이 있습니다.

0:16:22.630,0:16:27.279
여기에 데이터가 있는 경로(path)와 
아키텍처의 크기를 바꾸는

0:16:28.279,0:16:31.838
transform을 파라메터로 줬습니다.
상세한 내용은 잠시 후에 알려드릴게요.

0:16:32.180,0:16:37.839
그리고 나서, 어떤 종류의 "data augmentation"을
사용할지에 대한 파라메터를 추가 했습니다.

0:16:38.600,0:16:40.070


0:16:40.070,0:16:47.529
data augmentation을 이해하려면, 이 기법이
적용된 사진을 직접 보는게 가장 쉬울 겁니다.

0:16:47.529,0:16:51.819
여기에 제가 적용한 결과가 있습니다.

0:16:53.180,0:16:55.010
Image...Data 클래스를 

0:16:55.010,0:17:01.080
6번 생성 했습고, 각 생성때 마다
똑같은 고양이 사진을 출력했습니다.

0:17:01.080,0:17:04.780
어떤일이 일어났는지 보이시나요?

0:17:05.060,0:17:09.620
세번째 고양이는 좀더 왼쪽으로 움직였고
다섯번째는 오른쪽으로 움직였고, 

0:17:09.620,0:17:12.100
네번째는 좌우 반전 되었습니다.

0:17:12.100,0:17:16.300
다른 종류의 이미지를 원하면

0:17:16.520,0:17:24.300
다른 종류의 data augmentation이 필요 합니다.
예를들어서, 문자와 숫자를 인식하고 싶으면

0:17:25.520,0:17:30.400
좌우를 뒤집으면 안되겠죠. 뒤집으면
다른 의미를 가지게 되니까요.

0:17:31.040,0:17:33.699
반면에, 고양이나 강아지

0:17:34.429,0:17:40.329
사진에 대해서는, 상하를 뒤집지 말아야 겠죠.
왜냐하면 동물들이 보통 뒤집어지지 않으니까요.

0:17:40.550,0:17:44.469
또 다른 예로, 현재 진행중인 Kaggle에 보면

0:17:45.050,0:17:46.670
위성 사진에서 

0:17:46.670,0:17:50.859
빙산을 인식하는 경연이 있는데, 여기서는
사진의 위아래를 뒤집어야 할지도 모릅니다.

0:17:51.110,0:17:56.500
왜냐하면 빙산 주변에 뭐가 있는지는
별로 중요한게 아닐 수 있니까요.

0:17:58.250,0:18:01.119
transform을 설정 하는 한가지 예로

0:18:01.370,0:18:06.069
transforms_side_on가 있습니다.
측면으로 찍힌 사진이 있을때

0:18:06.590,0:18:09.939
이 사진은 좌우 뒤집기는 괜찮지만
상하 뒤집기는 안되는등

0:18:10.010,0:18:13.749
모든 변형 가능한 방법을 제공해 줍니다.
측면에 대한 뒤집기와

0:18:14.390,0:18:19.839
약간, 그러나 너무 많이는 아닌 각도의 회전시키
콘트라스트와 밝기를 약간 없애버리기

0:18:21.400,0:18:25.180
그리고 약간 확대나 축소하는 것
또, 약간 위치를 움직이는 것등

0:18:25.180,0:18:28.600
매번 다른 종류의 이미지가 생기게 됩니다.

0:18:30.200,0:18:32.420
>> 사람들이 이런 몇가지 질문을 하곤 해요.

0:18:34.110,0:18:39.410
>> 왜 손실 그래프의 최소 지점이 학습률이 더 높은데 
선택하면 안되는지 설명해 주실 수 있나요?

0:18:39.410,0:18:43.180
또, 학습률 기법이 

0:18:43.480,0:18:46.340
모든 종류의 CNN에 대해서

0:18:46.710,0:18:54.590
잘 동작하는지 궁금합니다.

0:18:58.050,0:19:03.919
(백그라운드 사운드)
옆에 남는 자리 있으면 손들어 주실래요?

0:19:10.650,0:19:15.469
왜 죄저점보다 더 낮은 학습률을 
사용해야 하는지에 대한 

0:19:15.469,0:19:21.700
학습률 발견자에 대한 질문이 있었습니다.
그리고, 학습률 발견자가 무슨 일을 하는지

0:19:22.860,0:19:26.900
이해하기 위해서,
이 그림으로 잠시 돌아가서 어떻게

0:19:28.530,0:19:35.060
어떤 학습률을 사용해야 하는지 알아보죠.
매번 특정 보폭만큼 움직이는데

0:19:35.250,0:19:38.660
각 움직임마다, 학습률을 두배 증가시킬 겁니다.

0:19:39.320,0:19:43.520
다시 말해 보자면

0:19:43.520,0:19:48.680
점점 더 약간씩 큰 보폭으로 움직이는 거죠.

0:19:49.260,0:19:51.260


0:19:51.480,0:19:53.220
좀더 크게

0:19:53.220,0:19:54.990
좀더 크게

0:19:54.990,0:19:56.700
그리고

0:19:56.700,0:20:00.470
이 과정의 목적은 최소지점을 찾느게 아닙니다.

0:20:00.840,0:20:06.949
이 과정의 목적은 빠르게 감소할 수 있게 해주는
학습률을 찾는 것입니다.

0:20:07.530,0:20:13.009
어느 지점에서 손실이 최소냐하면
여기 동그라미 친 부분이겠죠?

0:20:13.009,0:20:17.300
근데 그 지점의 학습률은 너무 커서,
막 앞뒤로 점프하게 

0:20:17.440,0:20:20.300
될 가능성이 있습니다.

0:20:20.460,0:20:29.660
그래서, 그 대신에 뭘 하고 싶냐하면 손실을 빠르게
증가시키는 학습률이 있는 지점으로 돌아가야 합니다.

0:20:30.720,0:20:37.340
지금 보시는 그래프가 매번 증가하는 학습률을 
보여줍니다. 매번이라 하면 각 미니배치 마다죠

0:20:37.420,0:20:40.840
미니배치 횟수에 대한 학습률의 변화입니다.

0:20:41.240,0:20:47.020
이 그래프는 학습률에 대한 손실의 변화입니다.
여기가 최저점이고, 이미 학습률이 너무 높은 곳이죠

0:20:47.780,0:20:52.300
그리고 이 지점이, 약간 앞으로 되돌아간 지점이고
이곳의 손실값은 빠르게 잘 증가합니다.

0:20:53.800,0:20:56.380
stochastic 경사하강법 이라는걸

0:20:56.759,0:20:58.968
잠시 후에 배우게 될 건데요, 

0:20:59.159,0:21:04.549
1e-3 지점이 사실상 더 가파르기 때문에
1e-2 대신 선택할지도 모릅니다.

0:21:04.820,0:21:07.540
이 지점에서 

0:21:07.740,0:21:09.800
더 빠르게

0:21:09.809,0:21:13.189
학습할 수도 있고, 시도해봐도 좋습니다.
하지만 왜

0:21:13.190,0:21:17.869
더 큰 수를 선택하는게, 더 나은 일반화를
가져다 주는지 잠시 후에 아시게 될거에요.

0:21:18.389,0:21:20.389
일단 이 부분은 잠시 미뤄두죠.

0:21:20.399,0:21:22.608
>> "높다"라고 말할때

0:21:23.279,0:21:26.209
>> 높은 학습률을 말한건가요?

0:21:27.059,0:21:29.059
>> 많은 반복이나 다른것 인가요?

0:21:29.849,0:21:35.809
높은 학습률을 말한겁니다. 학습률 발견자에서
반복수를 증가 시키면, 학습률이 높아지죠.

0:21:35.849,0:21:39.588
이 그래프틑 반복수에 대한 학습률의 변화에요.
이 변화에 대해서,

0:21:40.259,0:21:43.639
학습률이 증가하게 되면

0:21:44.039,0:21:48.829
손실은 감소하게 되죠. 그리고 학습률이
너무나 커지는 지점까지 도달하게 되구요.

0:21:49.080,0:21:51.199
그 지점에서는 손실값이 나빠지기 시작하죠.

0:21:51.450,0:21:57.289
>> 최저점이 1e-1 이긴 하지만, 1e-2를 선택해야
한다고 제안하셨 잖아요?

0:21:58.020,0:22:01.760
>>근데 지금 말씀하시는건

0:22:01.760,0:22:05.520
>> 왼쪽의 "높은" 방향으로 
되돌아가야 한다고 한것 같아요.

0:22:05.549,0:22:11.358
그걸 의미한건 아니에요. 죄송합니다.
더 낮은 학습률로

0:22:11.999,0:22:16.249
되돌아가야 한다고 말한 겁니다. 

0:22:17.129,0:22:21.228
>> 그러니까 낮은거라는 거죠?
네 맞습니다.

0:22:23.159,0:22:28.549
>> 지난 클래스에서, 모든 지역적 최저점들이 
동일하다고 했고

0:22:29.180,0:22:35.720
>> 이 그래프 역시 그 사실을 보여주는데요
그 사실이 관찰된 건가요? 아니면 
어떤 이론적 바탕에 기반한 건가요?

0:22:36.340,0:22:39.020
그건 이 그래프가 보여주려는게 아닙니다.

0:22:39.270,0:22:43.759
이 그래프는 단순히 학습률이 더 증가할때
더 나아지지 않는 지점이 

0:22:44.100,0:22:48.500
있다는걸 보여주고, 심지어 
더 나빠지기 시작한다는걸 보여주죠.

0:22:49.500,0:22:52.640
지역적 최저점들이 동일하다는건

0:22:53.520,0:22:55.320
완전히 다른 문제에요

0:22:55.740,0:23:00.100
완전히 다른거요. 나중에 이걸 설명하는
그림을 다시 보여드릴게요.

0:23:02.900,0:23:09.740
>> 매 에포크 마다, 기본이 되는 학습률을
찾아야만 하나요?

0:23:10.740,0:23:12.419
매 어떤거요?

0:23:12.420,0:23:14.640
>> 에포크요.
아 에포크 말이군요.

0:23:14.640,0:23:21.920
>> 학습을 진행하는 동안얼마나 많이 
학습률 발견자를 수행해야 하나요?

0:23:24.020,0:23:26.700
매우 좋은 질문입니다.

0:23:27.580,0:23:31.540
시작할때 일단 한번 수행하는건 확실합니다.

0:23:32.420,0:23:36.200
클래스 나중 부분에가면, 계층을 "unfreeze"하는 것에
대해서 배우게 될겁니다.

0:23:36.380,0:23:37.460
그리고

0:23:37.460,0:23:43.180
계층을 "unfreeze"한 후에, 학습률 발견자를 
한번 더 수행합니다. 제가 만약 학습에 관련해서

0:23:43.340,0:23:48.480
뭔가 변화를 준다면, 학습률 발견자를 
기본적으로 다시 수행해야 할 지도 모릅니다.

0:23:48.740,0:23:52.960
만약 계층을 "unfreeze" 하는 것 처럼 
학습하는 방법에 대한 뭔가를 바꾸게 되면

0:23:53.640,0:23:58.160
다음(다른) 학습이 불안정 하거나 너무 느리다는걸

0:23:58.920,0:24:03.920
알 수 있을텐데요, 그러면 이때 학습률 발견자를 다시
수행해 볼 수 있을겁니다. 이를 수행함에 있어서

0:24:04.180,0:24:07.580
해로운건 없어요. 오래 걸리지도 않고요.

0:24:08.160,0:24:10.110
좋은 질문이었습니다.

0:24:10.110,0:24:17.300
data augmentation으로 돌아가서, 
tfms_from_model을 수행할때

0:24:18.060,0:24:22.960
aug_tfms 인자의 값으로 주로 전달하는 것으로

0:24:23.820,0:24:30.320
transforms_side_on, transforms_top_down 또는
나중에 배우게 될 자신만의 transform 목록이 

0:24:31.170,0:24:36.920
될 수 있습니다. 일단 고양이와 개 사진은 측면에서
촬영된 것이어서 transform_side_on을 사용 했습니다.

0:24:37.280,0:24:42.520
각 이미지를 보게 되면, 약간의 확대/축소가 되거나
약간 위치가 이동하거나

0:24:43.000,0:24:45.880
약간 회전되거나

0:24:46.890,0:24:53.780
뒤집어져 있을 수 있습니다. data augmentaion은
완전히 새로운 데이터를 만드는 것은 아니지만

0:24:54.420,0:25:00.000
동일한 사진을 바라보는 다른 방식들을 의미합니다.
CNN이 관심있어하는 방식으로, 사실상

0:25:00.500,0:25:04.460
고양이와 개를 인식하는법의 학습을 가능하게 해주죠

0:25:05.130,0:25:11.690
누군가가 data augmentation이라고 하면, 
문제의 특정 분야 지식에 기반해서

0:25:13.350,0:25:20.000
같은 이미지를 조작할 수 있는 다른 방법들을 의미합니다. 
사람이 볼땐, 동일한 이미지 이지요.

0:25:20.000,0:25:24.260
그리고 조작된 이미지는 현실 세계에서도
존재할 것이라고 기대될 수 있어야 합니다.

0:25:25.830,0:25:30.710
그러면, 지금부터 우리가 할 수 있는건
from_path 메소드에서

0:25:30.710,0:25:37.279
tfms 인자로 transforms의 목록들을 전달하는 것입니다
이 목록이 곧 data augmentation의 방법들이구요

0:25:37.950,0:25:39.950


0:25:41.400,0:25:46.020
그리고나서, 우리가 수행하는건 fit 이 됩니다.

0:25:47.160,0:25:49.360
초기에는

0:25:49.840,0:25:57.060
data augmentation은 초기엔 아무것도 하지 않습니다.
왜냐하면, pretrained 메소드의 precompute의 값을

0:25:57.400,0:26:02.240
True로 설정했기 때문입니다. 마찬가지로
앞으로 여러번 이것에 대해서 이야기 하게 될겁니다.

0:26:03.160,0:26:09.500
이 값이 의미하는걸 설명하기 위해서,
전에 각 계층이 학습하는것을 시각화한 사진을 보셨죠?

0:26:10.520,0:26:14.900
왼쪽에 있는 활성화된 파라메터가 오른쪽의
꽃에서 부터

0:26:16.380,0:26:19.420
조류의 눈이라던지

0:26:19.430,0:26:24.339
찾는데요, 무슨일이 일어났던 걸까요?

0:26:25.550,0:26:32.889
CNN의 나중 계층들은 "activation(활성화)"라고 
불리는 것들이 있는데, 활성화의 의미는

0:26:32.890,0:26:37.689
어떤 숫자입니다. 이 숫자는 조류의 눈과 같은

0:26:38.840,0:26:44.680
특징이 이 위치에 존재할 확률에 대한
신뢰도를 보여줍니다.

0:26:44.960,0:26:48.610
나중에 이 내용을 많이 보게 되겠지만

0:26:49.520,0:26:52.869
미리 학습된 네트워크를 가지고 있을때

0:26:53.330,0:26:58.689
미리 학습되었다는건, 이미 뭔가를 인식하기 위해서
학습되었다는 것이죠. 

0:26:58.880,0:27:04.630
이 예제에서는 ImageNet 데이터셋에 있는
150만개의 이미지를 인식하기 위해 학습되었고요

0:27:04.630,0:27:09.360
그러면 이때 우리가 할 수 있는건
사물이 뭔지를 알아내기에 모든 충분한

0:27:10.020,0:27:14.020
정보를 가지고 있는
마지막에서 두번째인 계층에 대한

0:27:14.020,0:27:20.770
activation을 저장하는 것입니다.
여기서 저장한다는건

0:27:20.770,0:27:25.810
모든 이미지에 대하여 
조류의 눈, 강아지 얼굴, 또는 푹신한 귀등과 같은
것에 대한 

0:27:26.240,0:27:31.479
단계의 activation들을 저장한다는 의미 입니다.

0:27:32.270,0:27:35.739
우리는 이것들을 흔히 미리 계산된 
activations 이라고도 부르죠

0:27:35.740,0:27:41.020
우리가 새로운 분류 모델을 생성하려고 할 때

0:27:41.510,0:27:45.489
기본적으로 이 미리 계산된 activation의 
이점을 활용할 수 있는데

0:27:46.040,0:27:52.689
많은 내용이 이미 저장되어 있어서, 아주 빠르게
학습할 수 있게 됩니다. 이것에 기반하면

0:27:53.240,0:27:57.199
간단한 선형적 모델을 아주 빠르게 학습 시킬 수 있습니다.
설명드린게 precompute을 True로 

0:27:57.200,0:28:04.549
설정 했을때 발생하는 일입니다. 눈치 채셨겠지만
이번주에 처음으로, 새로운 모델을 돌렸어도

0:28:04.889,0:28:07.008
1~2분만 소요된 이유죠.

0:28:07.590,0:28:10.519
제가 돌렸을때는 5~10초정도 걸렸었구요.

0:28:10.620,0:28:13.939
여러분은 activation들의 precompute를 
수행해야 해서 1~2분 걸렸던 겁니다.

0:28:14.789,0:28:20.209
본인 컴퓨터나 AWS를 사용하면, 
precompute 는 한번만 수행되면 됩니다.

0:28:20.249,0:28:22.249
한번만 말입니다.

0:28:22.470,0:28:26.869
만약 Crestle을 사용하고 계신다면,
매번 Crestle을 다시 실행할때 마다 

0:28:27.330,0:28:31.610
매번 precompute를 수행해줘야 합니다.

0:28:32.399,0:28:34.309
미리 계산된 activation에 대해서

0:28:34.309,0:28:39.739
crestle은 어떤 특별한 공간을 사용하는데,
crestle이 재실행 될때마다 사라지는 특징이 있습니다

0:28:40.200,0:28:44.869
crestle과 같은 특별한 경우가 아니라면,
일반적으로 특정 데이터셋에 대해서 한번만

0:28:45.389,0:28:47.449
수행되면 됩니다.

0:28:49.110,0:28:54.019
이 부분에 대한 문제점은
각 이미지에 대해서 precompute를 사용하기 때문에

0:28:54.720,0:28:58.960
여기의 얼마나 많은 부분이 귀이며
저기의 얼마나 많은 부분이 도룡뇽의 눈이 

0:28:59.280,0:29:05.090
있는지등에 대해서 data augmentation이 
잘 동작하지 않을 것이라는 겁니다. 다시 말해보면

0:29:05.090,0:29:11.809
매번 다른 버전의 고양이 이미지를 보여줄텐데, 어떤 특정 버전의 고양이만의 activation이 미리 계산됩니다.

0:29:12.749,0:29:17.868
data augmentation을 사용하기 위해서는
단순히 learn.precompute의 값을 False로 설정하면 됩니다.

0:29:18.419,0:29:20.929
그리고나서몇번의 추가 에포크를 

0:29:21.720,0:29:23.129
수행하면 되죠.

0:29:23.129,0:29:27.469
보다시피, 더 많은 에포크를 수행함에 따라서

0:29:28.049,0:29:33.799
정확도가 딱히 더 좋아지지 않는걸 알 수 있습니다.
별로 좋은 소식은 아니지만, 다른 좋은 소식은

0:29:34.679,0:29:37.189
모델의 에러를 측정하는 방법인

0:29:37.710,0:29:41.720
학습 손실이 줄어들어서 좋아진다고 
볼 수 있다는 것입니다.

0:29:41.720,0:29:46.249
하지만, 검증 에러는 줄어들이 않아서

0:29:46.649,0:29:51.649
과적합이 발생하지도 않는 상황입니다.
과적합은 학습 손실이 검증 손실보다 훨씬 

0:29:52.110,0:29:58.039
낮다는 것이고, 코스 진행동안에 이에 대해서
엄청나게 많이 이야기 하게 될 것입니다.

0:29:58.039,0:29:59.009
어쨌든

0:29:59.009,0:30:02.059
학습 데이터셋에 대해서

0:30:02.220,0:30:07.069
검증 데이터셋 보다 훨씬 잘 동작한다면, 
모델이 일반화되지 않았다는 의미라는 거죠.

0:30:07.780,0:30:12.389
이 예제에서는 그런 상황이 발생하지 않았고, 좋은 상태
입니다. 하지만 많은 향상이 이루어지지도 않았어요.

0:30:13.300,0:30:16.289
그래서 이 문제를 다룰 방법을 생각해 내야만 하고요.

0:30:16.860,0:30:21.680
그러기 전에 먼저, 한가지 멋진 요령을
알려드리겠습니다.

0:30:21.920,0:30:24.240
cycle_len을 1로 설정하는 건데요

0:30:24.400,0:30:26.800
이건 이것대로 또 매우 흥미로운 내용입니다.

0:30:27.900,0:30:30.460
그게 뭐냐하면

0:30:30.460,0:30:32.579
cycle_len을 1로 설정하게 되면

0:30:33.250,0:30:40.800
딥러닝에 사용된 가장 최근에 발견된 
stochastic 경사하강을 재시작하게 됩니다.

0:30:41.440,0:30:42.760
최저점에

0:30:42.760,0:30:47.849
점점 더 점점 더

0:30:48.520,0:30:50.520
가까워 질수록

0:30:51.850,0:30:58.020
학습률을 줄여나가고 싶을거에요.
왜냐하면, 최저점에 가까워지기 시작하면

0:30:58.020,0:31:05.129
보폭을 줄여서 정확히 그 지점에
도달하길 원할 테니까요.

0:31:05.460,0:31:09.660
그래서, 학습을 진행하면 할수록

0:31:12.020,0:31:14.020
학습률은

0:31:14.320,0:31:16.740
아마도 점점 감소하게 될겁니다.

0:31:16.880,0:31:21.680
왜냐하면 정확히 원하는 최저점이라는
지점에 점점 더 가까이

0:31:21.820,0:31:24.260
도달하려고 하기 때문이죠.

0:31:24.260,0:31:29.940
학습이 진행되는 동안 학습률을 줄여나가는
이 발상은 annealing(냉각)

0:31:29.940,0:31:32.080
이라고 불리는데

0:31:32.080,0:31:35.820
아주아주 일반적이고, 아주아주 인기있는 방식입니다.

0:31:36.590,0:31:39.160
모든사람이 항상 사용한다고 볼 수 있을 거에요.

0:31:39.920,0:31:42.519
가장 일반적인 종류의 학습률 annealing은

0:31:43.400,0:31:50.050
소름끼치게 진부합니다. 기본적으로 
한동안 잘 동작할 것 같은 학습률을 선택합니다.

0:31:50.050,0:31:54.009
그리고나서, 학습이 더이상 잘 안되면
학습률을 10배 줄입니다.

0:31:54.010,0:31:58.809
이런식으로 학습률을 10배씩 더 줄여나가는데,
더이상 아무런 향상도 없을때 까지 반복 합니다.

0:31:59.200,0:32:05.360
대부분의 대학 연구 논문이나, 산업에 종사자들이
사용하는 방법이에요. 단계적인 annealing을 수작업으로

0:32:05.520,0:32:09.340
수행하는건데, 좀 짜증나는 방식이죠.

0:32:09.520,0:32:15.280
더 나은 접근방법은 단순히 직선과 같은 어떤 
함수적인 형태를 선택하는 것입니다.

0:32:16.010,0:32:20.650
정말 좋은 함수의 형태는 
코사인 함수 절반의 형태라는게 드러났죠.

0:32:22.710,0:32:27.319
최저점에 전혀 가깝지 않을때는

0:32:27.320,0:32:30.650
큰 학습률의 값이 설정되어 있을테고
그리고, 최저점에 아주 가까워질때

0:32:30.650,0:32:36.220
재빨리 학습률을 아주 낮은 학습률로 줄여서,
최저점까지 적은 수의 반복으로 내려가게 됩니다.

0:32:36.540,0:32:38.540
이 방법이 코사인 annealing 이라고 합니다.

0:32:38.900,0:32:45.460
삼각법을 공부한지 좀 지난 분들이 계시다면
코사인이란 기본적으로 이렇게 생긴 그래픕니다.

0:32:46.000,0:32:49.040
그리고 여기서, 반쪽의 영역이 되는거죠

0:32:49.440,0:32:53.280
우리는 코사인 annealing을 사용하게 될 겁니다.

0:32:54.570,0:32:56.570
근데 우리의 문제가

0:32:56.730,0:32:58.730
매우 고차원적인 공간의 것이라면

0:32:59.480,0:33:04.260
이 그림은 단지 3차원 형태만 보여드리긴 하지만

0:33:04.380,0:33:08.120
현실적으로 수백만 차원의 공간이 존재한다는걸
알고 계시길 바랍니다.

0:33:08.120,0:33:09.660
이 예제에서 보면

0:33:09.660,0:33:11.420
많은 수의

0:33:11.430,0:33:18.400
평평한 지점이 있습니다. 지역적 최소점은 아니지만,
꽤나 평평해서 좋아 보이는 지점이 있습니다.

0:33:18.540,0:33:23.200
그런데 이 평평한 지점들이 흥미로운것은
약간 다른데 있는데요

0:33:23.200,0:33:32.800
잠시만요, 보여드릴게요

0:33:32.860,0:33:39.960
제가 그리는 그래프처럼 생긴 표면이 있다고
일단 상상해 봅시다.

0:33:39.960,0:33:46.160
그리고 무작위로 시작지점을 오른쪽 위로
정했다고 상상해 봅시다.

0:33:46.940,0:33:51.380
그러면, annealing 스케줄에 의해서, 초기 학습률이
여기 아래로 내려가게 됩니다.

0:33:51.460,0:33:53.300
그리고 이때

0:33:53.300,0:33:59.230
꽤 좋아보이는 낮은 에러 지점입니다.
근데 문제는 일반화가 잘 안된다는 점입니다.

0:33:59.230,0:34:05.529
왼쪽/오른쪽 방향에 대해서 약간만 다를 수 있는
만약 다른 종류의 데이터셋을 사용한다면

0:34:06.340,0:34:10.380
갑자기 최악의 해답이 나오게 됩니다.

0:34:10.700,0:34:12.940
반면에 왼쪽 하단 지점은 기본적으로

0:34:13.020,0:34:15.159
손실에 대해서 동등하게 좋습니다.

0:34:15.159,0:34:20.829
그리고 여기서는 약간 다른 데이터셋이 대상이 되어도
여전히 좋은 해답이 나오게 되죠.

0:34:20.860,0:34:27.560
다시 말하면, 우리가 원하는건 왼쪽 하단의 해답입니다.
이 해답이 다른쪽의 녀석보다

0:34:27.560,0:34:31.400
일반화가 더 잘되었다고 볼 수 있는거죠.

0:34:31.400,0:34:38.020
그러면 이제 우리가 해야할 일을 말씀드리죠.
여기 보시면 여러개의 죄저 지즘이 있습니다.

0:34:39.200,0:34:45.099
그리고 표준적인 annealing 접근법은 점점 언덕의 
아래로 아래로, 최종적으로 한 지점으로 빠집니다.

0:34:45.700,0:34:50.400
근데, 대신에 우리는 학습률 스케줄이라는 것을
사용할 수 있습니다.

0:34:50.440,0:34:53.640
여기 보시는 그래프 같은 거에요.

0:34:53.690,0:34:59.200
코사인 annealing을 한 후 갑자기 다시 위로 점프, 
또 다시 코사인 annealing, 그리고 점프를 보여줍니다.

0:34:59.530,0:35:03.969
매번 위로 점프한다는건
만약 뾰족뾰족한 지점이 있다면

0:35:03.970,0:35:08.800
갑자기 학습률을 증가시켜서
완전히 다른 지점으로 점프시킨다는걸 말합니다.

0:35:09.470,0:35:12.459


0:35:12.460,0:35:15.099


0:35:15.859,0:35:19.299


0:35:19.300,0:35:24.729


0:35:24.740,0:35:28.419


0:35:29.089,0:35:31.089


0:35:31.580,0:35:33.580


0:35:40.570,0:35:46.469


0:35:46.780,0:35:49.409


0:35:50.620,0:35:53.399


0:35:54.190,0:35:56.190


0:35:56.230,0:35:58.230


0:36:00.130,0:36:05.069


0:36:05.740,0:36:10.560


0:36:12.310,0:36:18.630


0:36:19.450,0:36:21.450


0:36:21.760,0:36:28.320


0:36:29.010,0:36:30.820


0:36:30.820,0:36:37.110


0:36:37.420,0:36:40.740


0:36:41.230,0:36:47.640


0:36:48.160,0:36:51.780


0:36:52.360,0:36:56.519


0:36:57.520,0:36:59.110


0:36:59.110,0:37:02.099


0:37:02.800,0:37:04.800


0:37:05.110,0:37:10.979


0:37:12.010,0:37:16.739


0:37:16.740,0:37:22.260


0:37:22.330,0:37:26.789


0:37:27.460,0:37:33.780


0:37:33.780,0:37:34.860


0:37:34.860,0:37:37.679


0:37:37.960,0:37:40.919


0:37:42.130,0:37:48.119


0:37:48.820,0:37:54.299


0:37:54.580,0:37:59.460


0:37:59.460,0:38:03.509


0:38:05.950,0:38:11.819


0:38:12.610,0:38:15.180


0:38:15.180,0:38:18.659


0:38:18.660,0:38:26.339


0:38:27.190,0:38:31.559


0:38:32.110,0:38:36.450


0:38:36.520,0:38:42.180


0:38:42.430,0:38:44.380


0:38:44.380,0:38:51.599


0:38:53.470,0:38:59.760


0:39:00.820,0:39:02.820


0:39:03.220,0:39:07.680


0:39:08.080,0:39:13.259


0:39:13.260,0:39:18.239


0:39:18.240,0:39:21.570


0:39:22.630,0:39:24.630


0:39:27.040,0:39:31.650


0:39:32.470,0:39:40.290


0:39:41.260,0:39:43.260


0:39:43.300,0:39:50.850


0:39:51.730,0:39:54.570


0:39:54.880,0:40:01.369


0:40:01.830,0:40:04.489


0:40:09.420,0:40:11.420


0:40:14.100,0:40:18.350


0:40:19.470,0:40:24.290


0:40:25.050,0:40:27.889


0:40:31.830,0:40:33.830


0:40:34.200,0:40:36.200


0:40:36.359,0:40:40.129


0:40:40.740,0:40:45.229


0:40:45.780,0:40:47.150


0:40:47.150,0:40:51.290


0:40:51.900,0:40:56.210


0:40:56.640,0:41:01.670


0:41:02.670,0:41:04.670


0:41:05.369,0:41:07.369


0:41:08.100,0:41:15.709


0:41:15.900,0:41:19.310


0:41:20.090,0:41:22.090


0:41:24.630,0:41:26.630


0:41:27.570,0:41:29.570


0:41:30.210,0:41:32.210


0:41:32.520,0:41:36.889


0:41:37.680,0:41:39.680


0:41:40.200,0:41:48.020


0:41:48.540,0:41:56.389


0:41:56.609,0:41:57.869


0:41:57.869,0:42:03.679


0:42:04.109,0:42:08.659


0:42:09.540,0:42:11.749


0:42:13.200,0:42:19.159


0:42:19.160,0:42:21.160


0:42:22.020,0:42:26.089


0:42:26.090,0:42:30.650


0:42:33.180,0:42:35.180


0:42:36.360,0:42:43.640


0:42:49.020,0:42:51.439


0:42:52.530,0:42:58.999


0:42:59.760,0:43:03.169


0:43:04.560,0:43:07.730


0:43:09.180,0:43:11.180


0:43:11.310,0:43:17.390


0:43:17.910,0:43:21.049


0:43:21.360,0:43:27.230


0:43:27.360,0:43:28.760


0:43:28.760,0:43:31.309


0:43:31.770,0:43:36.199


0:43:37.980,0:43:43.219


0:43:43.500,0:43:46.669


0:43:49.140,0:43:50.460


0:43:50.460,0:43:53.689


0:43:54.540,0:43:58.220


0:43:59.040,0:44:01.040


0:44:03.150,0:44:04.950


0:44:04.950,0:44:12.559


0:44:14.850,0:44:16.850


0:44:17.010,0:44:18.900


0:44:18.900,0:44:20.900


0:44:21.900,0:44:23.900


0:44:24.960,0:44:27.949


0:44:28.650,0:44:30.829


0:44:33.059,0:44:35.059


0:44:35.369,0:44:37.369


0:44:37.650,0:44:43.789


0:44:44.400,0:44:51.319


0:44:52.559,0:44:55.639


0:44:56.460,0:45:01.220


0:45:01.220,0:45:04.909


0:45:05.220,0:45:10.309


0:45:10.920,0:45:14.720


0:45:14.970,0:45:19.579


0:45:21.029,0:45:23.029


0:45:23.609,0:45:28.909


0:45:30.240,0:45:32.190


0:45:32.190,0:45:39.289


0:45:40.289,0:45:47.539


0:45:47.539,0:45:49.999


0:45:50.880,0:45:53.809


0:45:55.470,0:45:57.420


0:45:57.420,0:46:00.139


0:46:01.170,0:46:07.549


0:46:07.549,0:46:10.849


0:46:10.920,0:46:17.629


0:46:17.690,0:46:22.220


0:46:22.950,0:46:24.539


0:46:24.539,0:46:31.609


0:46:32.369,0:46:37.159


0:46:38.909,0:46:42.829


0:46:43.679,0:46:50.629


0:46:51.419,0:46:55.158


0:46:55.159,0:46:59.239


0:46:59.339,0:47:03.078


0:47:03.779,0:47:08.718


0:47:09.179,0:47:16.249


0:47:16.380,0:47:19.400


0:47:20.400,0:47:26.749


0:47:27.390,0:47:29.390


0:47:29.640,0:47:32.209


0:47:33.390,0:47:34.829


0:47:34.829,0:47:41.478


0:47:41.640,0:47:43.529


0:47:43.529,0:47:45.529


0:47:46.019,0:47:52.158


0:47:52.859,0:47:53.669


0:47:53.669,0:47:59.719


0:47:59.719,0:48:01.079


0:48:01.079,0:48:03.768


0:48:04.679,0:48:07.999


0:48:08.819,0:48:13.369


0:48:13.369,0:48:19.338


0:48:24.959,0:48:26.959


0:48:28.169,0:48:33.949


0:48:34.859,0:48:37.578


0:48:40.049,0:48:41.699


0:48:41.699,0:48:47.449


0:48:47.449,0:48:49.579


0:48:50.130,0:48:51.970


0:48:51.970,0:48:56.309


0:48:57.249,0:49:01.439


0:49:03.640,0:49:09.269


0:49:09.940,0:49:17.339


0:49:17.339,0:49:20.758


0:49:21.910,0:49:28.319


0:49:29.049,0:49:33.538


0:49:33.539,0:49:39.359


0:49:40.089,0:49:43.169


0:49:43.960,0:49:45.960


0:49:48.759,0:49:53.009


0:49:55.119,0:50:01.048


0:50:01.869,0:50:06.568


0:50:07.719,0:50:13.859


0:50:15.350,0:50:16.490


0:50:16.490,0:50:23.180


0:50:28.980,0:50:36.439


0:50:38.160,0:50:41.539


0:50:41.539,0:50:45.799


0:50:45.839,0:50:49.758


0:50:50.099,0:50:56.599


0:50:57.269,0:50:59.508


0:51:02.849,0:51:07.159


0:51:07.619,0:51:14.629


0:51:19.440,0:51:26.210


0:51:26.210,0:51:30.199


0:51:30.990,0:51:35.809


0:51:36.960,0:51:38.960


0:51:43.470,0:51:47.779


0:51:48.539,0:51:52.008


0:51:52.769,0:51:57.529


0:51:58.259,0:52:00.259


0:52:00.630,0:52:06.740


0:52:06.779,0:52:09.799


0:52:10.589,0:52:13.758


0:52:14.190,0:52:19.069


0:52:21.119,0:52:25.639


0:52:28.589,0:52:31.909


0:52:31.910,0:52:35.180


0:52:35.820,0:52:37.070


0:52:37.070,0:52:44.509


0:52:44.910,0:52:47.000


0:52:47.849,0:52:49.680


0:52:49.680,0:52:51.680


0:52:51.780,0:52:59.239


0:52:59.700,0:53:05.599


0:53:06.000,0:53:11.419


0:53:11.910,0:53:19.700


0:53:20.339,0:53:24.979


0:53:25.380,0:53:29.300


0:53:29.910,0:53:33.319


0:53:34.500,0:53:37.609


0:53:37.609,0:53:39.609


0:53:39.630,0:53:46.579


0:53:47.069,0:53:51.949


0:53:51.950,0:53:55.520


0:53:57.810,0:54:01.879


0:54:02.790,0:54:06.109


0:54:06.109,0:54:11.299


0:54:11.339,0:54:16.039


0:54:16.040,0:54:23.269


0:54:24.329,0:54:29.869


0:54:30.480,0:54:35.179


0:54:35.400,0:54:39.049


0:54:39.869,0:54:44.478


0:54:45.390,0:54:52.729


0:54:53.759,0:54:58.039


0:54:59.190,0:55:03.019


0:55:04.769,0:55:09.409


0:55:09.930,0:55:15.859


0:55:15.859,0:55:17.190


0:55:17.190,0:55:21.139


0:55:21.299,0:55:25.008


0:55:28.680,0:55:34.669


0:55:39.660,0:55:41.660


0:55:42.119,0:55:47.808


0:55:49.170,0:55:51.170


0:55:51.869,0:55:56.838


0:55:58.920,0:56:02.359


0:56:02.940,0:56:08.509


0:56:08.969,0:56:11.269


0:56:11.789,0:56:16.069


0:56:16.170,0:56:18.349


0:56:19.469,0:56:26.659


0:56:30.560,0:56:34.749


0:56:35.360,0:56:40.809


0:56:40.810,0:56:43.090


0:56:49.730,0:56:55.330


0:56:55.369,0:57:02.858


0:57:03.380,0:57:09.580


0:57:11.990,0:57:16.929


0:57:20.720,0:57:27.909


0:57:29.720,0:57:35.649


0:57:36.560,0:57:41.709


0:57:42.740,0:57:44.590


0:57:44.590,0:57:52.209


0:57:52.210,0:57:56.649


0:57:56.650,0:57:57.160


0:57:57.160,0:58:03.310


0:58:03.470,0:58:06.159


0:58:06.710,0:58:11.230


0:58:11.230,0:58:13.150


0:58:13.150,0:58:19.150


0:58:19.880,0:58:20.530


0:58:20.530,0:58:27.879


0:58:28.790,0:58:35.709


0:58:36.380,0:58:42.160


0:58:42.560,0:58:47.499


0:58:47.500,0:58:51.219


0:58:54.109,0:58:55.640


0:58:55.640,0:58:59.710


0:59:00.310,0:59:02.310


0:59:02.720,0:59:07.660


0:59:08.570,0:59:14.110


0:59:14.750,0:59:20.680


0:59:20.750,0:59:26.500


0:59:26.810,0:59:29.590


0:59:29.590,0:59:33.789


0:59:35.270,0:59:37.190


0:59:37.190,0:59:40.510


0:59:40.700,0:59:46.150


0:59:46.150,0:59:48.729


0:59:48.730,0:59:53.620


0:59:53.620,1:00:00.910


1:00:01.460,1:00:03.460


1:00:05.090,1:00:07.600


1:00:08.780,1:00:14.380


1:00:14.930,1:00:15.820


1:00:15.820,1:00:19.299


1:00:19.670,1:00:26.319


1:00:26.320,1:00:28.719


1:00:29.450,1:00:30.530


1:00:30.530,1:00:32.170


1:00:32.170,1:00:35.139


1:00:35.330,1:00:39.640


1:00:39.890,1:00:46.449


1:00:47.030,1:00:51.340


1:00:51.350,1:00:56.229


1:00:56.230,1:00:58.630


1:00:58.630,1:01:00.630


1:01:00.710,1:01:06.250


1:01:06.250,1:01:09.610


1:01:10.200,1:01:12.200


1:01:13.000,1:01:16.199


1:01:17.590,1:01:25.559


1:01:25.660,1:01:31.290


1:01:31.960,1:01:33.960


1:01:36.849,1:01:38.470


1:01:38.470,1:01:40.830


1:01:41.440,1:01:46.169


1:01:46.170,1:01:48.359


1:01:48.359,1:01:53.159


1:01:53.160,1:01:55.160


1:01:55.900,1:02:00.930


1:02:01.480,1:02:06.240


1:02:06.790,1:02:12.209


1:02:12.210,1:02:19.230


1:02:19.869,1:02:24.838


1:02:28.390,1:02:30.390


1:02:30.910,1:02:36.120


1:02:36.120,1:02:40.979


1:02:41.560,1:02:46.019


1:02:46.020,1:02:49.739


1:02:50.440,1:02:52.440


1:02:53.140,1:02:55.140


1:02:58.120,1:03:01.229


1:03:01.870,1:03:05.489


1:03:05.920,1:03:09.750


1:03:09.750,1:03:13.860


1:03:14.320,1:03:17.249


1:03:17.350,1:03:22.049


1:03:22.050,1:03:23.350


1:03:23.350,1:03:27.149


1:03:35.410,1:03:41.010


1:03:42.190,1:03:44.190


1:03:44.680,1:03:45.880


1:03:45.880,1:03:47.440


1:03:47.440,1:03:50.399


1:03:52.630,1:03:58.560


1:03:59.110,1:04:01.739


1:04:02.590,1:04:08.489


1:04:08.490,1:04:14.070


1:04:15.670,1:04:18.480


1:04:23.500,1:04:29.280


1:04:29.560,1:04:31.560


1:04:32.500,1:04:35.820


1:04:36.520,1:04:38.939


1:04:40.349,1:04:44.639


1:04:44.859,1:04:46.859


1:04:47.200,1:04:52.139


1:04:52.140,1:04:58.890


1:04:59.739,1:05:01.739


1:05:02.200,1:05:05.759


1:05:06.430,1:05:07.960


1:05:07.960,1:05:15.059


1:05:15.759,1:05:17.049


1:05:17.049,1:05:18.700


1:05:18.700,1:05:21.539


1:05:23.619,1:05:28.978


1:05:28.979,1:05:32.729


1:05:34.490,1:05:41.510


1:05:43.440,1:05:49.730


1:05:51.660,1:05:53.660


1:05:54.089,1:05:57.109


1:05:57.990,1:06:01.699


1:06:02.940,1:06:04.940


1:06:05.730,1:06:07.079


1:06:07.079,1:06:08.880


1:06:08.880,1:06:10.940


1:06:11.550,1:06:15.199


1:06:15.960,1:06:19.879


1:06:19.880,1:06:25.309


1:06:26.040,1:06:32.629


1:06:33.630,1:06:38.390


1:06:38.390,1:06:42.319


1:06:42.319,1:06:43.980


1:06:43.980,1:06:47.000


1:06:47.280,1:06:52.399


1:06:53.910,1:06:59.119


1:06:59.910,1:07:02.839


1:07:02.839,1:07:05.929


1:07:06.869,1:07:10.939


1:07:11.520,1:07:18.770


1:07:19.619,1:07:22.009


1:07:22.890,1:07:27.740


1:07:29.339,1:07:31.220


1:07:31.220,1:07:36.859


1:07:36.930,1:07:40.069


1:07:40.260,1:07:43.969


1:07:44.730,1:07:46.730


1:07:47.599,1:07:50.298


1:07:51.539,1:07:59.059


1:07:59.059,1:08:01.969


1:08:02.729,1:08:08.959


1:08:10.349,1:08:16.818


1:08:16.819,1:08:22.548


1:08:22.710,1:08:30.109


1:08:31.109,1:08:34.579


1:08:35.489,1:08:41.479


1:08:42.179,1:08:47.269


1:08:47.940,1:08:50.179


1:08:51.299,1:08:54.979


1:08:54.980,1:09:00.230


1:09:00.230,1:09:04.669


1:09:06.270,1:09:08.330


1:09:09.119,1:09:12.679


1:09:13.650,1:09:19.040


1:09:23.069,1:09:30.169


1:09:30.690,1:09:32.690


1:09:33.690,1:09:38.000


1:09:38.339,1:09:40.339


1:09:41.040,1:09:43.040


1:09:43.199,1:09:44.509


1:09:44.509,1:09:48.439


1:09:48.509,1:09:53.329


1:09:53.909,1:09:56.179


1:09:57.000,1:09:58.020


1:09:58.020,1:10:00.259


1:10:00.860,1:10:06.679


1:10:07.590,1:10:14.659


1:10:18.060,1:10:20.719


1:10:22.110,1:10:29.600


1:10:29.600,1:10:34.879


1:10:34.880,1:10:38.270


1:10:38.820,1:10:43.009


1:10:44.550,1:10:47.360


1:10:48.449,1:10:50.658


1:10:51.540,1:10:53.540


1:10:54.389,1:10:56.389


1:11:03.510,1:11:10.310


1:11:10.310,1:11:14.060


1:11:14.060,1:11:18.859


1:11:18.860,1:11:22.790


1:11:24.690,1:11:26.219


1:11:26.219,1:11:27.600


1:11:27.600,1:11:32.689


1:11:32.760,1:11:34.760


1:11:35.969,1:11:37.969


1:11:38.699,1:11:41.719


1:11:46.590,1:11:48.590


1:11:51.060,1:11:53.509


1:11:54.270,1:11:56.929


1:11:57.570,1:12:01.130


1:12:01.590,1:12:06.409


1:12:06.719,1:12:14.149


1:12:14.690,1:12:19.190


1:12:19.469,1:12:23.929


1:12:23.930,1:12:26.329


1:12:26.820,1:12:33.259


1:12:33.260,1:12:39.980


1:12:40.710,1:12:46.430


1:12:46.739,1:12:50.599


1:12:50.940,1:12:56.600


1:12:57.480,1:12:59.480


1:13:00.150,1:13:03.109


1:13:03.110,1:13:10.130


1:13:10.680,1:13:13.309


1:13:13.770,1:13:18.770


1:13:18.770,1:13:25.910


1:13:26.580,1:13:32.600


1:13:33.120,1:13:35.070


1:13:35.070,1:13:40.850


1:13:41.850,1:13:49.069


1:13:50.190,1:13:56.449


1:13:57.300,1:14:01.640


1:14:01.640,1:14:06.499


1:14:06.989,1:14:08.930


1:14:08.930,1:14:13.489


1:14:13.770,1:14:19.100


1:14:19.320,1:14:24.020


1:14:24.660,1:14:26.660


1:14:27.179,1:14:31.579


1:14:31.580,1:14:35.059


1:14:35.460,1:14:42.139


1:14:42.720,1:14:44.720


1:14:45.090,1:14:49.639


1:14:49.860,1:14:56.059


1:15:00.450,1:15:05.420


1:15:05.420,1:15:07.110


1:15:07.110,1:15:09.110


1:15:09.270,1:15:12.439


1:15:12.750,1:15:18.739


1:15:18.810,1:15:21.410


1:15:22.110,1:15:29.179


1:15:29.760,1:15:33.170


1:15:33.180,1:15:39.139


1:15:39.140,1:15:40.770


1:15:40.770,1:15:42.799


1:15:43.440,1:15:48.109


1:15:51.450,1:15:53.750


1:15:54.690,1:15:59.540


1:15:59.580,1:16:05.690


1:16:06.270,1:16:08.270


1:16:09.030,1:16:14.179


1:16:14.850,1:16:17.780


1:16:18.330,1:16:20.630


1:16:21.000,1:16:26.899


1:16:26.900,1:16:28.900


1:16:29.190,1:16:30.140


1:16:30.140,1:16:35.450


1:16:35.610,1:16:41.089


1:16:41.640,1:16:43.640


1:16:44.220,1:16:50.490


1:16:50.890,1:16:53.070


1:16:53.680,1:16:56.880


1:16:57.610,1:16:58.630


1:16:58.630,1:17:02.429


1:17:02.430,1:17:08.849


1:17:09.370,1:17:11.370


1:17:11.770,1:17:13.800


1:17:14.530,1:17:16.530


1:17:17.290,1:17:22.560


1:17:24.100,1:17:29.789


1:17:30.820,1:17:34.559


1:17:35.140,1:17:40.499


1:17:41.650,1:17:45.540


1:17:45.540,1:17:47.790


1:17:48.130,1:17:53.459


1:17:53.560,1:17:58.919


1:17:59.440,1:18:02.070


1:18:02.620,1:18:04.240


1:18:04.240,1:18:10.349


1:18:11.560,1:18:16.110


1:18:16.110,1:18:18.959


1:18:19.390,1:18:25.410


1:18:25.410,1:18:30.479


1:18:31.030,1:18:34.469


1:18:35.170,1:18:39.120


1:18:39.120,1:18:42.059


1:18:42.580,1:18:46.260


1:18:46.260,1:18:52.919


1:18:53.290,1:18:59.819


1:19:00.070,1:19:02.489


1:19:04.359,1:19:06.359


1:19:06.820,1:19:10.019


1:19:10.119,1:19:17.279


1:19:18.219,1:19:20.219


1:19:20.800,1:19:24.689


1:19:25.269,1:19:30.239


1:19:30.760,1:19:34.079


1:19:35.260,1:19:42.780


1:19:43.030,1:19:48.599


1:19:50.349,1:19:52.349


1:19:52.539,1:20:00.148


1:20:00.789,1:20:07.349


1:20:07.389,1:20:13.618


1:20:14.409,1:20:18.239


1:20:19.269,1:20:25.799


1:20:25.800,1:20:28.109


1:20:28.659,1:20:30.929


1:20:31.479,1:20:35.039


1:20:35.829,1:20:41.069


1:20:42.699,1:20:47.728


1:20:49.149,1:20:51.149


1:20:52.510,1:20:57.959


1:20:58.300,1:21:03.449


1:21:04.030,1:21:05.499


1:21:05.499,1:21:08.909


1:21:09.130,1:21:13.089


1:21:13.670,1:21:15.879


1:21:16.430,1:21:20.440


1:21:22.460,1:21:26.679


1:21:27.050,1:21:31.779


1:21:32.030,1:21:39.190


1:21:39.710,1:21:47.679


1:21:48.590,1:21:56.259


1:21:56.510,1:22:00.790


1:22:01.880,1:22:05.889


1:22:06.140,1:22:09.430


1:22:11.090,1:22:15.040


1:22:15.830,1:22:17.480


1:22:17.480,1:22:18.680


1:22:18.680,1:22:20.480


1:22:20.480,1:22:24.580


1:22:26.640,1:22:34.279


1:22:35.310,1:22:37.310


1:22:41.940,1:22:43.500


1:22:43.500,1:22:50.629


1:22:52.350,1:22:55.309


1:22:57.600,1:22:59.600


1:23:00.330,1:23:02.330


1:23:02.429,1:23:08.479


1:23:08.790,1:23:13.159


1:23:13.350,1:23:18.709


1:23:19.710,1:23:21.710


1:23:22.320,1:23:25.219


1:23:25.890,1:23:28.939


1:23:29.580,1:23:36.739


1:23:37.290,1:23:41.269


1:23:41.520,1:23:42.020


1:23:42.020,1:23:48.649


1:23:48.800,1:23:50.840


1:23:52.260,1:23:55.699


1:23:56.040,1:24:02.359


1:24:02.360,1:24:04.940


1:24:04.940,1:24:10.609


1:24:10.890,1:24:16.519


1:24:17.219,1:24:20.149


1:24:20.670,1:24:24.859


1:24:25.290,1:24:29.779


1:24:30.090,1:24:35.659


1:24:35.659,1:24:38.999


1:24:39.880,1:24:47.130


1:24:47.679,1:24:51.509


1:24:52.060,1:24:53.580


1:24:53.580,1:24:58.830


1:25:00.010,1:25:02.040


1:25:04.389,1:25:08.909


1:25:08.909,1:25:14.459


1:25:15.760,1:25:17.760


1:25:18.639,1:25:23.789


1:25:25.060,1:25:27.449


1:25:27.969,1:25:33.928


1:25:34.239,1:25:41.098


1:25:41.920,1:25:45.899


1:25:45.900,1:25:48.480


1:25:48.480,1:25:52.589


1:25:53.139,1:25:56.909


1:25:58.060,1:26:03.029


1:26:03.909,1:26:07.319


1:26:07.960,1:26:10.889


1:26:10.889,1:26:16.379


1:26:16.960,1:26:23.520


1:26:23.520,1:26:25.520


1:26:27.940,1:26:34.799


1:26:39.179,1:26:41.689


1:26:44.909,1:26:52.129


1:26:54.030,1:26:55.679


1:26:55.679,1:26:57.449


1:26:57.449,1:27:00.949


1:27:00.949,1:27:07.699


1:27:08.909,1:27:12.919


1:27:12.920,1:27:17.599


1:27:18.630,1:27:24.799


1:27:24.800,1:27:26.800


1:27:27.449,1:27:33.049


1:27:34.530,1:27:41.300


1:27:41.519,1:27:43.519


1:27:44.010,1:27:46.489


1:27:47.369,1:27:50.328


1:27:51.119,1:27:53.119


1:27:53.940,1:27:55.940


1:27:56.489,1:28:01.458


1:28:03.059,1:28:06.228


1:28:07.709,1:28:09.739


1:28:11.639,1:28:15.018


1:28:15.019,1:28:19.639


1:28:19.639,1:28:21.209


1:28:21.209,1:28:22.349


1:28:22.349,1:28:24.469


1:28:27.389,1:28:29.179


1:28:29.179,1:28:32.149


1:28:32.280,1:28:37.489


1:28:38.130,1:28:39.440


1:28:39.440,1:28:44.929


1:28:44.929,1:28:48.408


1:28:48.850,1:28:54.720


1:28:55.420,1:29:01.500


1:29:02.320,1:29:04.710


1:29:04.990,1:29:11.969


1:29:11.970,1:29:13.090


1:29:13.090,1:29:16.410


1:29:17.050,1:29:20.490


1:29:21.700,1:29:23.200


1:29:23.200,1:29:29.189


1:29:30.280,1:29:34.319


1:29:35.020,1:29:40.080


1:29:40.270,1:29:43.379


1:29:44.170,1:29:47.670


1:29:48.040,1:29:52.950


1:29:53.770,1:29:58.560


1:29:58.810,1:30:03.780


1:30:04.570,1:30:10.680


1:30:11.350,1:30:17.939


1:30:17.940,1:30:19.940


1:30:21.190,1:30:22.440


1:30:22.440,1:30:29.250


1:30:30.040,1:30:34.410


1:30:34.410,1:30:38.939


1:30:38.940,1:30:43.020


1:30:44.500,1:30:49.740


1:30:49.810,1:30:52.709


1:30:53.320,1:30:54.610


1:30:54.610,1:30:56.440


1:30:56.440,1:31:00.450


1:31:00.850,1:31:03.579


1:31:04.580,1:31:06.580


1:31:06.830,1:31:10.000


1:31:10.000,1:31:12.939


1:31:12.940,1:31:18.190


1:31:18.770,1:31:25.479


1:31:25.670,1:31:30.069


1:31:30.350,1:31:36.340


1:31:37.190,1:31:44.829


1:31:44.830,1:31:52.030


1:31:53.120,1:31:58.750


1:32:00.530,1:32:02.530


1:32:02.630,1:32:04.630


1:32:05.150,1:32:08.199


1:32:09.170,1:32:11.170


1:32:11.480,1:32:16.719


1:32:17.660,1:32:19.660


1:32:20.000,1:32:22.929


1:32:22.930,1:32:27.159


1:32:27.740,1:32:35.650


1:32:36.170,1:32:37.220


1:32:37.220,1:32:41.949


1:32:41.950,1:32:49.780


1:32:52.160,1:32:58.720


1:32:59.720,1:33:07.449


1:33:08.120,1:33:13.270


1:33:13.270,1:33:17.589


1:33:18.289,1:33:19.939


1:33:19.939,1:33:21.939


1:33:22.610,1:33:28.269


1:33:28.579,1:33:31.629


1:33:32.840,1:33:36.249


1:33:36.249,1:33:39.909


1:33:39.909,1:33:45.398


1:33:45.399,1:33:50.589


1:33:50.899,1:33:52.929


1:33:52.929,1:33:59.379


1:33:59.840,1:34:02.229


1:34:03.260,1:34:08.949


1:34:08.949,1:34:10.789


1:34:10.789,1:34:12.499


1:34:12.499,1:34:19.089


1:34:19.090,1:34:26.229


1:34:26.630,1:34:28.630


1:34:29.209,1:34:32.349


1:34:32.349,1:34:37.478


1:34:37.479,1:34:39.479


1:34:43.610,1:34:47.349


1:34:48.199,1:34:50.199


1:34:50.869,1:34:55.629


1:34:56.510,1:35:01.329


1:35:01.329,1:35:05.708


1:35:06.709,1:35:10.629


1:35:12.199,1:35:15.339


1:35:17.209,1:35:22.839


1:35:22.840,1:35:25.929


1:35:26.599,1:35:28.599


1:35:29.869,1:35:34.089


1:35:34.090,1:35:37.090


1:35:37.809,1:35:39.809


1:35:42.219,1:35:44.669


1:35:44.670,1:35:50.850


1:35:50.850,1:35:57.120


1:35:57.310,1:36:00.330


1:36:00.429,1:36:02.380


1:36:02.380,1:36:07.980


1:36:07.980,1:36:13.589


1:36:13.870,1:36:20.519


1:36:20.770,1:36:25.199


1:36:25.199,1:36:29.279


1:36:29.800,1:36:36.480


1:36:37.060,1:36:41.789


1:36:42.699,1:36:46.499


1:36:47.050,1:36:51.029


1:36:51.030,1:36:54.659


1:36:55.179,1:36:58.859


1:37:00.179,1:37:01.330


1:37:01.330,1:37:03.040


1:37:03.040,1:37:05.129


1:37:05.130,1:37:10.199


1:37:10.270,1:37:15.270


1:37:15.270,1:37:21.749


1:37:22.690,1:37:27.899


1:37:28.480,1:37:33.779


1:37:34.570,1:37:36.570


1:37:36.610,1:37:41.190


1:37:41.949,1:37:43.949


1:37:45.280,1:37:47.610


1:37:48.610,1:37:55.880


1:37:57.030,1:38:04.280


1:38:06.929,1:38:11.539


1:38:11.540,1:38:14.959


1:38:16.170,1:38:21.500


1:38:22.800,1:38:25.670


1:38:25.670,1:38:30.230


1:38:30.230,1:38:36.109


1:38:36.110,1:38:38.110


1:38:39.660,1:38:41.660


1:38:46.350,1:38:51.949


1:38:53.780,1:38:58.820


1:38:59.640,1:39:06.559


1:39:12.600,1:39:19.249


1:39:19.250,1:39:21.290


1:39:21.290,1:39:27.649


1:39:28.170,1:39:30.170


1:39:33.510,1:39:40.880


1:39:42.870,1:39:45.559


1:39:46.560,1:39:50.600


1:39:51.150,1:39:57.080


1:39:59.910,1:40:02.719


1:40:04.080,1:40:06.709


1:40:08.559,1:40:13.319


1:40:13.420,1:40:18.479


1:40:19.300,1:40:24.449


1:40:25.989,1:40:29.518


1:40:30.429,1:40:35.219


1:40:35.559,1:40:43.109


1:40:43.780,1:40:45.510


1:40:45.510,1:40:46.840


1:40:46.840,1:40:52.409


1:40:52.989,1:40:57.328


1:40:58.119,1:41:03.509


1:41:03.820,1:41:08.729


1:41:08.729,1:41:15.479


1:41:16.630,1:41:22.679


1:41:23.590,1:41:26.519


1:41:29.499,1:41:34.289


1:41:34.289,1:41:39.958


1:41:40.539,1:41:43.859


1:41:45.010,1:41:47.010


1:41:48.429,1:41:54.388


1:41:55.090,1:41:59.699


1:42:01.949,1:42:04.158


1:42:05.130,1:42:10.069


1:42:11.579,1:42:13.289


1:42:13.289,1:42:15.589


1:42:16.409,1:42:20.538


1:42:23.099,1:42:27.558


1:42:27.559,1:42:31.819


1:42:39.659,1:42:44.538


1:42:44.729,1:42:50.239


1:42:50.729,1:42:53.689


1:42:55.530,1:43:01.639


1:43:02.550,1:43:03.809


1:43:03.809,1:43:10.819


1:43:12.780,1:43:15.739


1:43:16.289,1:43:21.049


1:43:21.719,1:43:24.408


1:43:25.349,1:43:27.529


1:43:28.199,1:43:30.059


1:43:30.059,1:43:32.268


1:43:33.719,1:43:39.138


1:43:39.139,1:43:41.839


1:43:41.840,1:43:46.969


1:43:46.969,1:43:51.498


1:43:52.559,1:43:58.248


1:43:58.249,1:44:03.469


1:44:03.900,1:44:09.769


1:44:10.709,1:44:12.709


1:44:14.439,1:44:16.559


1:44:18.639,1:44:22.679


1:44:22.679,1:44:28.199


1:44:28.659,1:44:31.498


1:44:31.659,1:44:37.409


1:44:37.409,1:44:38.429


1:44:38.429,1:44:39.159


1:44:39.159,1:44:45.149


1:44:48.090,1:44:49.929


1:44:49.929,1:44:55.289


1:44:55.290,1:45:02.219


1:45:06.010,1:45:10.170


1:45:10.840,1:45:13.980


1:45:14.800,1:45:20.639


1:45:22.090,1:45:24.210


1:45:24.210,1:45:30.179


1:45:30.179,1:45:33.839


1:45:33.969,1:45:40.019


1:45:40.210,1:45:45.419


1:45:45.820,1:45:50.340


1:45:50.949,1:45:55.499


1:45:55.570,1:45:59.520


1:45:59.590,1:46:02.849


1:46:03.489,1:46:11.039


1:46:11.920,1:46:18.989


1:46:19.869,1:46:22.859


1:46:33.820,1:46:35.820


1:46:38.920,1:46:43.109


1:46:44.619,1:46:51.299


1:46:51.940,1:46:56.639


1:46:57.880,1:47:02.190


1:47:03.310,1:47:05.310


1:47:05.560,1:47:08.160


1:47:10.880,1:47:13.040


1:47:17.130,1:47:20.839


1:47:20.840,1:47:25.429


1:47:26.310,1:47:31.850


1:47:33.510,1:47:36.679


1:47:37.260,1:47:40.610


1:47:41.010,1:47:45.920


1:47:46.020,1:47:50.779


1:47:50.780,1:47:56.480


1:47:57.449,1:47:59.449


1:48:00.179,1:48:02.658


1:48:05.969,1:48:07.940


1:48:07.940,1:48:11.839


1:48:13.670,1:48:15.670


1:48:16.760,1:48:22.900


1:48:24.860,1:48:28.420


1:48:29.840,1:48:31.960


1:48:31.960,1:48:37.870


1:48:38.000,1:48:41.830


1:48:42.170,1:48:46.330


1:48:46.880,1:48:52.390


1:48:53.300,1:48:55.300


1:48:57.110,1:49:03.699


1:49:04.520,1:49:06.230


1:49:06.230,1:49:13.060


1:49:14.750,1:49:18.879


1:49:19.550,1:49:26.350


1:49:26.350,1:49:30.309


1:49:31.610,1:49:36.489


1:49:36.890,1:49:41.289


1:49:41.930,1:49:43.930


1:49:44.090,1:49:49.299


1:49:49.670,1:49:54.699


1:49:54.699,1:49:58.779


1:49:59.420,1:50:05.710


1:50:06.590,1:50:11.080


1:50:11.840,1:50:14.710


1:50:14.710,1:50:19.120


1:50:19.120,1:50:24.219


1:50:24.219,1:50:28.239


1:50:28.790,1:50:31.149


1:50:31.429,1:50:36.399


1:50:36.400,1:50:38.400


1:50:38.570,1:50:42.009


1:50:42.770,1:50:49.029


1:50:49.030,1:50:54.219


1:50:54.219,1:50:58.479


1:50:58.480,1:51:00.320


1:51:00.320,1:51:07.690


1:51:08.119,1:51:09.440


1:51:09.440,1:51:11.329


1:51:11.329,1:51:13.779


1:51:13.780,1:51:20.559


1:51:20.659,1:51:22.309


1:51:22.309,1:51:28.779


1:51:28.780,1:51:32.050


1:51:34.099,1:51:38.049


1:51:39.440,1:51:42.129


1:51:42.829,1:51:45.729


1:51:46.520,1:51:47.869


1:51:47.869,1:51:55.719


1:51:58.280,1:52:04.690


1:52:05.750,1:52:08.949


1:52:09.980,1:52:11.179


1:52:11.179,1:52:13.209


1:52:13.969,1:52:19.328


1:52:21.020,1:52:22.909


1:52:22.909,1:52:27.429


1:52:27.429,1:52:31.359


1:52:31.880,1:52:37.719


1:52:39.860,1:52:44.770


1:52:46.040,1:52:52.390


1:52:54.470,1:52:56.470


1:52:58.640,1:53:00.640


1:53:01.100,1:53:03.100


1:53:03.140,1:53:05.140


1:53:05.450,1:53:10.599


1:53:10.600,1:53:14.140


1:53:16.190,1:53:21.760


1:53:24.980,1:53:26.980


1:53:27.440,1:53:31.690


1:53:31.730,1:53:36.160


1:53:36.500,1:53:39.850


1:53:40.850,1:53:45.399


1:53:46.010,1:53:52.780


1:53:53.840,1:53:59.559


1:53:59.690,1:54:01.690


1:54:02.390,1:54:04.629


1:54:04.630,1:54:09.460


1:54:10.490,1:54:12.999


1:54:13.760,1:54:16.419


1:54:16.420,1:54:23.710


1:54:23.710,1:54:29.530


1:54:30.140,1:54:31.700


1:54:31.700,1:54:32.720


1:54:32.720,1:54:34.300


1:54:34.300,1:54:37.300


1:54:37.340,1:54:39.909


1:54:40.400,1:54:41.960


1:54:41.960,1:54:44.859


1:54:44.950,1:54:49.090


1:54:50.660,1:54:57.609


1:54:59.000,1:55:01.000


1:55:01.670,1:55:08.770


1:55:09.800,1:55:17.350


1:55:18.020,1:55:23.919


1:55:24.140,1:55:29.409


1:55:30.020,1:55:35.859


1:55:35.860,1:55:42.100


1:55:42.800,1:55:44.800


1:55:45.830,1:55:47.300


1:55:47.300,1:55:52.029


1:55:52.100,1:55:59.379


1:55:59.380,1:56:03.219


1:56:03.860,1:56:09.250


1:56:09.250,1:56:13.750


1:56:14.510,1:56:17.199


1:56:17.199,1:56:22.928


1:56:25.460,1:56:26.810


1:56:26.810,1:56:33.370


1:56:34.159,1:56:36.159


1:56:36.469,1:56:43.719


1:56:43.719,1:56:49.448


1:56:49.449,1:56:56.979


1:56:56.980,1:56:58.489


1:56:58.489,1:57:00.489


1:57:00.679,1:57:02.679


1:57:02.840,1:57:08.620


1:57:08.989,1:57:11.859


1:57:12.560,1:57:14.560


1:57:14.720,1:57:19.059


1:57:19.970,1:57:22.659


1:57:23.450,1:57:28.539


1:57:29.660,1:57:31.839


1:57:32.870,1:57:38.260


1:57:38.450,1:57:42.789


1:57:44.870,1:57:49.300


1:57:49.940,1:57:51.940


1:57:52.100,1:57:57.249


1:57:57.830,1:58:03.010


1:58:03.740,1:58:10.269


1:58:11.090,1:58:13.090


1:58:13.550,1:58:16.059


1:58:16.820,1:58:18.820


1:58:19.160,1:58:21.160


1:58:21.200,1:58:27.729


1:58:29.450,1:58:31.420


1:58:31.420,1:58:36.220


1:58:36.290,1:58:40.330


1:58:41.000,1:58:43.000


1:58:43.340,1:58:50.440


1:58:50.990,1:58:53.109


1:58:53.180,1:58:59.349


1:58:59.350,1:59:02.289


1:59:02.290,1:59:05.620


1:59:06.170,1:59:08.530


1:59:09.140,1:59:11.800


1:59:13.190,1:59:16.000


1:59:16.640,1:59:22.660


1:59:23.190,1:59:25.460


1:59:26.010,1:59:31.250


1:59:32.099,1:59:38.119


1:59:38.429,1:59:42.379


1:59:43.080,1:59:48.679


1:59:48.840,1:59:54.500


1:59:55.139,2:00:00.919


2:00:01.440,2:00:03.679


2:00:04.590,2:00:11.630


2:00:12.690,2:00:17.149


2:00:18.030,2:00:22.580


2:00:23.520,2:00:24.570


2:00:24.570,2:00:26.900


2:00:27.510,2:00:33.260


2:00:33.530,2:00:36.349


2:00:37.170,2:00:39.170


2:00:40.199,2:00:42.199


2:00:43.530,2:00:50.239


2:00:50.310,2:00:52.310


2:00:52.710,2:00:57.409


2:00:59.040,2:01:00.869


2:01:00.869,2:01:02.869


2:01:03.780,2:01:07.070


2:01:08.730,2:01:10.730


2:01:11.010,2:01:16.010


2:01:16.010,2:01:18.010


2:01:18.540,2:01:24.560


2:01:24.599,2:01:29.839


2:01:30.610,2:01:34.239


2:01:34.940,2:01:36.940


2:01:37.010,2:01:38.750


2:01:38.750,2:01:43.659


2:01:45.500,2:01:47.500


2:01:48.650,2:01:56.409


2:01:57.800,2:02:02.560


2:02:05.480,2:02:07.689


2:02:08.960,2:02:13.659


2:02:13.660,2:02:19.180


2:02:19.340,2:02:22.060


2:02:23.780,2:02:27.639


2:02:31.010,2:02:34.119


2:02:36.020,2:02:37.430


2:02:37.430,2:02:39.230


2:02:39.230,2:02:41.230


2:02:41.510,2:02:44.739


2:02:45.410,2:02:47.410


2:02:49.490,2:02:53.920


2:02:54.890,2:02:58.900


2:03:00.020,2:03:01.610


2:03:01.610,2:03:04.389


2:03:05.180,2:03:07.180


2:03:08.520,2:03:10.520


2:03:10.990,2:03:14.879


2:03:16.000,2:03:18.299


2:03:18.970,2:03:21.119


2:03:21.190,2:03:26.609


2:03:26.710,2:03:29.640


2:03:30.460,2:03:32.460


2:03:33.190,2:03:38.790


2:03:38.790,2:03:45.990


2:03:46.180,2:03:48.180


2:03:48.430,2:03:52.439


2:03:52.750,2:03:55.109


2:03:55.720,2:04:02.970


2:04:02.970,2:04:04.970


2:04:05.680,2:04:10.019


2:04:11.410,2:04:16.260


2:04:18.190,2:04:20.879


2:04:22.510,2:04:24.510


2:04:25.270,2:04:28.950


2:04:28.950,2:04:33.839


2:04:33.840,2:04:34.600


2:04:34.600,2:04:39.419


2:04:39.820,2:04:45.600


2:04:47.080,2:04:50.039


2:04:50.040,2:04:55.379


2:04:56.680,2:04:58.680


2:04:58.990,2:05:03.059


2:05:04.060,2:05:05.610


2:05:05.610,2:05:11.190


2:05:11.440,2:05:17.069


2:05:17.260,2:05:22.990


2:05:25.970,2:05:30.820


2:05:31.460,2:05:37.390


2:05:37.540,2:05:39.540


2:05:39.950,2:05:45.249


2:05:45.440,2:05:49.150


2:05:49.150,2:05:54.400


2:05:55.100,2:05:58.990


2:05:59.180,2:06:01.930


2:06:02.570,2:06:04.570


2:06:05.600,2:06:07.600


2:06:11.330,2:06:18.400


2:06:19.100,2:06:25.059


2:06:25.870,2:06:31.570


2:06:32.630,2:06:38.199


2:06:38.900,2:06:40.100


2:06:40.100,2:06:42.100


2:06:42.530,2:06:44.530


2:06:44.960,2:06:46.460


2:06:46.460,2:06:47.870


2:06:47.870,2:06:49.870


2:06:51.110,2:06:53.650


2:06:53.810,2:06:59.110


2:07:00.560,2:07:01.670


2:07:01.670,2:07:08.170


2:07:08.170,2:07:10.170


2:07:11.749,2:07:14.858


2:07:16.159,2:07:18.159


2:07:18.800,2:07:22.869


2:07:23.629,2:07:27.309

