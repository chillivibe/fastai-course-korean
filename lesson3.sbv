0:00:00.000,0:00:02.360
돌아오신걸 환영합니다

0:00:04.160,0:00:07.220
눈치채셨을지 모르지만
그동안 커뮤니티 포럼에서

0:00:07.220,0:00:10.500
학생들의 멋진 활동들이 있었는데요

0:00:10.500,0:00:11.380
한가지 소개해 드리자면

0:00:12.285,0:00:15.000
이 코스의 다른 학우를 위해서

0:00:15.005,0:00:18.215
도움이 되는 많은 자료들을

0:00:18.215,0:00:21.415
만들어서 이해를 도와 줬더군요

0:00:21.415,0:00:24.325
누군가를 가르치면서

0:00:24.325,0:00:27.115
스스로도 더 잘 이해하는 기회도

0:00:27.115,0:00:28.475
얻은것 같습니다

0:00:28.785,0:00:30.975
제가 Wiki 에 그 중 몇가지를

0:00:31.320,0:00:34.400
리스트업 해서 포스팅 해뒀습니다

0:00:34.860,0:00:35.860


0:00:36.300,0:00:39.100
reshamas라는 분이
입문자들을 위한

0:00:39.100,0:00:42.105
많은 튜토리얼을 작성하셨는데요

0:00:42.105,0:00:44.965
그 내용중 하나는
AWS 접속에 문제를 겪는 분들을 위해서

0:00:44.965,0:00:48.045
전체적으로 단계별로

0:00:48.045,0:00:50.540
어떻게 로그인하고

0:00:50.540,0:00:53.900
모든걸 정상동작 하게 만드는지 보여줍니다

0:00:53.940,0:00:56.240


0:00:57.340,0:01:00.260
스스로 상기하는 차원에서,
노트를 남기시길 원하시면

0:01:00.340,0:01:03.135
블로그 포스트를 작성하시거나

0:01:03.135,0:01:06.195
이분처럼 마크다운에 작성하셔도 좋습니다

0:01:06.195,0:01:09.285
전에 깃헙을 사용해보신 적이 없으면

0:01:09.285,0:01:12.185
좋은 훈련이 되기도 할 겁니다
깃헙에 올려두시면

0:01:12.185,0:01:15.080
다른 사람들도 볼 수 있습니다.
물론, 커뮤니티 포럼에 올려주셔도 됩니다

0:01:15.200,0:01:20.975
reshamas가 작성한

0:01:20.975,0:01:23.885
좀 더 심화적인 내용으로는
tmux 라는게 있습니다.

0:01:23.885,0:01:26.995
편리한 도구인데요

0:01:27.000,0:01:31.520
보여드릴게요

0:01:32.160,0:01:35.240
AWS 컴퓨터에 로그인 한 직후에

0:01:35.300,0:01:38.195
tmux a 명령을 실행하면

0:01:38.195,0:01:41.245
모든 윈도우 창이 모두 나타납니다

0:01:41.245,0:01:44.145
백그라운드에서 작업중인것을
포함해서

0:01:44.145,0:01:47.355
VIM 으로 편집중인 내용등을
보여주는데

0:01:47.360,0:01:52.460
원하는 창으로 들어갔다가
다른 창으로 이동도 가능합니다

0:01:53.200,0:01:55.720
이 내용에 관심이 있으시면

0:01:55.940,0:01:58.940
이 깃헙에 사용법에 대한 
튜토리얼이 있습니다

0:01:58.945,0:02:02.145
그리고 여기 깃헙에는
다른 많은 내용들도 있습니다.

0:02:02.145,0:02:05.305
멋지게 정리해 줬습니다

0:02:05.305,0:02:08.275
Apil Tamang이라는 분은

0:02:08.275,0:02:11.195
지난주에 대한 아주 멋진 요약을

0:02:11.195,0:02:12.365
적어주셨습니다

0:02:12.705,0:02:15.895
이 요약이 다루는건

0:02:16.560,0:02:20.720
우리가 한 중요한 일들과
왜 그것들을 해야 하는지에 대한 것입니다

0:02:21.200,0:02:24.340
지난주에 배운 내용들이 어떻게
서로 들어맞는지 궁금하시면

0:02:24.340,0:02:26.920
이 내용은 꽤나 도움이 많이 될 것 같습니다

0:02:27.200,0:02:30.145
2시간정도 우리가 한 내용을

0:02:30.145,0:02:32.625
1-2 페이지에 정리 해뒀거든요

0:02:32.980,0:02:37.560
또, Pavel Surmenok의 글도 정말 좋은데요

0:02:38.800,0:02:42.920
학습률 발견자에 대해서
깊이 있게 다룹니다

0:02:44.020,0:02:46.960
많은 분들이 더 많은 것을 알고 싶어하신
주제 인데요

0:02:46.960,0:02:48.460
특히나

0:02:48.720,0:02:51.725
딥러닝을 전에 공부해보신 분이라면

0:02:51.725,0:02:54.735
오랫동안 고통받던 문제에 대한
해결책이 될 수 있는데

0:02:54.740,0:02:58.540
아직까지 본 적이 없던 방법이기 때문입니다

0:02:59.240,0:03:03.695
이 글은 그 내용에 대해서 적은 것인데,
제가 이 글의 링크를

0:03:03.695,0:03:06.695
트위터에 올렸을때

0:03:06.695,0:03:09.815
엄청 많은 사람들이
공유를 했었습니다.

0:03:09.815,0:03:12.745
아주 인기 있었고, 수천번 정도
공유 됐던거 같습니다

0:03:12.745,0:03:13.705
Redek은

0:03:13.705,0:03:16.715
많은 멋진 내용을 포스팅 했는데요

0:03:16.715,0:03:19.615
그 중 PyTorch에 대한 현업 종사자의 가이드라는
글이 마음에 들더군요

0:03:19.615,0:03:22.345
이 글의 내용은 약간
심화 학생들을 위한건데

0:03:22.345,0:03:25.325
PyTorch를 전에 사용해보지
못했지만

0:03:25.325,0:03:28.780
기본적으로 숫자를 다루는
프로그래밍을 해보신 분들에게

0:03:29.040,0:03:31.600
파고들어 가보는데
도움을 주기도 합니다

0:03:31.600,0:03:34.980
어떻게 PyTorch가 다른지
빠르게 훑어볼 수도 있습니다

0:03:35.440,0:03:38.140
약간 흥미로운 연구 내용도 있는데요

0:03:38.145,0:03:41.195
학습률과 배치 크기사이의 관계에 
대한 것입니다

0:03:41.195,0:03:43.995
한 학생이 지난주에 물어본 내용이기도 하죠

0:03:44.000,0:03:48.580
정확히 그 내용에 대해서,
다른 학생이 분석한 글을 작성 했습니다

0:03:49.320,0:03:51.915
이 학생이 한 것은

0:03:51.915,0:03:54.595
서로다른 학습률과 배치크기를 시도해보고

0:03:54.595,0:03:57.195
각 결과를 비교해 본 것입니다

0:03:57.405,0:04:00.195
멋진 실험으로, 다른분들도
직접 해보시길 바랍니다

0:04:00.655,0:04:03.525
Redek이 또 다른 글을 적었는데요

0:04:03.605,0:04:06.595
일종의 연구적인 내용으로

0:04:06.595,0:04:09.655
어떻게 "재시작하는 확률적 경사하강법"이

0:04:09.735,0:04:12.535
함수에서 좀 더 일반화 가능한 지점을

0:04:12.535,0:04:15.565
발견할 수 있는지에  대한 것입니다

0:04:15.565,0:04:18.145
Redek은 그 지점을

0:04:18.145,0:04:21.175
좀더 직접적으로 발견하기 위한
측정법을 알아보고자 시도해봤습니다

0:04:21.175,0:04:23.935
그렇게 성공적이진 않지만, 꽤나
흥미로웠습니다

0:04:23.935,0:04:26.965
여기 다른 블로그 글은

0:04:26.965,0:04:30.375
초보자를 위한 CNN에 대한 내용도
있었습니다.

0:04:31.035,0:04:32.035


0:04:32.935,0:04:36.095
이 코스의 마지막 즈음 배우게 될 내용인데

0:04:36.095,0:04:39.365
아시다시피 우리가 ResNet을
사용 했었습니다.

0:04:39.375,0:04:42.165
Anand Saha라는 분이
아주 인상적으로

0:04:42.215,0:04:44.945
ResNet이 무엇이고

0:04:44.945,0:04:47.855
어떤 내용이 흥미로운지에 대하여

0:04:47.860,0:04:51.720
분석하는 글을 작성했습니다
이미 인터넷에서 많이 공유되는 글입니다

0:04:52.140,0:04:55.460
먼저 좀더 심화 내용을 확인해 보고 싶으신 분은

0:04:55.675,0:04:58.325
읽어보시기 바랍니다.
Apil Tamang도 비슷한

0:04:58.325,0:05:01.315
내용의 글을 작성했습니다

0:05:01.645,0:05:03.295
많은 자료들이

0:05:03.685,0:05:06.725
커뮤니티 포럼에 있는걸 보셨고

0:05:06.725,0:05:09.665
초보자 포럼(beginner forum)도 있습니다

0:05:09.665,0:05:12.600
그리고

0:05:13.340,0:05:20.620
멍청한 질문이라는건 없지만,
많은 주변사람들이 심화적인 내용을

0:05:20.860,0:05:23.565
이야기하는데 본인은 그렇지 않다면

0:05:23.565,0:05:26.565
초보자 포럼이 이를 도와줄 겁니다

0:05:26.565,0:05:29.445
그리고 본인 스스로가

0:05:29.475,0:05:32.535
심화 학생이라고 생각하고,
질문에 대한 대답을

0:05:32.535,0:05:35.445
도와줄 수 있다면 그렇게 해 주세요
단지 질문에 대답할때

0:05:35.445,0:05:38.215
사람들에게 친근한 방식으로

0:05:38.215,0:05:41.185
대답해주려고 노력해 주세요

0:05:41.185,0:05:45.420
1년 미만의 프로그래밍 경험과,
머신러닝을 해본적이 없다는 가정을 하고 말이죠

0:05:46.280,0:05:49.700
저는 이 클래스의

0:05:50.100,0:05:51.580
다른 학생들이

0:05:51.995,0:05:54.825
뭔가 기여할 수 있다고 느끼길 희망합니다.

0:05:54.825,0:05:57.080
단지 많은 사람들이

0:05:57.660,0:06:00.495
제 생각엔 전에 인터넷에 뭔가를
포스팅 해본적이 없는것 같은데

0:06:00.495,0:06:03.345
뭔가를 블로그 포스팅하기 위해서

0:06:03.345,0:06:06.205
특별한 사람일 필요는 없다는걸
기억해 주세요

0:06:06.205,0:06:09.055
생각나는걸 적고

0:06:09.275,0:06:12.275
인터넷에 올리세요

0:06:12.275,0:06:15.145
한가지 손쉬운 것은
포럼에 글을 올렸는데

0:06:15.145,0:06:18.125
상세한 부분까지 확신이 없다면

0:06:18.125,0:06:21.025
피드백을 받을 수 있는 기회가 있습니다.

0:06:21.025,0:06:23.795
"그건 그렇게 동작하는게 아니야,
대신 이렇게 동작하는거에요" 라던지

0:06:23.800,0:06:26.520
"이렇게 까지 생각한 부분이

0:06:26.520,0:06:29.060
아주 흥미로운 통찰력 이군요" 같은것 말이죠

0:06:29.620,0:06:32.635
지금까지 우리가 한 내용은

0:06:32.640,0:06:36.160
실무 종사자로서 받은 일종의
입문자용의 내용으로

0:06:36.400,0:06:41.040
이미지에 대한 컨볼루션 뉴럴넷을 다뤘습니다

0:06:41.060,0:06:43.860
이론적인 내용이나
왜 그렇게 동작하는지나

0:06:43.960,0:06:46.895
수학적인 내용에 대해선 아직
이야기 되지 않았습니다

0:06:46.895,0:06:49.935
하지만, 반면에

0:06:49.935,0:06:52.975
아주 잘 동작하는

0:06:52.975,0:06:56.165
모델을 만드는 방법을 배웠습니다

0:06:56.165,0:06:59.255
세계적 수준의 모델이죠

0:06:59.260,0:07:04.300
오늘 그 부분을 약간 리뷰하도록 하겠습니다

0:07:05.160,0:07:07.635
그리고 오늘은

0:07:07.635,0:07:10.565
CNN이 뭐고, 컨볼루션이 무엇이며

0:07:10.565,0:07:13.925
어떻게 동작하는지에 대한 이론적인 내용을
깊이있게 공부할 것입니다

0:07:14.080,0:07:17.100
그리고 나선, 여기 보이시는 원형의
각 단계에 대해서

0:07:17.160,0:07:20.075
딥러닝이 적용 가능한
어플리케이션 분야의 예를

0:07:20.075,0:07:23.175
소개할 것입니다

0:07:23.175,0:07:26.195
뉴럴넷 구조화된 데이터에
사용해서

0:07:26.195,0:07:29.175
로지스틱 이나, 금융 데이터와 같은 것에 대해서

0:07:29.175,0:07:32.305
뭔가를 예측하는 예를 소개할 것입니다.

0:07:32.305,0:07:35.365
그다음으로는 언어에 응용 되는,

0:07:35.365,0:07:38.685
보통 NLP라고 부르는 것으로
순환 구조의 뉴럴넷 을 사용하게 됩니다

0:07:39.055,0:07:41.945
네번째는 협업 필터링에 대한 것으로

0:07:42.545,0:07:45.575
추천 시스템 같은 것을 위한 것입니다

0:07:45.575,0:07:48.395
전에 우리가 한 이미지에 대한

0:07:48.395,0:07:51.405
CNN과 비슷한 것입니다.

0:07:51.405,0:07:54.365
이론적 디테일에 대한 설명 없이

0:07:54.365,0:07:57.395
우선 최신예 모델을 소개해 드리고

0:07:57.395,0:08:00.305
어떻게 모델을 만들어서, 동작시킬지를
알게 해드리는 것이죠

0:08:00.305,0:08:03.195
그리고 나서, 다시 역순으로 공부하게 됩니다

0:08:03.195,0:08:06.125
협업 필터링에 대해서

0:08:06.125,0:08:09.105
꽤 자세하게 파고들고,
내부적인 코드를 어떻게 작성하고

0:08:09.105,0:08:12.085
내부적으로 수학이 어떻게 적용되는지
공부하게 될 것입니다

0:08:12.085,0:08:15.095
그리고 동일한 과정을
구조화된 데이터 분석에도 진행합니다

0:08:15.095,0:08:17.985
그 다음으로는 CNN 이미지에 대해서
그렇게 하고

0:08:17.985,0:08:21.095
마지막으로는 순환 뉴럴넷(RNN)에 대해서
자세히 들여다 볼 것입니다

0:08:21.095,0:08:22.165


0:08:23.540,0:08:26.160
일단, 약간의 리뷰를 하면서

0:08:26.280,0:08:28.260
시작해 봅시다

0:08:28.580,0:08:30.000


0:08:30.695,0:08:33.655
그리고, 전에 건너뛰었던 부분에 대해서

0:08:33.655,0:08:36.655
약간 더 상세하게 이야기 해 보겠습니다

0:08:36.660,0:08:39.420
저는 모든 학생분들이

0:08:39.560,0:08:42.780
지난주의 개 품종에 관련된
숙제를 완료했다는걸

0:08:43.280,0:08:46.140
확인해 두고 싶습니다

0:08:46.140,0:08:49.840
기본적으로 배운 내용을
다른 종류의 데이터셋에 적용하는 것으로

0:08:49.840,0:08:52.120
가장 쉬운 예가
개 품종에 관련된 문제 입니다

0:08:52.140,0:08:55.240
모든 분들이 갖춰야할 모든 내용은
이 문제를 해결해 보는것으로 커버 됩니다

0:08:55.485,0:08:58.515
우선 첫번째로
어떻게 데이터셋을

0:08:58.515,0:09:01.305
다운로드 할지를 아셔야 합니다

0:09:01.305,0:09:03.955
데이터를 다운로드 할 수 있는
두 가지 장소가 있는데,

0:09:03.955,0:09:07.045
첫번째는 Kaggle이고

0:09:07.045,0:09:09.995
나머지 하나는 그 외의 장소입니다

0:09:09.995,0:09:12.985
그래서 우선

0:09:12.985,0:09:16.095
Kaggle에서 다운받아 보겠습니다

0:09:16.095,0:09:19.245
다운받기 위해서
Kaggle CLI 라는 것을

0:09:19.245,0:09:21.160
사용했는데요

0:09:22.360,0:09:25.235


0:09:25.240,0:09:27.900
일단 설치하기 전에

0:09:27.900,0:09:30.420
제 생각에는 이미 설치되어 있을 것 같습니다

0:09:31.700,0:09:32.700


0:09:33.595,0:09:36.455
네 맞아요, 사용중인 환경에 이미

0:09:36.525,0:09:39.215
설치가 되어 있습니다

0:09:39.215,0:09:42.215
이 명령어가 동작하는 방식은

0:09:42.215,0:09:45.175
Kaggle 웹 사이트에서 스크랩해서
다운로드를 진행하는데

0:09:45.175,0:09:48.315
Kaggle이 웹 사이트 구조를 바꾸면
정상 동작하지 않게 됩니다

0:09:48.315,0:09:51.335
Kaggle CLI를 사용하실때

0:09:51.335,0:09:54.135
Kaggle 웹사이트가 바뀐적이 있으면

0:09:54.140,0:09:57.185
Kaggle CLI의 가장 최신 버전을
사용 중 이신지

0:09:57.400,0:10:03.440
pip install kaggle-cli --upgrade
명령으로

0:10:03.700,0:10:06.480
체크 해 보시길 바랍니다.

0:10:06.485,0:10:09.885
이 명령을 수행하면

0:10:09.885,0:10:13.660
Kaggle CLI 포함 관련 라이브러리들이
모두 가장 최신 버전인지 확인합니다

0:10:13.660,0:10:15.880
여기까지 하셨다면

0:10:15.885,0:10:17.535
설명을 따라해 보시면 되는데

0:10:17.535,0:10:20.345
Rashma가 작성한 마크다운에

0:10:20.345,0:10:25.180
보시면, Kaggle CLI 관련 알아둬야 할
모든 내용이 들어 있습니다

0:10:25.940,0:10:29.220
간단하게 설명드리면

0:10:29.395,0:10:32.745
다음으로 하셔야 할 것은

0:10:32.975,0:10:34.965
kg download 명령 뒤에

0:10:35.965,0:10:38.305
사용자명과

0:10:38.715,0:10:41.515
패스워드를 적어주시고,
경연 제목을 적어 주셔야 합니다

0:10:41.515,0:10:44.605
kg download -u 사용자명 -p 패스워드 -c 경연이름

0:10:44.605,0:10:47.865
많은 학생분들이 경연이름으로 뭘 넣어야 할지

0:10:47.905,0:10:50.545
헷갈려 하시는데

0:10:50.545,0:10:53.735
브라우져의 주소창에 보시면
/c/ 뒤에서 부터 /data 전까지의

0:10:53.735,0:10:56.755
부분이 이 경연의 이름 입니다

0:10:56.760,0:10:59.920
여기선 planet-understanding-the-amazon-from-space
가 되는군요

0:11:00.320,0:11:03.265
Kaggle CLI로 다운로드 하기 전에

0:11:03.265,0:11:05.920
웹 사이트에서 한번은

0:11:05.920,0:11:08.780
Download를 클릭해주시기 바랍니다

0:11:08.780,0:11:12.080
최초에 한번은 경연에 대한
규칙에 동의를 해줘야 하기 때문입니다

0:11:12.085,0:11:13.945
만약 이 과정을 깜박 하시면

0:11:14.405,0:11:17.445
kg download 명령어가
규칙 동의하는걸 깜박하지 않았냐는

0:11:17.445,0:11:20.335
힌트를 줄 겁니다

0:11:20.335,0:11:23.445
만약 Kaggle을 사용자이름이 아니라

0:11:23.445,0:11:26.575
구글 계정 같은 것으로 로그인 하는경우엔

0:11:26.575,0:11:29.625
동작하지 않을 겁니다. 
이때는 비밀번호 찾기를 수행하면

0:11:29.625,0:11:32.635
Kaggle에서 일반 비밀번호를
보내줄 겁니다

0:11:32.635,0:11:35.775
여기까지가 Kaggle에서
다운로드를 하는 과정입니다

0:11:35.775,0:11:38.925
Kaggle에서 데이터셋을 다운로드 하면
그 경연에 관련된 데이터가

0:11:38.925,0:11:41.000
들어있는 폴더를 통째로 받게 됩니다.

0:11:41.420,0:11:44.520
뭔가의 이유로 이 방법을
사용하지 않을도 모르는데요

0:11:44.525,0:11:47.275
첫번째 이유는 데이터셋이 Kaggle에
없는 경우 입니다.

0:11:47.275,0:11:50.385
두번째 이유는 Kaggle에 있는 데이터셋 전부를

0:11:50.385,0:11:53.365
다운로드 하고 싶지 않은 경우입니다

0:11:53.365,0:11:56.345
예를 들어서, 오늘 보게될 이 화면에 있는

0:11:56.345,0:11:59.355
경연은 두가지 포맷으로

0:11:59.355,0:12:02.585
데이터셋을 제공합니다.
TIF와 JPG죠

0:12:02.585,0:12:05.700
TIF 데이터셋은 19GB 크기이고
JPG는 600MB 정도인데

0:12:05.760,0:12:09.040
두가지 모두를 다운받는 경우는
많이 없을 겁니다

0:12:09.385,0:12:12.335
이 경우에 사용 가능한
커뮤니티 포럼에서 누군가 말해준

0:12:12.340,0:12:15.280
멋진 툴을 소개해 드리고자 합니다.

0:12:17.680,0:12:22.780
크롬 브라우져의 확장 어플이 있는데요
CurlWget 이라는 이름입니다

0:12:23.420,0:12:26.360
CurlWget을 검색하셔서

0:12:26.360,0:12:28.200
설치하시면 됩니다

0:12:28.200,0:12:30.140
이미 설치가 완료 되었다면

0:12:30.140,0:12:33.175
ADDED TO CHROME 이라고 표시되게 됩니다

0:12:33.180,0:12:34.600
지금부터는

0:12:34.600,0:12:37.460
뭐든지 다운로드 받고 싶은게 있으면

0:12:37.560,0:12:40.900
일단 다운로드 버튼을 눌러서
그 파일의 다운로드가 시작되게 한 후

0:12:41.080,0:12:44.020
다운로드 중인걸 취소 합니다

0:12:44.020,0:12:46.560
그리고, 우측 상단에 보시면 
노란색 버튼이 추가된것이 보이실 겁니다

0:12:46.880,0:12:49.820
눌러보시면,
다운로드 하기 위한 명령이 나와 있습니다

0:12:50.140,0:12:52.240
이 명령을 복사해서

0:12:52.740,0:12:55.480
터미널(콘솔) 윈도우 창에다가

0:12:56.040,0:12:58.500
붙여 넣고

0:12:58.840,0:13:00.460
Enter를 누르시면

0:13:00.785,0:13:03.975
다운로드가 시작됩니다.
이 확장 어플이 하는 일은

0:13:04.080,0:13:06.740
쿠키라던지, 헤더등 그 파일을 다운로드하는데 
필요한 모든 내용을

0:13:07.060,0:13:10.020
긁어오는 것입니다

0:13:10.025,0:13:12.805
Kaggle 뿐만 아니라

0:13:12.805,0:13:15.805
TV 프로그램이나 다른 데에서도

0:13:15.805,0:13:18.755
유용하게 사용될 수 있는 프로그램입니다

0:13:18.760,0:13:21.080
로그인이나 다른 이유에 의해 가려진 모든 종류의

0:13:21.500,0:13:24.620
컨텐츠를 가져올 수 있는 것이죠

0:13:24.635,0:13:27.565
데이터 과학에도 꽤나 유용하게
사용 될 수 있는데

0:13:27.565,0:13:32.240
왜냐하면 비디오 같은걸 종종
콘솔창에서 분석해야 하기 때문입니다

0:13:32.900,0:13:34.780
지금까지
데이터를 구하기 위한

0:13:34.780,0:13:36.420
두 가지 방법을
소개 드렸습니다

0:13:36.835,0:13:38.205


0:13:39.445,0:13:42.455
그러면, 데이터를 구한 다음에는

0:13:42.455,0:13:45.225
모델을 만들어야 합니다

0:13:45.675,0:13:48.635
아시겠지만, 그 전에

0:13:48.635,0:13:51.635
데이터가 data라는 폴더에
들어 있다고 가정하고 있습니다

0:13:51.675,0:13:53.755
Notebook이 있는 폴더의

0:13:53.755,0:13:56.885
하위 폴더입니다

0:13:56.885,0:13:58.795
반드시 데이터를

0:13:59.315,0:14:02.005
data 폴더에 넣어야 하는건 아니라서

0:14:02.005,0:14:05.005
/home 이나 
다른 하드 드라이브나

0:14:05.005,0:14:06.740
어디든지 넣어도 됩니다.

0:14:07.140,0:14:10.175
courses/dl1 폴더 안을 확인해 보시면

0:14:10.175,0:14:13.015
data 폴더라는건 사실

0:14:13.015,0:14:15.415
다른 장소를 가리키는

0:14:15.715,0:14:18.785
심볼릭 링크라는걸 알 수 있습니다

0:14:18.785,0:14:21.635
어디든지 원하는 곳에
데이터를 넣어두고

0:14:21.635,0:14:24.775
단순히 심볼릭 링크를 걸어주시면 됩니다

0:14:24.775,0:14:28.045
심볼릭 링크를 사용해보신적이 없다면

0:14:28.045,0:14:31.135
리눅스의 얼라이어스나, 맥이나 윈도우즈의 바로가기와
비슷한 것이라고 보시면 됩니다.

0:14:31.315,0:14:34.385
꽤나 유용하고, 포럼에 가보시면

0:14:34.385,0:14:35.965
사용법에 대한 글이 있습니다.

0:14:36.315,0:14:38.825
심볼링 링크에 대한 다른 한가지 예로

0:14:38.825,0:14:41.495
Notebook이 있는 폴더에 있는

0:14:41.495,0:14:44.325
fastai 모듈도 심볼링 링크가 걸린것이고

0:14:44.325,0:14:46.620
실제로는 다른 장소에 있습니다.

0:14:48.280,0:14:53.335
리눅스에서 실제 저장된 장소가 어딘지
확인해 보고 싶으면

0:14:53.335,0:14:56.345
ls 명령에 -l 옵션을 줘서
폴더 내에 들어있는 것의 목록을

0:14:56.345,0:15:01.920
출력하면 됩니다
그러면 심볼링 링크가 걸린 곳을 확인 가능합니다

0:15:02.360,0:15:04.040


0:15:05.545,0:15:07.155


0:15:07.700,0:15:08.700


0:15:09.340,0:15:13.720
지금까지 우리가 한 것중에 약간
모호한게 있다면

0:15:13.720,0:15:15.620
모든걸 처음부터 끝까지 수행하는데

0:15:15.620,0:15:18.840
필요한 최소한의 코드양이
얼마나 되는가 입니다

0:15:18.845,0:15:21.935
여기 띄워진 화면을 넘어가지 않는
영역에서 보시는것이

0:15:21.935,0:15:25.385
고양이, 개를 분류하는 문제에 대한
세계적 수준의 결과물을 얻기 위해서

0:15:25.385,0:15:28.485
처음부터 끝까지 수행한 전체 과정 입니다

0:15:28.485,0:15:31.175
단 한가지 건너뛴 단계가 있는데

0:15:31.175,0:15:34.405
Kaggle에서 데이터를 다운로드 하고

0:15:34.405,0:15:36.745
압축을 푸는 과정 입니다.

0:15:37.095,0:15:40.075
말 그대로 모든 과정을
보여드리고 있는 겁니다

0:15:40.320,0:15:42.000


0:15:42.000,0:15:45.560
fastai 라이브러리를 임포트 합니다.
conv_learner를 임포트하면

0:15:45.560,0:15:48.980
기본적으로 다른 나머지것들을
모두 임포트 합니다

0:15:48.980,0:15:50.360


0:15:50.360,0:15:53.640
데이터셋이 있는 곳의 PATH를 지정해 주고

0:15:54.080,0:15:57.380
이미지 크기와, 배치 크기를 정해줍니다

0:15:57.380,0:15:57.880


0:15:58.575,0:16:01.575
tfms_from_model이 뭘 하는지
잠시후에 더 많이 배우게 되겠지만,

0:16:01.575,0:16:05.015
기본적으로 어떻게 데이터를
변형할지를 정의하는 겁니다

0:16:05.095,0:16:07.765
이 특정 모델에 알맞은 형태로

0:16:07.765,0:16:10.775
이미지 데이터를 변형하는데

0:16:10.775,0:16:13.775
이미지가 측면에서 촬영됐다고 가정하고 있고

0:16:13.775,0:16:16.565
최대 10% 확대를 허용하고 있습니다.

0:16:16.865,0:16:19.645
그리고 설정된 PATH에서

0:16:19.875,0:16:22.845
데이터를 가져오게 됩니다.

0:16:22.845,0:16:25.935
기억 하시겠지만, PATH 안에는
train, valid 같은 폴더가 있고

0:16:25.935,0:16:29.005
또 그 안에는 dogs와 cats 같은
같은 폴더가 있었죠.

0:16:29.005,0:16:31.660
한가지 알아두실건

0:16:31.660,0:16:35.455
train이나 valid의 폴더의 이름을 
다른 것으로 변경할 수 있는데요

0:16:35.455,0:16:38.465
데이터가 다른 폴더에 들어 있는 경우

0:16:38.465,0:16:41.645
그 폴더 이름을 train, valid로 변경해도 되지만

0:16:41.645,0:16:45.320
함수 옵션을 보시면, 폴더명을
이정해 줄 수도 있습니다.

0:16:45.800,0:16:46.360


0:16:46.360,0:16:49.720
또한 test에 대한 폴더 이름도 
지정 가능합니다

0:16:49.725,0:16:52.775
결과를 Kaggle에 제출하거나를 원하시면

0:16:52.775,0:16:55.415
레이블링되지 않은 테스트 데이터셋이

0:16:55.420,0:16:57.780
포함된 폴더명을 넣어줘야 합니다.

0:16:58.000,0:16:59.000


0:16:59.440,0:17:03.175
그리고 다음으로는 미리 학습된 
모델로부터 모델을 생성합니다

0:17:03.175,0:17:06.155
미리 학습된 모델로는 resnet50이 사용 되었고,

0:17:06.155,0:17:09.365
방금 생성한 data 객체를 넣어줍니다.
그리고나서 , fit() 메소드를 호출하게 됩니다

0:17:09.365,0:17:12.900
디폴트로는
마지막 몇개 계층을 제외한

0:17:12.900,0:17:16.860
모든 계층들이 freeze 됩니다.

0:17:17.640,0:17:19.840
그리고 결과가
보여지고요

0:17:19.840,0:17:20.340


0:17:20.540,0:17:23.725
약 2분 30초 정도 걸렸습니다

0:17:23.725,0:17:26.785
그리고  precompute=True를 설정하지
않았다는걸 알아두세요

0:17:26.785,0:17:30.480
이게 의미하는게 뭔지 약간 혼란스러운
분들이 계신것 같은데

0:17:30.880,0:17:33.605
단순히 첫번째 단계에서

0:17:33.605,0:17:36.585
뭔가를 빠르게 하기 위한 것입니다

0:17:36.585,0:17:39.665
그냥 건너뛰어도 상관 없는 부분입니다.

0:17:39.665,0:17:42.765
이 내용에 혼란을 느끼거나
이 내용이 어떤 문제를 일으킨다면

0:17:42.765,0:17:44.960
그냥 놔두고 건너뛰세요

0:17:45.680,0:17:48.595
단순히

0:17:48.595,0:17:51.375
몇 중간단계를 캐싱해서
재계산 할 필요 없게 만들어주는

0:17:51.380,0:17:55.400
일종의 지금길 입니다.
그리고 미리 계산된 activation을 사용할땐

0:17:55.400,0:17:57.515
data augmentation이

0:17:57.515,0:18:00.185
동작하지 않습니다.

0:18:00.185,0:18:03.285
data augmentati on을 요청했는데
precompute 값이 True이면

0:18:03.725,0:18:06.705
어떠한 data augmentation도 일어나지 않는 것이죠

0:18:06.705,0:18:10.040
왜나하면 캐시된 activation을 사용하기 때문입니다

0:18:10.680,0:18:13.665
여기서는 모든걸 가능한한 쉽게
설명드리기 위해서

0:18:13.665,0:18:16.525
precompute 부분을 포함시키지
않았습니다.

0:18:16.525,0:18:19.160
그리곤 길이가 1인 cycle을 세번 수행했습니다.

0:18:19.960,0:18:21.520
그 다음으로는

0:18:21.580,0:18:25.420
unfreeze를 호출해서, 이번엔
네트워크 전체가 학습되도록 설정 했습니다.

0:18:25.840,0:18:28.825
bn_freeze()를 그 다음으로 사용 했는데

0:18:28.825,0:18:31.935
아직 보지 못한 내용이고
잠시 후에 배우게 될 것인데

0:18:31.940,0:18:35.040
일단 지금 알아두셔야 할 
내용을 말씀드리겠습니다

0:18:35.040,0:18:36.860
resnet50나

0:18:37.185,0:18:40.075
resnext101 같이

0:18:40.075,0:18:43.385
엄청 크고 깊은 모델을 사용하는데

0:18:43.385,0:18:46.385
데이터셋이 개/고양이처럼

0:18:46.385,0:18:49.465
어떤 물체를 표준적인 방식으로

0:18:49.465,0:18:51.495
측면에서 촬영한 사진이

0:18:51.495,0:18:54.045
ImageNet의 데이터셋과

0:18:54.045,0:18:56.920
크기가 비슷한 200~500 픽셀 정도 된다면

0:18:57.820,0:19:01.280
아마도 unfreeze 할때

0:19:01.285,0:19:03.935
bn_freeze 함수를 
호출해야 할 겁니다

0:19:03.935,0:19:06.855
좀더 심화 학생들을 위한 설명은

0:19:06.855,0:19:09.935
batch normalization을 
평균쪽으로 움직여서

0:19:09.935,0:19:12.905
업데이트 하는 것입니다

0:19:12.905,0:19:15.955
이 코스의 중반 이후에
그게 무슨 의민지 배우게 될 겁니다

0:19:15.960,0:19:19.960
다른 라이브러리에서는 지원되지 않는데
꽤나 중요하다고 판단되는 것 중 하나입니다

0:19:19.960,0:19:22.535
쨋든 한번의 추가 에포크를

0:19:22.540,0:19:25.480
전체 네트워크를 학습시키기 위해 수행합니다

0:19:26.120,0:19:29.555
그리곤 마지막엔 TTA를 수행합니다

0:19:29.555,0:19:31.935
TTA를 수행해서
가능한한 최고의

0:19:31.935,0:19:34.815
예측 결과를 얻을 수 있도록 하는 것입니다

0:19:34.815,0:19:37.885
그래서 결과적으로
99.45%의 정확도를 얻었습니다

0:19:37.885,0:19:39.920
이게 전부 입니다

0:19:39.940,0:19:42.620
새로운 데이터를 가지고
시도하는 경우

0:19:43.025,0:19:46.085
지금까지 보여드린 것이

0:19:46.085,0:19:49.405
수행되어야 할 최소한의 과정 입니다.

0:19:49.405,0:19:52.445
그런데 여기서는 학습률 뭐로 사용할지

0:19:52.445,0:19:55.575
폴더 구조가 어떻게 생겼는지

0:19:55.575,0:19:58.815
이미 알고 있다고 가정한채
진행을 했습니다

0:19:59.065,0:20:00.105


0:20:01.285,0:20:03.905
쨋든 보여드린게
최소한의 과정입니다.

0:20:03.905,0:20:05.585
그리고, 한가지

0:20:06.255,0:20:09.325
fastai가 아닌 다른 라이브러리를 사용해서

0:20:09.325,0:20:12.355
동일한 과정을 어떻게 하는지
알려드리고 싶은데요

0:20:12.355,0:20:15.485
다른 라이브러리 중 설명드리기

0:20:15.485,0:20:18.745
가장 좋은 것은 Keras 입니다.

0:20:18.745,0:20:21.775
fastai가 PyTorch 위에 만들어진것 처럼

0:20:21.775,0:20:24.825
Keras도 다른 많은 종류의 백엔드 위에
만들어졌습니다.

0:20:24.825,0:20:27.435
요즘엔 대부분의 사람들이
백엔드로

0:20:27.435,0:20:28.795
TensorFlow를 사용합니다

0:20:29.105,0:20:31.835
하지만 MXNet (아마존) 또는

0:20:31.840,0:20:35.240
CNTK (마소) 를 백엔드로 사용할 수도 있습니다

0:20:36.700,0:20:40.180
git pull로 저장소 내용을 내려받으면

0:20:42.265,0:20:45.275
keras_lesson1 이라는 파일이 있는데요,

0:20:45.275,0:20:46.315
이 파일의 내용은

0:20:46.805,0:20:49.595
lesson1을 Keras로 동일하게
만들어 본 것입니다.

0:20:49.860,0:20:54.440
다른 라이브러리는 어떻게 하는지에 대한
느낌을 알게 해주기 위함이죠

0:20:57.320,0:21:01.175
일단 지금은 bn_freeze에 대해선
이야기 하지 않겠습니다.

0:21:01.180,0:21:06.060
단지, 모델이 resnet34보다 큰

0:21:06.180,0:21:09.480
resnet50 라던지 resnext101같은

0:21:09.485,0:21:12.455
모델을 사용할때

0:21:12.460,0:21:15.800
학습의 대상 데이터셋이

0:21:16.040,0:21:19.155
평범한 사진과 크기,
물체가 사진 전반을 차지하는등

0:21:19.155,0:21:21.945
ImageNet과 매우 유사하다면

0:21:21.945,0:21:24.865
unfreeze 이후에
learn.bn_freeze(True)를

0:21:24.865,0:21:27.895
아마도 추가해야 할 것입니다.

0:21:27.900,0:21:30.120
궁금하시면

0:21:30.120,0:21:33.600
bn_freeze 포함 또는 미포함
모두 학습을 시도해 보세요

0:21:33.600,0:21:37.420
좀더 심화 학생분들은 포럼에서
관련 내용을 토론하실 수 있겠지만

0:21:37.420,0:21:40.020
일단 저희는 전체 코스의 중반 이후인

0:21:40.020,0:21:43.080
CNN image in depth 부분에서

0:21:43.240,0:21:47.180
이 내용을 자세히 다룰 예정입니다.

0:21:51.640,0:21:54.620
Keras를 사용하는 법을 잠시 보여드리겠습니다

0:21:54.900,0:21:58.780
일단 이거저거 많이 임포트 합니다

0:21:58.780,0:22:00.780
그리고...

0:22:01.420,0:22:04.355
말씀드린것 처럼

0:22:04.355,0:22:07.405
train, valid 폴더,
그리고 dogs, cats 폴더 처럼

0:22:07.405,0:22:09.780
구조를 잡는건 레이블링된

0:22:09.780,0:22:13.320
이미지를 제공하는
일종의  표준적인 방식 입니다.

0:22:13.320,0:22:15.615
그리고 Keras도 이 방식을 사용합니다

0:22:15.620,0:22:19.780
어디에 학습용, 검증용 데이터셋이 있는지
배치 크기를 뭐로 설정할건지

0:22:19.960,0:22:24.040
알려줘야 하는거죠
(batch_size가 두번 정의되어서 삭제함)

0:22:24.140,0:22:27.145
보시다시피, Keras를 사용하면

0:22:27.145,0:22:31.040
전과 비교해서 훨씬 더 많은
코드가 동일한 일을 수행하는데 필요합니다

0:22:31.380,0:22:34.215
그리고 각 코드의 부분마다

0:22:34.215,0:22:37.125
설정해야할 값들이
아주 많습니다.

0:22:37.125,0:22:41.300
만약 값을 잘못 설정하면
모든게 망가져 버리겠죠

0:22:43.040,0:22:46.015
각 코드의 의미를 간단하게
요약해 드리겠습니다

0:22:46.020,0:22:48.520
기본적으로 Keras에서는

0:22:48.520,0:22:51.240
단 하나의 data 객체를 만들기 보단

0:22:51.240,0:22:54.195
일단 어떻게 data를 생성할지를 정하는

0:22:54.195,0:22:57.205
DataGenerator라는걸 정의 해야 합니다

0:22:57.205,0:23:01.220
DataGenerator 정의시, 어떤 종류의
data augmentation 을

0:23:01.220,0:23:04.100
하고 싶은지 명시 해주고

0:23:04.500,0:23:08.215
어떤 종류의 정규화 기법을 사용할지를

0:23:08.220,0:23:10.800
사용할지를 명시해줘야 합니다

0:23:10.800,0:23:12.320
반면에, fastai로는

0:23:12.540,0:23:15.435
뭐가 됐던지 ResNet50가

0:23:15.435,0:23:18.715
원하는게 있다면, 
그것을 해달라고 하기만 하면 됩니다.

0:23:18.715,0:23:21.735
뭐기 필요한지 야주 약간만 알면 됩니다

0:23:21.735,0:23:24.485
일반적으로, 인터넷에서 Keras 코드를
복사&붙여넣기 하면

0:23:24.485,0:23:28.440
잘 동작하는 코드라고 확신할 수 있습니다

0:23:29.240,0:23:32.165
그리고, Keras에는

0:23:32.165,0:23:35.045
표준적으로 가장 잘 동작하는

0:23:35.045,0:23:37.865
data augmentation 파라메터 값들이

0:23:37.865,0:23:40.915
정의되어 있지 않기 때문에,
여기 보시는 코드는 단순히

0:23:40.920,0:23:43.180
Keras 도큐먼트에서 복사해온 것입니다

0:23:43.180,0:23:45.035
최고의 파라메터

0:23:45.035,0:23:48.325
조합인지는 모르겠지만,
공식 문서에서 사용되는 것이라는 거죠

0:23:48.605,0:23:51.275
어떻게 data를 생성할지를

0:23:51.275,0:23:53.985
수평으로 뒤집거나 확대를 하거나

0:23:53.985,0:23:56.925
잘라내거나 하는 것으로 정의하고

0:23:56.925,0:23:59.705
그 내용을 기반으로 data 생성자 객체를
생성합니다

0:23:59.705,0:24:02.840
이때, 생성 대상이 되는 이미지가

0:24:02.880,0:24:06.015
들어있는 폴더를 지정해줍니다

0:24:06.020,0:24:08.100
fastai에서 사용하는 것과

0:24:08.120,0:24:11.240
동일한 폴더 구조를 가지는 폴더를
명시했습니다

0:24:11.480,0:24:14.560
몇 파라메터는 fastai 내용과

0:24:14.560,0:24:17.900
겹치는데요, 
생성하고자 하는 이미지의 크기라던지

0:24:17.900,0:24:20.575
미니 배치의 배치 크기라던지 말입니다

0:24:20.575,0:24:23.545
class_mode는 별로 자세히 안보셔도 되는데

0:24:23.545,0:24:25.935
예측 결과의 대상이 두가지인경우

0:24:25.935,0:24:28.465
'binary' 라고 적어주면 되고,

0:24:28.465,0:24:31.535
두가지 이상인 경우라면, 
"categorical" 이라고 적어주면 됩니다

0:24:31.535,0:24:34.915
여기서는 개냐? 고양이냐? 문제니까
'binary' 입니다

0:24:36.125,0:24:38.585
fastai 대비 좀 더

0:24:38.585,0:24:40.895
복잡해지는 부분으로는

0:24:40.895,0:24:43.725
검증 데이터셋에 대해서도
동일한 코드를 적어줘야 한다는 겁니다

0:24:43.725,0:24:46.995
TTA를 사용하지 않는다면 
data augmentation이 없는 data 생성자를

0:24:47.000,0:24:52.040
정의해 줘야 하는데,
직접 그런 부분까지 코드를 적어줘야 하는 것이죠

0:24:52.860,0:24:53.860
그리고

0:24:54.445,0:24:57.405
학습할때는 이미지들의 순서를

0:24:57.405,0:24:59.835
무작위로 섞어줘서, 무작위로 선택된

0:24:59.835,0:25:02.785
이미지들을 학습될 수 있도록 하는데

0:25:02.785,0:25:06.400
검증단계에서는 그러지 않아도 되므로
shuffle=False로 설정 하였습니다

0:25:06.400,0:25:09.015
검증 데이터셋을 무작위로 섞게 되면

0:25:09.015,0:25:12.505
모델이 얼마나 잘 하는지를 추적하지 못합니다
레이블과 순서가 달라지기 때문입니다

0:25:13.525,0:25:16.775
여기까지 보여드린 코드가
기본적으로 Keras를 사용하면

0:25:16.780,0:25:18.640
매번 작성해야 하는 것입니다

0:25:19.000,0:25:19.840


0:25:19.840,0:25:21.380
제가 이전 예제에서

0:25:21.385,0:25:24.415
ResNet50을 선택했던 이유는
Keras에는

0:25:24.415,0:25:27.385
ResNet34가 없기 때문인데,

0:25:27.385,0:25:30.065
fastai와 비교해보고 싶었기 때문입니다

0:25:30.260,0:25:31.260


0:25:32.200,0:25:35.120
그리고,
Keras에는 fastai와는 다르게

0:25:35.120,0:25:37.960
특정 데이터셋에 맞아 들어가는

0:25:37.960,0:25:41.115
모델을 생성하기 위한 방법이 없어서
직접 이부분을 코딩 해줘야 하죠

0:25:41.115,0:25:44.545
그 과정은 일단,
사용하려는 기본 모델을 생성하고

0:25:45.160,0:25:50.620
그 모델 위에 추가하려고하는
계층들을 직접 넣어줘야 합니다

0:25:51.500,0:25:54.415
이 코스 후반쯤 가면
왜 이 특정 3개의 계층이

0:25:54.420,0:25:57.820
추가됐는지를 알게 될 겁니다

0:25:59.020,0:26:01.915
여기까지 하면, 다음으로는

0:26:01.915,0:26:04.995
모델을 생성하면 되는데

0:26:05.125,0:26:08.175
Keras에서는 자동으로 freeze 하기위한

0:26:08.175,0:26:10.695
API가 없기 때문에

0:26:10.695,0:26:12.635
각 계층들을 loop 해서

0:26:12.985,0:26:15.835
freeze 하고자 하는 계층들에 대해서

0:26:15.835,0:26:19.105
trainable 값을 False로 설정해 줘야 합니다

0:26:19.755,0:26:22.875
또, Keras에는
fastai나 PyTorch에는 없는 개념인

0:26:22.875,0:26:25.895
모델을 컴파일(compile) 한다는 것이 있는데

0:26:25.895,0:26:29.045
일단 모델이 사용할 준비가 되었다면,
complie 해줘야 합니다.

0:26:29.045,0:26:31.855
이때, 어떤 optimizer를 사용할지
어떤 손실함수(loss) 를 사용할지

0:26:31.855,0:26:34.695
어떤 평가지표를 사용할지를
알려줘야 합니다.

0:26:35.140,0:26:38.100
fastai에서는 이런 것들을
알려줄 필요가 없습니다

0:26:38.100,0:26:41.220
어떤 것들을 사용할지 이미 정해져 있기 때문입니다

0:26:41.340,0:26:43.420
하지만, 이 디폴트 내용을

0:26:43.420,0:26:45.520
다른것으로 교체할 수도 있습니다

0:26:45.975,0:26:49.005
여기까지 완료 하게 되면
fastai에서 fit()을 호출한것과 다르게

0:26:49.005,0:26:51.785
Keras에서는 fit_generator()를 호출해야 합니다

0:26:51.785,0:26:55.280
이때, 앞에서 정의한
학습 데이터셋 생성자, 검증 데이터셋 생성자

0:26:55.280,0:26:58.200
두 개의 data 생성자를 넣어줘야 합니다

0:26:58.620,0:27:01.375
한가지 이해하기 힘든 부분이 있는데

0:27:01.375,0:27:04.185
Keras에서는 하나의 에포크에

0:27:04.185,0:27:07.165
몇개의 배치가 들어가는지를
알려줘야 합니다

0:27:07.165,0:27:11.240
학습 데이터셋이 크기를
배치 크기로 나눠줘야 하죠

0:27:11.880,0:27:15.360
그리고 에포크의 수를 알려줘야 하고

0:27:15.420,0:27:18.480
workers는 데이터 전처리에 사용되는

0:27:18.480,0:27:21.240
프로세스(작업자)의 수를 나타냅니다

0:27:21.440,0:27:23.600
fastai에도 있는 기능이지만

0:27:23.600,0:27:27.540
Keras에서는 디폴트로는 사용되지 않습니다

0:27:27.540,0:27:28.800


0:27:28.800,0:27:33.180
적당히 빠른 처리 속도를 원하면
workers 옵션을 포함 시키셔야 할 겁니다

0:27:33.480,0:27:36.415
기본적으로 여기까지가

0:27:36.420,0:27:41.300
추가된 마지막 계층들에 대한
미세조정을 하기 위한 코드 입니다

0:27:42.200,0:27:46.440
결과를 보시면, 검증 데이터셋에 대한
95%의 정확도를 얻었습니다

0:27:46.440,0:27:48.895
근데 95%로 도달하는 과정이 
약간 이상합니다

0:27:48.895,0:27:51.475
시작 했을때 49%인 정확도가

0:27:51.480,0:27:54.320
69%에서 95%로 변화 했습니다

0:27:54.760,0:27:58.080
왜 초반에 이렇게 낮은지 잘 모르겠군요.
제 생각엔 일반적이지 않은거 같아요

0:27:58.085,0:28:01.115
제 코드나, Keras에 버그가 있을지도 몰라서

0:28:01.115,0:28:04.125
누군가가 원인을 알아내주길
트위터에 글을 올렸는데요

0:28:04.125,0:28:06.775
아직까진 아무도 원인을 모릅니다

0:28:06.780,0:28:10.840
이러한 상황이
제가 fastai를 이 코스에서

0:28:10.840,0:28:13.675
사용하려는 이유 중 하나입니다.

0:28:13.675,0:28:16.675
뭔가를 망치기가 오히려 어렵거든요

0:28:16.680,0:28:19.780
제가 망치든 누군가가 망치든 말이에요

0:28:22.305,0:28:25.325
맞아요. 여기서 Keras의 백엔드는
TensorFlow가 사용 되었습니다

0:28:25.325,0:28:28.325
만약 이 코드를 직접 실행해 보고 싶으시면

0:28:28.325,0:28:31.335
Terminal 창에 가셔서

0:28:31.340,0:28:37.560
pip install tensorflow-gpu keras
명령을 실행해 주세요

0:28:38.000,0:28:40.735
fastai 환경의 일부가 아니니까

0:28:40.740,0:28:44.080
따로 설치해 주셔야 합니다

0:28:44.680,0:28:49.100
이 명령으로 설치되는 내용이
notebook 실행에 필요한 모든 것입니다

0:28:50.200,0:28:51.380


0:28:52.735,0:28:53.735
그리고

0:28:54.155,0:28:56.925
Keras에는 계층 그룹핑이나

0:28:56.925,0:28:59.855
차등 학습률이나,
부분적 unfreeze같은게 없습니다

0:28:59.855,0:29:02.865
그렇기 때문에, 뭘 하던지 간에

0:29:02.865,0:29:05.635
수동적으로 모든것을 적용해야 합니다

0:29:05.635,0:29:08.375
모든 계층들을 전부 출력해서
그 중 얼마나 많은 계층들을 미세조정할지

0:29:08.375,0:29:10.915
여기서는 140번째 이후의 모든 계층에 대해서

0:29:10.920,0:29:13.955
미세 조정을 적용 했습니다
미세 조정이 끝나면

0:29:13.960,0:29:16.740
다시한번 컴파일(compile)을
해 줘야 합니다

0:29:16.740,0:29:18.480
그리곤 학습을 다시 수행해야 하죠

0:29:18.975,0:29:21.785
다시 학습해도 마찬가지로

0:29:21.785,0:29:24.255
학습 데이터셋에 대한 정확도는
거의 비슷하게 유지 되었지만

0:29:24.260,0:29:27.220
검증 데이터셋에 대한 정확도에는
뭔가 문제가 있는거 같습니다

0:29:27.500,0:29:31.680
(잘못 말함)

0:29:31.680,0:29:34.740
Keras로 진행하면서
주로 알 수 있는 것은

0:29:35.020,0:29:38.375
약간 성가실 정도로
코드의 양이 많다는 것과

0:29:38.375,0:29:41.405
성능이 fastai와 비교해서
매우 다르다는 것입니다.

0:29:41.405,0:29:44.785
학습 데이터셋에 대해선 4번의 에포크 후에
97% 정도의 정확도를 얻었고

0:29:44.785,0:29:47.765
약 8분정도 소요 되었는데

0:29:47.765,0:29:51.255
fastai와 비교해 보면,

0:29:51.585,0:29:54.515
검증 데이터셋에 대한
정확도가 99.5% 정도이고

0:29:54.515,0:29:57.525
학습 시간도 약 4-5분 정도 걸려서

0:29:57.525,0:30:01.440
훨씰 빠르게 수행되었습니다

0:30:05.740,0:30:10.000
하시고자 하는 일이 뭔지에 따라 다릅니다

0:30:10.020,0:30:13.360
만약 모바일 기기로 배포해야 한다면

0:30:13.735,0:30:16.565
모바일에 대한 PyTorch의 지원은

0:30:16.565,0:30:19.075
아직 초기 단계이기 때문에

0:30:19.075,0:30:22.115
TensorFlow를 사용해야 할지도 모릅니다

0:30:22.115,0:30:25.035
또는 몸담고 있는 회사가

0:30:25.035,0:30:28.125
TensorFlow를 사용하기로
이미 결정했다면 이를 사용해야 하겠지요

0:30:28.125,0:30:31.115
이 코스에서 배운 내용을
TensorFlow로

0:30:31.115,0:30:34.065
변경하거나, 새로 작성해야 한다면

0:30:34.065,0:30:37.015
우선은 Keras를 사용해야 할 지도 모릅니다

0:30:37.015,0:30:40.105
하지만 최신예적인 결과를 얻기 위해선

0:30:40.240,0:30:42.880
fastai와 비교해서

0:30:43.160,0:30:46.180
기본적으로 좀 더 어렵고,
좀 더 시간을 할애해야 합니다

0:30:46.185,0:30:49.485
fastai에서 제공하는

0:30:49.485,0:30:52.365
모든 최신예 알고리즘들을

0:30:52.365,0:30:55.415
똑같이 만들어야 하기 때문에
동일한 수준의 결과를 얻는것은

0:30:55.415,0:30:57.385
쉽지 않을 것입니다.

0:30:58.100,0:31:02.620
하지만 기본적인
아이디어는 아주 유사합니다

0:31:02.620,0:31:04.275
그리고

0:31:04.275,0:31:06.905
저희가 fastai로 하는 것들을

0:31:06.905,0:31:09.995
똑같이 해내는게 불가능 하지 않다는 겁니다

0:31:09.995,0:31:12.565
하지만, 재시작하는 확률적 경사하강이나

0:31:12.565,0:31:14.955
차등 학습률,

0:31:14.955,0:31:18.025
batch normalization freeze 같은 것들을
구현해야 할 겁니다

0:31:18.025,0:31:20.965
하지만, 다행히(?)

0:31:20.965,0:31:24.345
포럼의 누군가가 fastai를

0:31:24.345,0:31:27.215
Keras 또는 Tensorflow 에 호환 가능한 형태

0:31:27.215,0:31:30.705
만드는걸 시도하고 있는것 같습니다

0:31:30.755,0:31:33.705
몇 주전에 구글에도 이야기를 했는데

0:31:33.705,0:31:36.825
fastai를 Tensorflow로 포팅하는데
 꽤나 흥미를 보이는것 같았습니다

0:31:37.185,0:31:40.285
이 강의를 온라인으로 들으실때 쯤이면

0:31:40.285,0:31:43.180
그런게 존재할 지도 모르겠군요

0:31:46.025,0:31:48.525
어쨋든

0:31:48.525,0:31:52.300
Keras와 TensorFlow는

0:31:53.440,0:31:56.095
다루는게 그렇게 어렵지 않으니까

0:31:56.095,0:31:58.945
사용해야 할 때가 온다면 걱정하지 마세요

0:31:58.945,0:32:02.365
이 코스를 완주하고나면,
수 일 내에 배울 수 있을 겁니다

0:32:07.705,0:32:10.955
현재까지 설명드린게

0:32:10.955,0:32:13.635
지난주의 숙제를

0:32:13.635,0:32:16.755
완료하기 위해서

0:32:16.755,0:32:18.985
알아둬야할 거의 모든 것입니다
(데이터셋만 개 품종으로 바뀌었죠)

0:32:19.405,0:32:20.875


0:32:21.655,0:32:23.925
상기 차원에서 말씀드리는 건데

0:32:24.025,0:32:27.225
지난주 강의의 마지막 부분에서

0:32:27.225,0:32:29.355
데이터를 탐구하는 방법을

0:32:29.765,0:32:32.805
조금 보여드렸습니다.

0:32:32.805,0:32:36.175
분류 카테고리들이 뭐가 있으며

0:32:36.180,0:32:38.900
이미지 크기들이 어떻게 구성되는지
같은 것들 말입니다

0:32:38.960,0:32:41.985
만약 그 부분이 기억 안나시거나
지난주의 내용이 이해가 잘 안되시면

0:32:41.985,0:32:44.695
지난주의 강의를 다시 한번
시청하시길 바랍니다

0:32:44.985,0:32:48.055
한가지 이야기 하지 않은 것이 있는데

0:32:48.055,0:32:51.085
어떻게 Kaggle에 결과를
제출하는지에 대한 것입니다

0:32:51.085,0:32:54.395
어떻게 예측 결과를 얻는지에 대한 것으로
그 부분을 보여드리도록 하겠습니다

0:32:54.985,0:32:57.895
이번주의 Wiki에는 이미 어떻게 하는지

0:32:57.895,0:33:01.135
적어뒀으니 확인해 보시기 바랍니다

0:33:01.135,0:33:04.255
일단 Kaggle 웹사이트에 가보시면

0:33:04.255,0:33:07.255
모든 경연들은 'evaluation' 이라는 섹션이 있습니다

0:33:07.255,0:33:10.265
이 섹션이 뭐를 제출해야 하는지에 대해서
설명을 해줍니다

0:33:10.265,0:33:13.605
그 섹션의 내용의 일부 중 두 줄을 복사해서
보이시는 화면에 붙여넣었습니다

0:33:13.945,0:33:17.395
이게 의미하는 것은
한 파일을 제출해야 하는데

0:33:17.585,0:33:20.935
그 파일의 첫번째 줄에는
문자 그대로 id 부터

0:33:21.240,0:33:25.180
, (콤마)로 이어지는 모든 개 품종들이 열거 됩니다

0:33:25.180,0:33:27.415
그리고나서, 
그 다음 줄 부터는

0:33:27.420,0:33:29.660
예측 결과 하나하나에 대한

0:33:29.660,0:33:34.360
이미지 id와 예측 결과에 표시되는
각 개 품종별 확률값을 넣어줘야 합니다

0:33:34.720,0:33:37.400
이 파일을 어떻게 만들어야 할까요?

0:33:37.820,0:33:40.855
우선 저희가 만든 data 객체에는

0:33:40.855,0:33:43.715
.classes 속성값이 있는데

0:33:43.715,0:33:46.525
알파벳 순서대로 모든 
분류 카테고리의

0:33:46.525,0:33:48.085
목록(배열)을 표현합니다

0:33:49.420,0:33:50.780
그리고

0:33:51.580,0:33:54.840
모든 서로다른 분류 카테고리(클래스)를 가져왔고

0:33:55.695,0:33:58.735
data 객체에 있는 .test_ds 속성은
테스트 데이터셋을 의미합니다

0:33:58.740,0:34:04.700
테스트 데이터셋의 .fnames 속성값은
테스트 데이터셋에 포함된 모든 파일이름 목록을 가집니다

0:34:04.800,0:34:07.795
한가지 상기시켜 드릴 내용이 있는데

0:34:07.800,0:34:10.080
개 품종 데이터셋은

0:34:10.080,0:34:13.455
각 분류 카테고리가 서로다른
폴더로 존재하는

0:34:13.455,0:34:16.195
Keras 가 원하는 형태(포맷)로 제공되지 않았습니다

0:34:16.195,0:34:18.965
대신에 각 레이블에 대한 정보를
CSV 파일로 제공합니다

0:34:18.965,0:34:23.040
그래서 CSV 파일이 있을땐

0:34:23.860,0:34:26.865
ImageClassifierData 클래스의
from_path 메소드 대신에

0:34:26.865,0:34:30.025
from_csv 메소드를 사용했습니다

0:34:30.025,0:34:33.025
Keras에는 이와 동일한 기능의
함수가 없어서

0:34:33.025,0:34:36.055
Kaggle 포럼에서는 사람들이

0:34:36.055,0:34:38.725
Keras 에서 사용되는 형태로

0:34:38.725,0:34:41.845
변환하는 스크립트를 공유하곤 합니다

0:34:41.845,0:34:45.380
fastai를 사용사면, 단순히 CSV 파일을 
from_csv 메소드에 던져주기만 하면 됩니다.

0:34:46.000,0:34:49.015
CSV 파일은
data 객체에게

0:34:49.015,0:34:52.295
어떤 분류 종류들이 있는지를
자동으로(?) 알려주고

0:34:52.785,0:34:55.815
테스트 데이터셋의 폴더에 있는

0:34:55.820,0:34:59.940
이미지 파일들이 뭐가 있는지도
알아낼 수 있습니다

0:35:00.540,0:35:03.200
이 두 가지의 정보를 가지고 있으면

0:35:03.720,0:35:06.625
Kaggle에 결과를 제출할
준비가 갖춰진 것입니다

0:35:06.625,0:35:10.620
개와 고양이 예제에서 본 것 처럼,
항상 TTA를 사용하는게 좋다고 생각 하는데

0:35:10.620,0:35:12.960
결과를 많이 향상시켜 주기 때문이죠

0:35:12.960,0:35:15.740
특히나 만든 모델이 비교적 덜 좋다고 
생각되면 더욱 그렇습니다

0:35:16.120,0:35:19.120
이를 위해서 learn.TTA 메소드를 
호출하면 되는데

0:35:19.460,0:35:25.920
learn.TTA 메소드에
is_test 파라메터 값을

0:35:26.220,0:35:29.640
True로 지정해주면,
TTA를 사용한 예측 대상을

0:35:30.000,0:35:34.500
검증 데이터셋이 아닌
테스트 데이터셋으로 지정함을 의미합니다

0:35:35.020,0:35:39.700
그리고 당연한 것이지만,
테스트 데이터셋에 대해서는

0:35:40.080,0:35:45.280
정확도 값을 구하지 못합니다. 왜냐하면, 
테스트 데이터들은 레이블링되어 있지 않으니까요

0:35:46.160,0:35:49.155
디폴트로서, 대부분의 PyTorch 기반의 모델은

0:35:49.155,0:35:52.165
예측 결과에 대한 Log(로그) 값을
돌려주는데요,

0:35:52.165,0:35:57.800
이 값들을  확률값들로 되돌려 놓으려면,
 np.exp 메소드에 이 값을 던져주면됩니다

0:35:58.180,0:36:02.040
현재 예제에서 테스트 데이터셋은
10,357개의 이미지로 구성되어 있고

0:36:02.155,0:36:04.925
분류 대상이 되는 개 품종의 종류는
120개 정도가 존재합니다

0:36:04.925,0:36:09.320
예측 확률값이 들어있는 행렬(matrix)의
크기를 확인해 보면 알 수 있습니다

0:36:09.720,0:36:13.020
그리고, 이 결과를
아까 보신 형태의

0:36:13.160,0:36:15.620
포맷으로 만들어 내야 합니다

0:36:15.980,0:36:18.975
그러기 위한 가장 쉬운 방법은
pandas 를 사용하는 것입니다

0:36:18.975,0:36:21.865
아직 pandas를 잘 모르신다면,
온라인에 많은 자료가 있고

0:36:21.865,0:36:24.845
저희가 운영하는 머신러닝 코스에서도

0:36:24.845,0:36:27.855
pandas를 많이 사용하니까 확인해 보시기 바랍니다

0:36:27.860,0:36:32.460
간단하게 말씀드리면, 
pd.DataFrame 에 아까의 확률 결과 행렬을 넣어주고

0:36:32.460,0:36:37.040
행들의 이름이 분류 카테고리들 이라고
(ds.column = data.classes) 알려줍니다

0:36:37.500,0:36:41.100
그리고, id를 첫번째 행의 이름으로 삽입하고

0:36:41.100,0:36:45.220
id 행의 열값들을 파일이름으로 채워줍니다

0:36:45.220,0:36:49.100
이때, 파일이름의 시작부분에는

0:36:49.420,0:36:52.980
필요 없는 다섯 글자(test/)가 포함되어 있어서

0:36:53.040,0:36:55.080
이 부분을 제외한

0:36:55.080,0:36:59.080
파일이름의 부분만 잘라냈습니다

0:37:00.940,0:37:03.360
여기까지 하면,
결과 DataFrame은

0:37:03.400,0:37:05.580
화면에 보시는것 처럼 구성 됩니다

0:37:05.900,0:37:09.820
정확히 우리가 원하는 형태 입니다

0:37:09.820,0:37:12.260
그러면, Dataframe 객체의

0:37:12.260,0:37:19.300
아.. 변수이름을 잘못 입력 했네요
ds가 아니라 df로 변경하겠습니다

0:37:19.300,0:37:23.060
DataFrame 이니까 df 으로 말이죠

0:37:24.380,0:37:29.140
이 시점에서 df.to_csv 메소드를
호출할 수 있는데

0:37:29.520,0:37:32.395
이 결과 파일의 크기가 꽤나 크다는 것을
종종 눈치채실 겁니다

0:37:32.400,0:37:35.860
그래서, 결과를 gzip으로 
압축하는게 좋은 생각일 수 있습니다

0:37:35.860,0:37:39.000
그러면 자동으로 결과가
압축되어 저장되게 됩니다

0:37:39.000,0:37:42.760
결과적으로 to_csv 메소드는
CSV 파일을 생성하는데, 이때 CSV 파일을

0:37:42.760,0:37:46.200
압축해서, Jupyter Notebook이 구동중인
곳에다 저장을 해 줍니다

0:37:46.440,0:37:49.240
서버에서 구동중이시면
Kaggle에 제출하기 위해서

0:37:49.245,0:37:52.465
이 파일을 내 컴퓨터로 옮겨와야 하거나

0:37:52.465,0:37:56.880
kg submit 형태의 
Kaggle CLI를 사용해도 됩니다

0:37:57.120,0:38:00.780
저는 보통 내 컴퓨터로 옮겨오는 편입니다

0:38:00.785,0:38:04.005
결과가 올바른지 다시한번
확인해 보기 위해서 말이죠

0:38:04.175,0:38:07.355
파일을 다운로드 하기 위한
손수윈 방법을 소개해 드리겠습니다

0:38:07.355,0:38:10.845
FileLink 라는 것을
아까 압축한 파일의 경로로 생성하면

0:38:10.995,0:38:14.395
그 파일의 경로에 대한 링크를 생성해 주는데
이 링크를 클릭하게되면

0:38:14.400,0:38:19.000
그 파일을 서버로부터
내 컴퓨터로 다운로드 할 수 있습니다

0:38:19.560,0:38:22.420
한번 클릭해 보겠습니다

0:38:22.640,0:38:25.120
그리고 어딘가에 저장을 하고

0:38:25.360,0:38:33.720
저장된 곳에 가보면,

0:38:34.040,0:38:40.380
제출용 파일이 다운로드된 것을
확인할 수 있습니다

0:38:40.680,0:38:43.420
이 파일을 열어보면

0:38:43.820,0:38:46.920
정확히 제가 요청한 형태로

0:38:46.925,0:38:49.995
id 와 120개 서로 다른 개 품종 이름이 있습니다

0:38:50.120,0:38:52.980
이것은 데이터의 첫번째 줄인데,
파일이름이 있고

0:38:52.980,0:38:55.280
120 개의 서로다른 확률값이 저장되어 있습니다

0:38:55.800,0:39:01.580
파일이 다운로드 되면, 이것을
Kaggle 에 제출할 수 있습니다

0:39:02.500,0:39:06.300
지금까지 한 것을 보면,
인터넷에서 어떤 파일이든

0:39:06.580,0:39:11.280
크롬 확장 프로그램을 사용해서

0:39:11.500,0:39:14.580
AWS나 Paperspace 서버로 집어 넣거나

0:39:14.580,0:39:16.655
할 수 있습니다.

0:39:16.655,0:39:19.585
또한, FileLink 라는 것을 사용해서
우리의 서버로부터

0:39:19.585,0:39:22.525
파일을 손수비게 다운로드 할 수 있는걸 배웠습니다.

0:39:22.645,0:39:25.845
좀더 터미널 커맨드 사용에 익숙하신 분은
SCP 명령을 사용 할 수도 있습니다

0:39:25.845,0:39:29.105
저는 모든걸 Notebook에서 하는걸
좀 더 선호하는 편입니다

0:39:30.245,0:39:31.245


0:39:32.965,0:39:35.845
지난주에 한가지 질문 받은게 있는데

0:39:35.845,0:39:38.875
단 한장의 이미지 파일에 대해서

0:39:39.275,0:39:42.295
결과를 예측하고 싶은 경우

0:39:42.525,0:39:45.215
어떻게 하는지에 대한 것입니다.

0:39:45.215,0:39:47.805
예를들어서,
검증 데이터셋의 첫번째 파일을

0:39:47.805,0:39:50.625
가져옵니다.
파일 이름을 출력해 봤고요

0:39:50.940,0:39:54.240
그리고, 언제든지
기본적인 파이썬 라이브러리인

0:39:54.500,0:39:59.340
Image.open()을 사용해서
이미지를 열어볼 수 있습니다

0:40:00.480,0:40:01.480


0:40:02.485,0:40:05.255
그리고 뭘 할 수 있는지에 대해서

0:40:05.255,0:40:07.555
가장 짧은 코드로 설명 드리겠습니다

0:40:07.560,0:40:11.080
단순히 learn.predict_array를 호출 하면 되는데

0:40:11.220,0:40:15.940
이때, 파라메터로 해당 이미지를
전달해주면 됩니다

0:40:17.560,0:40:20.505
전달되는 이미지는
변형될 필요가 있습니다.

0:40:20.505,0:40:25.420
전에 tfms_from_model()을 보신 적이 있을 건데

0:40:25.420,0:40:29.800
전에 이 함수는 하나의 값만을 리턴 하는 형태로
사용 됐었습니다

0:40:29.805,0:40:32.515
하지만, 사실상 내부적으로는
두 가지 값인

0:40:32.515,0:40:35.535
trn_tfms 하고, val_tfms 을 리턴합니다.
(학습용, 검증용)

0:40:35.535,0:40:38.555
즉, 두 종류의 데이터셋에 대해서
분할이 가능한 겁니다

0:40:38.560,0:40:42.540
여기 코드에서는
학습용 데이터 변형이 적용 되었는데,

0:40:42.540,0:40:45.940
사실은 검증용 데이터 변형이
적용 되어야 겠지요

0:40:46.080,0:40:47.560
이 함수가 호출되면

0:40:47.760,0:40:50.720
변형된 이미지들이 포함된

0:40:50.725,0:40:54.135
배열을 되돌려 주게 됩니다

0:40:54.645,0:40:57.085
생성된 모델에 전달되거나

0:40:57.085,0:41:00.115
얻어진 결과든 모든 것들은

0:41:00.115,0:41:02.895
보통 미니 배치 형태라고
가정됩니다

0:41:02.895,0:41:06.025
쉽게 말하면 여러장의 이미지들이라는 것이죠

0:41:06.025,0:41:09.500
numpy 사용법에 대해선 나중에 좀 더
이야기 할 것인데요,

0:41:10.080,0:41:13.035
여기서 우리는 단 한장의 이미지만
있는 상황 이라서

0:41:13.035,0:41:15.885
이 한장의 이미지를
미니 배치 형태로 바꿔줘야 합니다

0:41:15.885,0:41:18.775
다른말로 표현하자면,
Tensor 라는 형태를 생성해야 한다는 겁니다

0:41:18.780,0:41:22.360
한장의 이미지는 행x열x채널 로 이루어 지지만

0:41:22.400,0:41:26.940
미니배치의 형태는
이미지 갯수x행x열x채널이 되는 거죠

0:41:27.660,0:41:30.700
한장의 이미지지만
4차원의 형태로 표현해야 합니다

0:41:30.705,0:41:33.715
numpy에 이를 손쉽게 하는 방법이 있는데

0:41:33.720,0:41:37.760
배열의 인덱스에 None을 넣어주면

0:41:38.100,0:41:41.595
첫 부분에, 추가적인 차원을 추가해 줍니다

0:41:41.595,0:41:44.635
하나의 이미지 형태를

0:41:44.635,0:41:47.585
하나의 이미가 들어간 미니배치 형태가 되는거죠

0:41:47.585,0:41:50.305
단 한장의 이미지에 대해서

0:41:50.660,0:41:53.300
PyTorch나 Fastai를 사용해서

0:41:53.300,0:41:55.220
뭔가를 하려고 하실때,

0:41:55.480,0:41:58.540
마주칠 수 있는 상황으로

0:41:58.545,0:42:01.625
4차원의 입력이 요구되는데

0:42:01.625,0:42:04.625
주어진건 3차원인 경우가 있을 수 있습니다

0:42:05.165,0:42:08.145
그리고 배열의 첫번째 요소가

0:42:08.145,0:42:11.135
뭔가 이상한 값을 가진다면

0:42:11.140,0:42:15.880
그 배열은 미니배치인 경우가 있을 겁니다
(초보자들 한테 해주는 말인듯)

0:42:18.580,0:42:25.600
지금까지 보여드린 코드가 실전에서
사용해야 하는 모든것을 말해줍니다

0:42:26.560,0:42:27.560


0:42:27.700,0:42:31.060
그러면 이번에는 약간의 이론을
공부해볼 차례 입니다

0:42:31.060,0:42:33.880
컨볼루션 뉴럴넷의 내부에서

0:42:33.885,0:42:36.705
실제로 무슨일이 벌어지는지에 대해서 말입니다

0:42:36.705,0:42:37.700


0:42:38.520,0:42:43.220
레슨1에서의 내용을 기억하실지도 모르겠군요

0:42:44.400,0:42:46.320
그때

0:42:47.300,0:42:50.020
약간의 이론적인 내용을

0:42:50.380,0:42:53.735
여기 보시는 멋진 웹사이트 (http://setosa.io/ev) 로
설명 드렸었습니다

0:42:53.735,0:42:56.945
시각적으로 뭔가를 설명해주는
웹사이트 입니다

0:42:57.055,0:43:00.065
우리가 배운 내용은
컨볼루션이란 기본적으로

0:43:00.065,0:43:05.680
딥러닝에서 거의 항상 3x3 크기를 가지는
행렬(Matrix)가 있을때

0:43:05.920,0:43:09.560
이 행렬의 각 요소들과
사진의 3x3 영역의 각 요소들을

0:43:09.560,0:43:12.255
서로 곱하고

0:43:12.260,0:43:14.740
각 곱해진 값들을 모두 더하면

0:43:14.740,0:43:19.020
결과 사진(오른쪽)의 특정 한 지점(픽셀)
의 값을 얻게 되는 것입니다

0:43:19.520,0:43:23.300
이번에는 이 모든 것들이 함께 어떻게

0:43:23.680,0:43:27.840
여기 보시는 Zeiler와 Burgers의 논문에 나온 다양한
계층들로 바뀌게 되는지

0:43:27.840,0:43:30.200
살펴보도록 하겠습니다

0:43:30.435,0:43:32.795
그러기 위해서
이번에도

0:43:32.795,0:43:35.785
저보다 똑똑한 누군가의 자료를 활용하도록 하겠습니다

0:43:35.785,0:43:39.420
이 자료는 Otavia Good 이라는 사람이 작성한 것으로

0:43:39.420,0:43:41.535
이 분은 Word Lens라는걸

0:43:41.535,0:43:44.725
만드신 분인데, Word Lens는 현재

0:43:44.725,0:43:47.495
구글 번역기의 일 부분으로 들어가 있습니다

0:43:47.495,0:43:50.745
해보신 분도 있을텐데

0:43:50.745,0:43:55.040
카메라로 외국어 단어가 있는 곳을 가리키면

0:43:55.040,0:43:56.880
실시간으로

0:43:56.885,0:44:00.265
변역된 결과를 외국어 단어 위에 표시해 줍니다

0:44:00.755,0:44:03.825
그리고 Otavia가 지금부터 보실

0:44:03.825,0:44:07.820
멋진 비디오를 공유해 줬지요
그는 현재 구글에 있습니다

0:44:08.300,0:44:11.495
이 비디오를 여러분께 보여주고자 합니다

0:44:11.495,0:44:14.505
왜냐하면 무슨 일이 일어나는지를
아주 잘 설명해 주고 있기 때문인데요

0:44:14.505,0:44:17.575
비디오를 시청하신 후에

0:44:17.575,0:44:20.305
네트워크를 구성하는 
전체적인 컨볼루션 계층의 순서를

0:44:20.305,0:44:23.475
어떻게 구현하는지에 대해서
MS 엑셀을 이용해서 보여 드리겠습니다

0:44:23.475,0:44:26.085
시각적으로 공부하시는 분이나
스프레드시트로 공부하시는 분이나

0:44:26.085,0:44:29.385
모두 이 내용을 이해하실 수 있도록 말이죠

0:44:29.435,0:44:30.515


0:44:31.195,0:44:34.205
이 코스 나중에 하게 될 것 중 하나로

0:44:34.205,0:44:36.915
숫자를 인식하는 방법을 배우게 될 건데요

0:44:36.915,0:44:40.185
처음부터 끝까지 어떻게 하는지
모든 것을 배울 겁니다

0:44:40.455,0:44:43.465
이 비디오는 문자를 인식하는 경우에 대한 것인데

0:44:43.465,0:44:46.505
여기에 'A'가 있습니다
하지만 이 문자 'A"는 명백히

0:44:46.505,0:44:49.915
사실상 숫자들의 격자 입니다.

0:44:49.915,0:44:52.595
그리고, 지금 보고 계시는건

0:44:52.595,0:44:55.565
첫번째 컨볼루션 필터가
작업하는 것을 보여줍니다

0:44:55.565,0:44:58.175
여기서 보여주는 모든 내용은

0:44:58.180,0:45:00.860
모든것들이 이미 학습되어 있다고 가정합니다

0:45:00.860,0:45:04.460
필터를 이미지 위에서
오른쪽 아래 방향으로 닦아 내려갑니다

0:45:04.460,0:45:07.255
그러면서 왼쪽 부분을 검게 만드는데
여기서 사용된 커널의 값은 대략

0:45:07.255,0:45:09.835
-1 0 1
-1 0 1
-1 0 1
형태가 됩니다

0:45:09.835,0:45:12.575
이미지 각각의 3x3 부분에 대해서

0:45:12.580,0:45:16.460
방금 말씀드린 커널의 행렬과 값을 곱하게 됩니다

0:45:16.460,0:45:19.520
행렬의 곱셈이 아니라,
각 요소끼리의 곱셈 입니다 (element wise)

0:45:19.860,0:45:22.800
그래서 뭔 일이 일어났냐 하면

0:45:22.805,0:45:25.545
커널의 흰색 모서리가 'A'의 모서리와

0:45:25.545,0:45:27.105
맞아 들어 가지만

0:45:27.480,0:45:30.540
커널의 검은색 모서리는 맞아들어가지 않을때

0:45:30.540,0:45:33.115
양성반응인 초록색을 얻게 됩니다

0:45:33.115,0:45:36.055
그 반대가 되는 모든 부분에 대해서는
음성반응인 빨간색을 얻게 됩니다

0:45:36.060,0:45:39.700
방금 보신게 처음 하나의 필터에 대한 것이고

0:45:40.600,0:45:43.545
첫번째 커널의 결과를 얻은 것이었습니다

0:45:43.545,0:45:45.880
지금 보시는 것은 또다른 새로운 커널로

0:45:45.880,0:45:48.395
커널 윗 부분이 하얀색(1)입니다

0:45:48.395,0:45:51.085
'A' 문자를 표현하는 행렬의

0:45:51.085,0:45:53.965
모든 3x3 부분들을 스캔해서,

0:45:53.965,0:45:56.415
커널의 9개 비트와 곱하고,

0:45:56.415,0:45:59.085
빨간색과 초록색 부분들과

0:45:59.085,0:46:02.155
얼마나 그 색이 짙은지를
찾아내게 됩니다

0:46:02.155,0:46:05.135
그리고, 지금까지 2개의
필터가 있는데

0:46:05.135,0:46:08.225
하나는 아랫쪽, 다른 하나는 왼쪽
경계선에 대한 필터로

0:46:08.225,0:46:10.885
위 결과에서, 윗쪽 경계선이
빨간색이고

0:46:10.885,0:46:12.980
아랫쪽 경계선이
초록색입니다.

0:46:12.980,0:46:16.580
아래 결과에선, 왼쪽 경계선은 초록색
오른쪽 경계선은 빨간색입니다

0:46:16.760,0:46:18.920
이 다음 단계에선

0:46:18.920,0:46:22.055
ReLU (Rectified Linear Unit) 라는
'비선형성'이 추가 됩니다

0:46:22.055,0:46:25.545
이 함수는 음성값들을 모두 0으로 만드는데
보시다 시피 빨간색이 모두 사라졌습니다

0:46:25.995,0:46:28.885
다시 보면, 계층 1에서는
문자 'A'를 입력 받고

0:46:28.885,0:46:31.865
계층 2는 두 가지 컨볼루션 필터가
적용된 결과를 보여주고

0:46:31.865,0:46:34.340
계층 3에서는
모든 빨간색 부분을

0:46:34.340,0:46:37.735
없애 버린 결과를 보실 수 있습니다

0:46:37.735,0:46:40.815
ReLU이 적용된 결과이죠

0:46:40.815,0:46:43.815
계층 4는 max pull이라고 불리는 것인데

0:46:43.820,0:46:47.420
직전 계층의 각 2x2 픽셀 중
가장 큰 값을 대표로 뽑아서

0:46:47.520,0:46:50.880
2x2 행렬을 1x1로 대체하게 됩니다

0:46:50.980,0:46:55.420
결과적으론, 전체 크기가 절반으로
줄어들게 되는 것입니다

0:46:55.420,0:46:59.280
그 다음으로는
정확히 동일한 가정을 반복합니다

0:46:59.280,0:47:03.415
3x3 필터를 직전단계에서 구해진

0:47:03.415,0:47:06.725
두 개의 결과에 대해서 적용합니다

0:47:06.920,0:47:09.420
그리고 또 다시

0:47:09.420,0:47:12.000
빨간색 부분을 제거합니다.
ReLU를 사용해서

0:47:12.000,0:47:17.620
모든 음수값을 제거하고
양성 반응이 된 부분만을 남깁니다

0:47:17.620,0:47:18.320


0:47:18.785,0:47:21.245
이런 식으로 컨볼루션 뉴럴넷의

0:47:21.245,0:47:24.200
다음 계층으로 이동하게 됩니다

0:47:24.200,0:47:27.520
뒷쪽에 있는 계층(이전 컨볼루션 계층)을
보시면

0:47:27.545,0:47:30.535
아랫쪽, 왼쪽 경계선과 같이

0:47:30.535,0:47:33.585
꽤나 해석 가능한 수준 입니다.
하지만, 그 다음 계층에서는

0:47:33.585,0:47:36.645
컨볼루션의 결과들을 조합하기 때문에

0:47:36.645,0:47:39.635
직관적으로 무슨일이 일어나는지

0:47:39.635,0:47:41.885
훨씬 더 불분명해지기 시작합니다

0:47:42.205,0:47:44.975
그 다음으로 또 다른 max pool을 수행하여

0:47:44.980,0:47:47.260
모든 2x2나 3x3 부분들을

0:47:47.260,0:47:51.000
이들이 가지고 있는 가장 큰 값의 숫자를
1x1 형태로 표현하여 대체합니다

0:47:51.000,0:47:54.440
예를 들어서, 왼쪽 상단의 2x2는 모두
검은색 이라서, 1x1의 검은색으로 교체되는 거죠

0:47:54.440,0:47:57.920
그리곤, 이렇게 만들어진 결과를 가지고

0:47:57.920,0:48:02.180
'A', 'B', 'C', 'D', 'E' 일때 기대되는 
값들이 들어 있는

0:48:02.180,0:48:05.700
일종의 템플릿과 비교를 하게 되는데

0:48:06.020,0:48:08.860
얼마나 가깝게 각 템플릿과
들어맞는지를 보는 것입니다

0:48:08.995,0:48:12.055
이 비교 또한 완전히 똑같은
방식으로 할 수 있는데

0:48:12.060,0:48:15.540
여기 보이는 결과에 대한  
4x8 행렬의 모든 요소값을

0:48:15.540,0:48:18.380
각 템플릿에 대한 4x8 행렬의 
모든 요소값과

0:48:18.380,0:48:20.415
곱한 후, 각 템플릿마나 나온

0:48:20.415,0:48:23.565
곱셈 결과를 더해줍니다

0:48:23.565,0:48:26.800
그러면, 각각에 대해서 
얼마나 빈번하게 맞아들어가는지 vs

0:48:26.800,0:48:29.420
얼마나 빈번하게 맞아들어 가지 않는지를 
알 수 있는데

0:48:29.420,0:48:32.600
이 결과는 각 문자가 될 확률로

0:48:32.600,0:48:34.860
변환됩니다

0:48:34.860,0:48:38.660
이 예제에서는 'A'라는 템플릿과
아주 잘 맞아 들어 가는 결과인 것입니다

0:48:38.840,0:48:42.260
여기선 어떤 학습 과정도 하지 않았습니다

0:48:42.260,0:48:45.660
미리 학습된 모델이 있을때
동작하는 방식을 보여드린 것입니다

0:48:45.660,0:48:49.000
인터넷에서 ImageNet에 대하여 학습된
미리 학습된 모델을 다운로드 하고,

0:48:49.000,0:48:51.500
ImageNet의 입력 이미지에 어떠한
변형도 가하지 않거나

0:48:51.500,0:48:54.360
어떤 모델을 학습시킨 후

0:48:54.360,0:48:56.625
테스트 데이터셋이나 새로운 이미지에

0:48:56.625,0:48:59.635
적용시킬 때

0:48:59.635,0:49:02.435
지금까지 보신 것과 비슷한
과정이 수행된다고 볼 수 있습니다

0:49:02.435,0:49:05.535
기본적으로 입력 데이터를

0:49:05.535,0:49:08.535
하나의 컨볼루션 계층에 여러개의
컨볼루션 필터를

0:49:08.540,0:49:10.220
적용하고,

0:49:10.680,0:49:13.720
음성 반응부분을 버리기 위한
ReLU를 적용하고,

0:49:13.720,0:49:16.800
그 다음으로는 max pool을 수행 하는데

0:49:17.560,0:49:20.295
이 세 단계를 여러번 반복하게 됩니다.

0:49:20.295,0:49:23.265
그러면, 새로운 문자

0:49:23.265,0:49:25.875
'A'든 'B'든 무슨 문자든지 간에

0:49:25.875,0:49:28.765
이 과정을 적용시킬 수 있는 것입니다

0:49:28.765,0:49:29.765


0:49:30.535,0:49:33.605
보셨듯이
아주 멋진 시각화 자료로

0:49:33.605,0:49:36.685
저라면 이렇게 못 만들었을 것 같아서

0:49:36.685,0:49:40.065
Otavia에게 감사를 드리고 싶군요

0:49:40.115,0:49:42.955
사실 이 시각화는 수작업으로 만들어진게 아니라

0:49:42.955,0:49:45.845
실제로 동적으로 컨볼루션을

0:49:45.845,0:49:48.865
처리하기 위한 컴퓨터 소프트웨어를 제작한 후

0:49:48.865,0:49:50.815
이를 구동하여 만들어진 것입니다

0:49:51.555,0:49:54.565
저는 좀더 스프레드시트를 사용하는 사람인데요

0:49:54.565,0:49:57.605
여기 보시면

0:49:57.605,0:50:00.555
똑같은 것이 스프레드시트로 표현되어 있습니다

0:50:00.555,0:50:03.825
이 내용은 깃헙 저장소에서 찾으실 수 있는데요

0:50:03.945,0:50:07.035
해당 저장소를 복제(git clone) 한 후,
스프레드시트 파일을 열거나

0:50:07.035,0:50:09.865
github.com/fastai 로 가셔서

0:50:09.865,0:50:12.925
이 파일을 클릭하셔도 됩니다

0:50:12.925,0:50:18.280
정확히 어디에 파일이 있는지 보여드리겠습니다

0:50:18.600,0:50:22.000
https://github.com/fastai/fastai
저장소로 가셔서

0:50:22.220,0:50:24.835
courses 폴더로 들어 가시고

0:50:24.840,0:50:28.860
dl1 폴더를 선택합니다.
그러면 excel 폴더가 있는게 보일 겁니다

0:50:28.860,0:50:31.260
이 폴더 안에
모든 엑셀파일이 들어 있고

0:50:31.260,0:50:33.785
여기서 바로 다운 받거나
저장소를 복제해도 되는데

0:50:33.785,0:50:37.320
이번에 저희가 보게될 파일은
conv-example.xlsx 입니다

0:50:37.780,0:50:40.340
이 파일을 열어 보시면

0:50:40.340,0:50:42.080
좌측 상단에

0:50:42.505,0:50:45.845
Input이 있는데

0:50:45.915,0:50:48.755
이 예제 에서 사용된 Input은 숫자 7 입니다

0:50:48.755,0:50:51.905
이 데이터를 MNIST라고 불리는 
데이터셋에서 가져왔는데

0:50:51.905,0:50:54.985
나중에 상세한 내용을 좀 더 들여다 볼 것입니다

0:50:54.985,0:50:57.995
그 데이터 중, 무작위로 하나를 선택해서

0:50:57.995,0:51:01.315
엑셀에 넣은 것입니다.
보시면 아실텐데,  모든 픽셀 값의 범위가

0:51:01.485,0:51:04.065
0 부터 1 사이로 되어 있습니다

0:51:04.640,0:51:07.540
아주 빈번하게 이 범위는

0:51:07.740,0:51:11.360
1바이트로 표현 가능한 0 ~ 255 이거나

0:51:11.560,0:51:15.980
0~1 사이의 실수가 되는데,
뭐가 되었든지 딱히 문제가 될 것은 없습니다

0:51:16.580,0:51:19.075
PyTorch에 대해서 배울때쯤

0:51:19.075,0:51:22.035
일반적으로, 이 문제를 실수(float)로
다루게 되기 때문에

0:51:22.035,0:51:24.315
문제 해결의 단계 중 하나로서

0:51:24.320,0:51:28.140
주어진 숫자들을 0~1 사이의 값으로
변환하는 일을 하게 됩니다

0:51:28.760,0:51:31.520
여기서 저는 엑셀의 조건문을 사용해서

0:51:31.525,0:51:34.405
높은 값을 가진 셀을
더 빨갛게 색칠하도록 만들었습니다

0:51:34.405,0:51:38.320
그 결과 숫자 7을 명확히 보실 수 있을 겁니다

0:51:38.780,0:51:41.400
하지만 여기엔 
많은 숫자들이 단순히 엑셀로

0:51:41.405,0:51:44.455
임포트되어 있는 것입니다

0:51:44.460,0:51:46.460
그래서 이 숫자들이 입력의 행렬 입니다.

0:51:47.320,0:51:51.120
Otavia가 앞에서 뭘 했는지 상기해 보면

0:51:51.440,0:51:56.920
이 입력에 2개의 필터를 적용했었습니다.

0:51:57.260,0:52:00.195
그리고, 엑셀 시트의 중간 부분에 보시면

0:52:00.200,0:52:02.900
상단 경계선을 감지해내는 3x3 필터 하나를

0:52:02.900,0:52:04.260
생성해 뒀습니다

0:52:04.455,0:52:07.405
필터의 상단 부분은 1,

0:52:07.405,0:52:10.445
중간 부분은 0, 그리고 하단 부분은 -1의 
값으로 설정하였습니다

0:52:10.445,0:52:13.920
이 필터가 적용된 결과인 오른쪽의
값들 중 하나를 예로서 살펴 봅시다.

0:52:13.920,0:52:16.255
아무 값을 선택하고
F2 키를 누르면

0:52:16.260,0:52:19.540
왼쪽에서 하이라이트되는 부분이 보이실 텐데

0:52:19.540,0:52:24.140
입력 데이터 중 선택된 값이 계산된
대상 3x3 부분입니다

0:52:24.580,0:52:27.360
그 3x3 부분의 값들을 살펴보면
상단에 1.0, 1.0, 1.0 값들은 각각

0:52:27.360,0:52:29.720
필터 상단의 값들인 1, 1, 1과 곱해집니다

0:52:30.180,0:52:32.280
그리고 하단의 0.1, 0.0, 0.0 값들은 각각

0:52:32.540,0:52:35.220
필터 하단의 값들인 -1, -1, -1과 곱해집니다

0:52:35.705,0:52:38.995
이걸 다른 말로 표현하면
양수795

0:52:38.995,0:52:41.855


0:52:41.855,0:52:43.765


0:52:44.545,0:52:47.275


0:52:47.275,0:52:48.545


0:52:48.985,0:52:50.885


0:52:51.675,0:52:54.895


0:52:54.895,0:52:57.675


0:53:00.455,0:53:01.455


0:53:01.455,0:53:04.585


0:53:04.585,0:53:07.535


0:53:07.535,0:53:08.535


0:53:08.985,0:53:11.555


0:53:12.115,0:53:13.255


0:53:13.745,0:53:16.495


0:53:17.765,0:53:20.445


0:53:21.295,0:53:23.515


0:53:24.135,0:53:25.535


0:53:25.925,0:53:26.955


0:53:27.415,0:53:29.065


0:53:29.525,0:53:30.525


0:53:31.765,0:53:34.235


0:53:34.575,0:53:37.575


0:53:38.025,0:53:40.245


0:53:40.895,0:53:44.005


0:53:44.005,0:53:47.325


0:53:47.335,0:53:50.285


0:53:50.955,0:53:53.965


0:53:53.965,0:53:57.205


0:53:57.635,0:53:59.205


0:53:59.775,0:54:01.415


0:54:01.825,0:54:03.345


0:54:03.735,0:54:06.805


0:54:06.805,0:54:08.065


0:54:08.855,0:54:12.095


0:54:13.355,0:54:15.965


0:54:15.965,0:54:18.945


0:54:19.155,0:54:20.155


0:54:20.535,0:54:23.565


0:54:23.565,0:54:26.625


0:54:26.625,0:54:29.815


0:54:29.815,0:54:32.795


0:54:32.795,0:54:35.505


0:54:35.505,0:54:38.505


0:54:38.505,0:54:41.585


0:54:41.585,0:54:44.925


0:54:44.925,0:54:47.955


0:54:48.295,0:54:50.345


0:54:50.935,0:54:53.495


0:54:54.605,0:54:57.365


0:54:57.365,0:54:58.935


0:55:00.825,0:55:04.215


0:55:04.845,0:55:07.705


0:55:07.705,0:55:11.165


0:55:11.855,0:55:14.915


0:55:14.915,0:55:17.915


0:55:17.915,0:55:21.145


0:55:21.145,0:55:22.415


0:55:23.575,0:55:26.645


0:55:26.645,0:55:27.645


0:55:28.025,0:55:31.175


0:55:31.175,0:55:34.125


0:55:34.125,0:55:37.515


0:55:37.585,0:55:40.315


0:55:40.315,0:55:43.145


0:55:43.445,0:55:46.505


0:55:46.505,0:55:49.665


0:55:49.665,0:55:52.645


0:55:52.645,0:55:55.655


0:55:55.655,0:55:58.525


0:55:58.525,0:55:59.745


0:56:00.055,0:56:03.075


0:56:03.075,0:56:05.755


0:56:06.635,0:56:07.635


0:56:07.735,0:56:10.295


0:56:10.295,0:56:13.145


0:56:13.145,0:56:16.635


0:56:16.635,0:56:18.635


0:56:19.035,0:56:22.155


0:56:22.155,0:56:25.295


0:56:25.365,0:56:28.385


0:56:28.385,0:56:31.345


0:56:31.685,0:56:33.355


0:56:34.495,0:56:37.315


0:56:38.305,0:56:41.635


0:56:41.645,0:56:44.915


0:56:44.915,0:56:47.855


0:56:47.855,0:56:51.025


0:56:51.375,0:56:54.705


0:56:56.085,0:56:59.235


0:56:59.235,0:57:00.745


0:57:01.875,0:57:04.785


0:57:04.785,0:57:05.785


0:57:06.805,0:57:09.005


0:57:09.615,0:57:10.805


0:57:11.375,0:57:12.545


0:57:13.235,0:57:16.275


0:57:16.275,0:57:19.425


0:57:19.425,0:57:22.215


0:57:22.445,0:57:24.675


0:57:25.305,0:57:28.525


0:57:28.525,0:57:31.485


0:57:31.485,0:57:34.155


0:57:34.155,0:57:37.425


0:57:37.425,0:57:40.405


0:57:40.405,0:57:43.665


0:57:43.715,0:57:46.865


0:57:46.865,0:57:49.915


0:57:49.915,0:57:52.125


0:57:52.575,0:57:55.165


0:57:55.845,0:57:58.865


0:57:58.865,0:57:59.865


0:58:00.535,0:58:03.515


0:58:03.515,0:58:05.805


0:58:06.845,0:58:08.425


0:58:09.815,0:58:11.145


0:58:12.615,0:58:13.915


0:58:14.915,0:58:15.915


0:58:16.495,0:58:19.625


0:58:19.625,0:58:22.915


0:58:23.205,0:58:24.495


0:58:25.115,0:58:28.195


0:58:28.195,0:58:31.475


0:58:31.665,0:58:34.665


0:58:34.665,0:58:37.845


0:58:37.845,0:58:39.505


0:58:40.535,0:58:43.185


0:58:43.185,0:58:44.755


0:58:45.295,0:58:48.125


0:58:48.125,0:58:51.205


0:58:51.205,0:58:54.195


0:58:54.195,0:58:57.065


0:58:57.065,0:58:59.695


0:58:59.695,0:59:03.095


0:59:03.265,0:59:06.635


0:59:06.985,0:59:10.135


0:59:10.145,0:59:12.315


0:59:13.515,0:59:16.715


0:59:16.725,0:59:19.845


0:59:19.845,0:59:22.765


0:59:22.765,0:59:25.725


0:59:25.725,0:59:28.645


0:59:28.645,0:59:31.815


0:59:31.815,0:59:34.765


0:59:34.765,0:59:37.805


0:59:38.325,0:59:39.325


0:59:39.425,0:59:42.375


0:59:42.375,0:59:45.525


0:59:46.155,0:59:49.105


0:59:49.445,0:59:52.375


0:59:52.375,0:59:55.155


0:59:55.155,0:59:58.065


0:59:58.065,1:00:00.685


1:00:00.685,1:00:03.835


1:00:03.835,1:00:07.015


1:00:07.015,1:00:10.435


1:00:10.435,1:00:13.545


1:00:13.545,1:00:16.415


1:00:16.415,1:00:17.415


1:00:17.625,1:00:19.165


1:00:19.835,1:00:22.845


1:00:22.845,1:00:26.105


1:00:26.105,1:00:29.355


1:00:29.435,1:00:32.345


1:00:32.345,1:00:33.345


1:00:33.595,1:00:36.715


1:00:36.715,1:00:38.975


1:00:39.475,1:00:42.645


1:00:42.645,1:00:44.555


1:00:45.015,1:00:47.955


1:00:47.955,1:00:50.995


1:00:51.035,1:00:54.065


1:00:54.065,1:00:56.835


1:00:56.835,1:00:59.955


1:00:59.955,1:01:03.115


1:01:03.125,1:01:05.285


1:01:06.815,1:01:08.135


1:01:08.595,1:01:11.845


1:01:11.845,1:01:14.845


1:01:14.845,1:01:17.835


1:01:17.835,1:01:19.305


1:01:20.455,1:01:23.885


1:01:24.155,1:01:27.065


1:01:27.065,1:01:30.035


1:01:30.035,1:01:33.485


1:01:34.065,1:01:35.935


1:01:36.605,1:01:39.695


1:01:39.695,1:01:42.785


1:01:42.785,1:01:45.325


1:01:45.325,1:01:48.005


1:01:48.005,1:01:51.345


1:01:51.345,1:01:53.445


1:01:55.695,1:01:58.575


1:01:58.575,1:02:01.615


1:02:01.615,1:02:04.415


1:02:04.415,1:02:06.675


1:02:07.215,1:02:08.205


1:02:08.205,1:02:11.615


1:02:11.895,1:02:14.715


1:02:14.715,1:02:17.695


1:02:17.695,1:02:19.515


1:02:19.925,1:02:23.065


1:02:23.065,1:02:25.865


1:02:25.865,1:02:27.805


1:02:28.325,1:02:30.675


1:02:31.075,1:02:34.215


1:02:34.215,1:02:37.315


1:02:37.655,1:02:40.645


1:02:40.645,1:02:44.085


1:02:44.095,1:02:46.205


1:02:48.955,1:02:49.955


1:02:50.655,1:02:53.435


1:02:53.435,1:02:55.275


1:02:56.145,1:02:59.245


1:02:59.245,1:03:02.235


1:03:02.235,1:03:05.375


1:03:05.375,1:03:07.375


1:03:07.955,1:03:10.975


1:03:10.975,1:03:13.355


1:03:13.355,1:03:16.515


1:03:16.515,1:03:19.985


1:03:19.985,1:03:21.295


1:03:21.605,1:03:24.295


1:03:25.105,1:03:28.075


1:03:28.075,1:03:29.315


1:03:30.115,1:03:32.945


1:03:32.945,1:03:35.885


1:03:35.885,1:03:38.935


1:03:40.435,1:03:43.835


1:03:43.925,1:03:46.965


1:03:46.965,1:03:49.535


1:03:49.535,1:03:52.685


1:03:52.685,1:03:55.925


1:03:55.925,1:03:58.985


1:03:58.985,1:04:01.875


1:04:02.275,1:04:05.395


1:04:05.765,1:04:08.275


1:04:08.275,1:04:10.955


1:04:10.955,1:04:13.885


1:04:13.885,1:04:15.135


1:04:15.445,1:04:18.685


1:04:18.685,1:04:19.685


1:04:19.865,1:04:22.725


1:04:22.725,1:04:25.775


1:04:25.775,1:04:28.725


1:04:28.725,1:04:31.645


1:04:31.645,1:04:34.065


1:04:34.065,1:04:36.955


1:04:36.955,1:04:39.965


1:04:39.965,1:04:42.945


1:04:42.945,1:04:45.895


1:04:45.895,1:04:48.925


1:04:49.315,1:04:52.335


1:04:52.335,1:04:55.405


1:04:55.405,1:04:58.495


1:04:59.095,1:05:01.585


1:05:02.295,1:05:05.345


1:05:05.345,1:05:08.345


1:05:08.345,1:05:11.245


1:05:11.245,1:05:14.385


1:05:14.385,1:05:17.325


1:05:17.325,1:05:20.365


1:05:20.365,1:05:23.445


1:05:23.445,1:05:26.515


1:05:26.515,1:05:29.395


1:05:29.395,1:05:31.695


1:05:31.695,1:05:34.595


1:05:34.595,1:05:36.475


1:05:37.325,1:05:40.265


1:05:40.265,1:05:43.185


1:05:43.185,1:05:46.105


1:05:46.105,1:05:48.775


1:05:48.775,1:05:51.955


1:05:51.955,1:05:54.705


1:05:54.705,1:05:57.055


1:05:57.055,1:06:00.035


1:06:00.035,1:06:01.035


1:06:01.465,1:06:04.135


1:06:04.135,1:06:06.995


1:06:06.995,1:06:09.795


1:06:09.795,1:06:13.245


1:06:13.615,1:06:16.705


1:06:16.705,1:06:20.175


1:06:20.405,1:06:21.405


1:06:21.455,1:06:24.025


1:06:24.025,1:06:26.765


1:06:26.765,1:06:28.105


1:06:28.575,1:06:30.405


1:06:31.005,1:06:33.795


1:06:33.795,1:06:36.535


1:06:36.535,1:06:39.295


1:06:39.575,1:06:41.735


1:06:41.735,1:06:45.235


1:06:45.245,1:06:48.015


1:06:48.015,1:06:51.035


1:06:51.035,1:06:52.035


1:06:52.835,1:06:55.345


1:06:55.345,1:06:58.145


1:06:58.145,1:07:01.055


1:07:01.055,1:07:04.045


1:07:04.045,1:07:07.545


1:07:07.685,1:07:10.745


1:07:10.745,1:07:13.665


1:07:13.805,1:07:16.595


1:07:16.595,1:07:19.715


1:07:19.715,1:07:22.735


1:07:22.735,1:07:25.655


1:07:25.655,1:07:28.365


1:07:28.365,1:07:30.105


1:07:30.425,1:07:33.395


1:07:33.395,1:07:35.275


1:07:36.775,1:07:39.175


1:07:39.495,1:07:42.805


1:07:42.805,1:07:45.835


1:07:45.835,1:07:48.855


1:07:48.855,1:07:51.835


1:07:52.395,1:07:53.895


1:07:54.325,1:07:56.455


1:07:57.125,1:07:58.125


1:07:58.435,1:08:01.295


1:08:01.295,1:08:03.745


1:08:03.745,1:08:07.095


1:08:07.095,1:08:10.215


1:08:10.215,1:08:13.305


1:08:13.305,1:08:15.995


1:08:15.995,1:08:19.295


1:08:19.295,1:08:22.375


1:08:22.375,1:08:25.505


1:08:25.505,1:08:28.465


1:08:28.465,1:08:30.855


1:08:30.855,1:08:33.855


1:08:33.855,1:08:35.675


1:08:36.635,1:08:39.645


1:08:39.645,1:08:42.215


1:08:46.935,1:08:50.335


1:08:51.095,1:08:53.875


1:08:55.195,1:08:56.695


1:08:57.135,1:08:59.465


1:08:59.945,1:09:02.905


1:09:02.905,1:09:06.295


1:09:06.375,1:09:07.985


1:09:08.335,1:09:11.115


1:09:11.115,1:09:13.105


1:09:13.715,1:09:17.045


1:09:17.045,1:09:20.135


1:09:20.145,1:09:23.045


1:09:24.395,1:09:25.505


1:09:26.205,1:09:27.205


1:09:27.375,1:09:28.375


1:09:28.435,1:09:31.475


1:09:31.475,1:09:32.775


1:09:33.225,1:09:35.735


1:09:35.735,1:09:37.455


1:09:38.165,1:09:39.165


1:09:39.675,1:09:42.685


1:09:42.685,1:09:43.685


1:09:44.065,1:09:46.885


1:09:46.885,1:09:50.115


1:09:50.115,1:09:52.165


1:09:52.635,1:09:55.785


1:09:55.785,1:09:58.835


1:09:58.835,1:10:01.715


1:10:01.715,1:10:04.295


1:10:04.295,1:10:07.385


1:10:07.385,1:10:08.765


1:10:09.475,1:10:12.535


1:10:12.535,1:10:15.675


1:10:15.675,1:10:19.135


1:10:19.135,1:10:22.165


1:10:22.165,1:10:25.015


1:10:25.015,1:10:28.045


1:10:28.045,1:10:29.045


1:10:29.615,1:10:32.995


1:10:32.995,1:10:33.995


1:10:34.135,1:10:35.715


1:10:36.095,1:10:37.095


1:10:37.415,1:10:40.575


1:10:40.575,1:10:43.385


1:10:43.875,1:10:44.985


1:10:45.655,1:10:48.045


1:10:48.455,1:10:51.535


1:10:51.535,1:10:54.335


1:10:54.505,1:10:57.995


1:10:58.445,1:11:01.535


1:11:01.535,1:11:03.915


1:11:03.915,1:11:06.895


1:11:06.895,1:11:09.865


1:11:10.065,1:11:12.965


1:11:12.965,1:11:15.955


1:11:15.955,1:11:19.065


1:11:19.065,1:11:22.155


1:11:22.155,1:11:25.565


1:11:25.925,1:11:26.925


1:11:27.045,1:11:30.005


1:11:30.145,1:11:33.135


1:11:33.135,1:11:35.795


1:11:35.795,1:11:37.895


1:11:38.405,1:11:41.715


1:11:42.155,1:11:45.195


1:11:45.195,1:11:47.895


1:11:48.815,1:11:51.815


1:11:51.815,1:11:55.105


1:11:55.475,1:11:58.455


1:11:58.455,1:11:59.785


1:12:00.255,1:12:02.845


1:12:02.845,1:12:05.515


1:12:05.515,1:12:08.855


1:12:08.855,1:12:11.345


1:12:11.895,1:12:15.065


1:12:15.065,1:12:17.325


1:12:17.845,1:12:20.965


1:12:21.355,1:12:24.725


1:12:24.725,1:12:27.555


1:12:27.555,1:12:30.565


1:12:30.565,1:12:31.575


1:12:33.605,1:12:35.875


1:12:36.475,1:12:39.585


1:12:39.585,1:12:40.825


1:12:41.185,1:12:42.755


1:12:46.185,1:12:48.825


1:12:48.825,1:12:50.155


1:12:50.905,1:12:51.905


1:12:52.255,1:12:55.315


1:12:55.315,1:12:58.365


1:12:58.365,1:13:01.495


1:13:01.495,1:13:02.495


1:13:02.905,1:13:05.415


1:13:06.175,1:13:07.175


1:13:07.425,1:13:10.585


1:13:10.585,1:13:11.705


1:13:12.845,1:13:15.915


1:13:16.275,1:13:19.405


1:13:19.435,1:13:22.705


1:13:22.705,1:13:25.305


1:13:25.895,1:13:29.015


1:13:29.995,1:13:33.105


1:13:33.105,1:13:36.315


1:13:36.315,1:13:39.595


1:13:39.595,1:13:42.185


1:13:43.175,1:13:46.305


1:13:46.925,1:13:49.565


1:13:49.835,1:13:52.955


1:13:52.955,1:13:55.625


1:13:56.105,1:13:59.125


1:13:59.125,1:14:02.265


1:14:02.365,1:14:05.275


1:14:05.275,1:14:07.285


1:14:07.985,1:14:10.895


1:14:10.895,1:14:13.865


1:14:13.865,1:14:17.275


1:14:17.355,1:14:20.075


1:14:20.075,1:14:23.075


1:14:23.075,1:14:25.625


1:14:26.135,1:14:29.025


1:14:29.025,1:14:30.505


1:14:31.215,1:14:34.135


1:14:34.585,1:14:37.895


1:14:38.405,1:14:39.405


1:14:40.525,1:14:43.615


1:14:43.735,1:14:47.105


1:14:47.125,1:14:50.055


1:14:50.055,1:14:52.205


1:14:52.665,1:14:55.605


1:14:55.605,1:14:58.545


1:14:58.545,1:15:01.605


1:15:01.605,1:15:04.915


1:15:05.275,1:15:08.245


1:15:08.245,1:15:11.625


1:15:12.145,1:15:15.295


1:15:15.545,1:15:18.575


1:15:18.575,1:15:21.685


1:15:21.685,1:15:24.265


1:15:24.265,1:15:27.325


1:15:27.325,1:15:30.265


1:15:30.265,1:15:32.585


1:15:32.895,1:15:35.845


1:15:36.305,1:15:39.165


1:15:39.165,1:15:40.165


1:15:40.355,1:15:43.855


1:15:43.895,1:15:46.895


1:15:46.895,1:15:49.875


1:15:49.875,1:15:52.535


1:15:52.945,1:15:55.165


1:15:55.505,1:15:57.315


1:15:58.455,1:16:01.505


1:16:01.505,1:16:03.975


1:16:04.225,1:16:06.855


1:16:06.855,1:16:09.835


1:16:09.835,1:16:12.865


1:16:13.795,1:16:15.725


1:16:18.445,1:16:21.185


1:16:21.955,1:16:23.485


1:16:24.005,1:16:26.895


1:16:26.895,1:16:27.925


1:16:28.235,1:16:29.235


1:16:29.425,1:16:32.455


1:16:32.655,1:16:35.985


1:16:36.055,1:16:38.955


1:16:38.955,1:16:41.515


1:16:41.515,1:16:43.235


1:16:43.795,1:16:45.135


1:16:45.705,1:16:46.705


1:16:46.885,1:16:49.285


1:16:49.465,1:16:52.455


1:16:52.455,1:16:54.385


1:16:55.285,1:16:56.285


1:16:56.805,1:16:59.725


1:16:59.725,1:17:01.985


1:17:02.415,1:17:05.435


1:17:05.435,1:17:08.615


1:17:08.615,1:17:11.815


1:17:11.815,1:17:12.815


1:17:13.885,1:17:16.925


1:17:16.925,1:17:17.925


1:17:19.135,1:17:22.125


1:17:22.125,1:17:25.085


1:17:25.085,1:17:27.745


1:17:28.185,1:17:31.145


1:17:31.145,1:17:33.285


1:17:33.285,1:17:36.335


1:17:36.335,1:17:37.805


1:17:39.025,1:17:40.155


1:17:40.755,1:17:41.755


1:17:41.785,1:17:44.845


1:17:44.845,1:17:47.485


1:17:48.305,1:17:51.335


1:17:51.335,1:17:52.995


1:17:53.775,1:17:54.745


1:17:54.745,1:17:57.795


1:17:57.795,1:18:00.915


1:18:00.915,1:18:03.885


1:18:03.885,1:18:06.445


1:18:06.445,1:18:07.675


1:18:08.635,1:18:09.635


1:18:10.315,1:18:13.275


1:18:13.275,1:18:16.135


1:18:16.855,1:18:17.855


1:18:19.685,1:18:20.685


1:18:22.565,1:18:24.405


1:18:27.155,1:18:29.845


1:18:30.925,1:18:31.925


1:18:33.295,1:18:36.245


1:18:36.245,1:18:39.725


1:18:40.485,1:18:42.805


1:18:43.175,1:18:46.535


1:18:46.535,1:18:47.535


1:18:49.685,1:18:52.585


1:18:52.955,1:18:55.735


1:18:56.245,1:18:59.245


1:18:59.245,1:19:01.755


1:19:03.045,1:19:05.485


1:19:06.215,1:19:09.215


1:19:09.215,1:19:11.565


1:19:12.285,1:19:14.935


1:19:14.935,1:19:18.245


1:19:18.245,1:19:21.105


1:19:21.105,1:19:24.225


1:19:24.225,1:19:27.545


1:19:28.075,1:19:31.285


1:19:31.485,1:19:34.385


1:19:34.385,1:19:36.685


1:19:37.165,1:19:40.055


1:19:40.055,1:19:42.035


1:19:43.745,1:19:46.395


1:19:46.395,1:19:48.245


1:19:49.505,1:19:51.255


1:19:51.865,1:19:54.575


1:19:54.575,1:19:57.515


1:19:57.515,1:20:00.775


1:20:00.835,1:20:03.805


1:20:03.805,1:20:06.725


1:20:06.725,1:20:09.795


1:20:09.795,1:20:12.985


1:20:12.985,1:20:16.335


1:20:16.685,1:20:18.415


1:20:18.955,1:20:19.955


1:20:20.185,1:20:22.985


1:20:22.985,1:20:24.015


1:20:26.245,1:20:27.275


1:20:27.685,1:20:30.645


1:20:30.645,1:20:33.565


1:20:33.565,1:20:36.625


1:20:36.625,1:20:38.955


1:20:39.365,1:20:42.505


1:20:43.425,1:20:44.425


1:20:45.215,1:20:48.585


1:20:48.585,1:20:51.265


1:20:51.265,1:20:54.215


1:20:54.215,1:20:57.225


1:20:57.225,1:20:59.855


1:20:59.855,1:21:01.615


1:21:02.335,1:21:05.505


1:21:05.505,1:21:06.505


1:21:07.565,1:21:10.545


1:21:10.545,1:21:13.845


1:21:13.875,1:21:16.225


1:21:16.775,1:21:18.895


1:21:19.275,1:21:22.375


1:21:23.135,1:21:26.105


1:21:26.105,1:21:29.065


1:21:29.065,1:21:31.975


1:21:31.975,1:21:35.415


1:21:36.215,1:21:39.125


1:21:39.125,1:21:42.145


1:21:42.145,1:21:45.595


1:21:45.905,1:21:48.675


1:21:48.675,1:21:51.925


1:21:52.075,1:21:55.005


1:21:55.005,1:21:58.065


1:21:58.065,1:22:00.735


1:22:00.735,1:22:03.595


1:22:03.595,1:22:05.795


1:22:06.215,1:22:08.485


1:22:08.485,1:22:11.435


1:22:11.735,1:22:14.695


1:22:14.695,1:22:18.045


1:22:18.715,1:22:22.115


1:22:22.115,1:22:24.895


1:22:24.895,1:22:27.625


1:22:27.625,1:22:30.345


1:22:30.345,1:22:33.025


1:22:33.265,1:22:36.205


1:22:36.205,1:22:39.125


1:22:39.125,1:22:41.185


1:22:41.495,1:22:44.485


1:22:44.485,1:22:47.915


1:22:48.155,1:22:50.495


1:22:51.315,1:22:54.015


1:22:54.015,1:22:55.575


1:22:56.175,1:22:57.175


1:22:57.215,1:23:00.175


1:23:00.175,1:23:02.105


1:23:02.445,1:23:05.455


1:23:05.515,1:23:08.595


1:23:08.595,1:23:09.595


1:23:10.005,1:23:12.885


1:23:12.885,1:23:15.725


1:23:15.725,1:23:18.185


1:23:18.775,1:23:21.765


1:23:21.765,1:23:24.995


1:23:24.995,1:23:28.095


1:23:28.095,1:23:31.175


1:23:31.175,1:23:32.175


1:23:32.525,1:23:33.525


1:23:35.315,1:23:38.475


1:23:38.815,1:23:41.855


1:23:41.855,1:23:43.905


1:23:43.905,1:23:46.885


1:23:46.885,1:23:50.065


1:23:50.065,1:23:52.815


1:23:53.235,1:23:56.215


1:23:56.215,1:23:59.055


1:23:59.055,1:24:00.055


1:24:00.425,1:24:01.855


1:24:02.445,1:24:04.265


1:24:07.505,1:24:08.975


1:24:10.945,1:24:14.015


1:24:14.015,1:24:16.845


1:24:16.845,1:24:19.915


1:24:19.915,1:24:22.845


1:24:23.085,1:24:25.985


1:24:27.025,1:24:29.635


1:24:29.635,1:24:32.785


1:24:32.785,1:24:35.755


1:24:35.755,1:24:38.545


1:24:38.545,1:24:41.555


1:24:41.555,1:24:44.595


1:24:44.595,1:24:45.825


1:24:46.525,1:24:49.575


1:24:49.575,1:24:52.785


1:24:52.785,1:24:55.875


1:24:55.875,1:24:57.945


1:24:58.295,1:25:01.295


1:25:01.295,1:25:03.675


1:25:04.125,1:25:06.885


1:25:06.885,1:25:09.055


1:25:09.055,1:25:12.205


1:25:12.205,1:25:15.185


1:25:15.185,1:25:17.915


1:25:18.255,1:25:21.125


1:25:21.125,1:25:23.925


1:25:23.925,1:25:27.275


1:25:27.625,1:25:28.925


1:25:28.925,1:25:31.545


1:25:31.935,1:25:34.955


1:25:34.955,1:25:37.805


1:25:37.805,1:25:40.755


1:25:40.755,1:25:43.615


1:25:43.745,1:25:46.785


1:25:47.185,1:25:50.155


1:25:50.155,1:25:52.935


1:25:52.935,1:25:53.935


1:25:54.255,1:25:56.855


1:25:56.855,1:25:59.905


1:25:59.905,1:26:02.385


1:26:03.195,1:26:05.895


1:26:05.895,1:26:08.885


1:26:08.885,1:26:11.895


1:26:11.895,1:26:14.595


1:26:14.595,1:26:17.555


1:26:17.885,1:26:21.045


1:26:21.045,1:26:24.055


1:26:24.055,1:26:27.075


1:26:27.075,1:26:30.085


1:26:30.085,1:26:33.045


1:26:33.045,1:26:35.875


1:26:35.875,1:26:38.615


1:26:38.615,1:26:41.575


1:26:41.575,1:26:44.545


1:26:44.545,1:26:45.545


1:26:46.155,1:26:48.395


1:26:48.915,1:26:52.015


1:26:52.015,1:26:55.015


1:26:55.015,1:26:57.635


1:26:57.975,1:27:00.975


1:27:00.975,1:27:04.015


1:27:04.015,1:27:07.215


1:27:07.215,1:27:10.245


1:27:10.245,1:27:13.305


1:27:13.305,1:27:16.315


1:27:16.315,1:27:17.315


1:27:17.625,1:27:18.625


1:27:19.555,1:27:22.725


1:27:22.725,1:27:25.375


1:27:25.375,1:27:28.335


1:27:28.335,1:27:31.055


1:27:31.055,1:27:34.205


1:27:34.365,1:27:37.235


1:27:37.505,1:27:40.255


1:27:40.255,1:27:42.905


1:27:42.905,1:27:45.915


1:27:45.915,1:27:49.075


1:27:49.075,1:27:51.905


1:27:51.905,1:27:55.145


1:27:56.115,1:27:59.135


1:27:59.135,1:28:02.245


1:28:02.245,1:28:05.105


1:28:05.105,1:28:08.255


1:28:08.255,1:28:11.335


1:28:11.335,1:28:12.525


1:28:12.885,1:28:15.635


1:28:15.635,1:28:16.645


1:28:17.205,1:28:20.375


1:28:20.375,1:28:23.395


1:28:23.595,1:28:26.935


1:28:27.025,1:28:30.055


1:28:30.055,1:28:31.655


1:28:32.035,1:28:34.885


1:28:34.885,1:28:37.885


1:28:37.885,1:28:38.885


1:28:40.845,1:28:43.775


1:28:43.775,1:28:46.635


1:28:46.635,1:28:49.395


1:28:49.395,1:28:50.525


1:28:50.945,1:28:53.825


1:28:53.825,1:28:56.565


1:28:56.565,1:28:59.565


1:28:59.565,1:29:02.525


1:29:02.525,1:29:03.525


1:29:03.735,1:29:05.775


1:29:06.865,1:29:10.235


1:29:10.235,1:29:11.255


1:29:11.845,1:29:14.975


1:29:15.305,1:29:18.425


1:29:18.425,1:29:21.655


1:29:21.655,1:29:24.685


1:29:24.685,1:29:27.865


1:29:27.865,1:29:31.175


1:29:31.175,1:29:34.465


1:29:34.485,1:29:37.445


1:29:37.445,1:29:40.305


1:29:40.305,1:29:43.275


1:29:43.275,1:29:44.945


1:29:45.265,1:29:48.165


1:29:48.165,1:29:51.265


1:29:51.475,1:29:54.475


1:29:55.165,1:29:58.265


1:29:58.265,1:30:01.285


1:30:01.285,1:30:04.495


1:30:04.965,1:30:08.105


1:30:08.105,1:30:10.155


1:30:10.715,1:30:13.815


1:30:13.815,1:30:17.085


1:30:17.085,1:30:20.025


1:30:20.585,1:30:22.035


1:30:23.555,1:30:24.575


1:30:24.885,1:30:27.025


1:30:28.095,1:30:31.485


1:30:31.795,1:30:34.205


1:30:34.205,1:30:37.345


1:30:37.345,1:30:40.015


1:30:40.015,1:30:42.965


1:30:42.965,1:30:45.985


1:30:46.565,1:30:49.645


1:30:49.645,1:30:52.575


1:30:52.575,1:30:55.555


1:30:55.555,1:30:56.555


1:30:56.595,1:30:59.615


1:30:59.615,1:31:02.085


1:31:02.905,1:31:05.745


1:31:05.745,1:31:08.885


1:31:08.885,1:31:11.875


1:31:11.875,1:31:14.155


1:31:14.155,1:31:16.915


1:31:16.915,1:31:19.555


1:31:21.645,1:31:22.645


1:31:22.875,1:31:24.505


1:31:24.855,1:31:27.915


1:31:27.915,1:31:30.945


1:31:30.945,1:31:32.375


1:31:32.745,1:31:34.335


1:31:34.855,1:31:37.985


1:31:37.985,1:31:41.415


1:31:41.595,1:31:44.805


1:31:45.025,1:31:48.285


1:31:49.395,1:31:52.755


1:31:52.945,1:31:55.855


1:31:55.855,1:31:58.075


1:31:58.075,1:32:01.025


1:32:01.025,1:32:03.965


1:32:03.965,1:32:05.685


1:32:06.005,1:32:09.115


1:32:09.115,1:32:10.115


1:32:10.315,1:32:11.405


1:32:12.075,1:32:15.165


1:32:15.165,1:32:17.975


1:32:17.975,1:32:20.915


1:32:20.915,1:32:23.915


1:32:23.915,1:32:26.935


1:32:26.935,1:32:29.885


1:32:29.885,1:32:32.965


1:32:32.965,1:32:35.845


1:32:35.845,1:32:38.745


1:32:38.745,1:32:41.965


1:32:42.005,1:32:44.885


1:32:44.885,1:32:47.995


1:32:47.995,1:32:51.165


1:32:51.165,1:32:53.195


1:32:54.725,1:32:57.695


1:32:57.695,1:32:59.075


1:32:59.075,1:33:01.695


1:33:02.255,1:33:05.055


1:33:05.055,1:33:06.605


1:33:07.455,1:33:08.455


1:33:09.895,1:33:10.895


1:33:11.885,1:33:14.805


1:33:15.125,1:33:16.645


1:33:16.975,1:33:19.385


1:33:19.935,1:33:21.675


1:33:22.165,1:33:25.345


1:33:25.345,1:33:28.335


1:33:28.335,1:33:31.605


1:33:36.525,1:33:37.525


1:33:38.605,1:33:39.605


1:33:39.845,1:33:42.665


1:33:42.755,1:33:45.855


1:33:45.855,1:33:48.875


1:33:48.875,1:33:52.375


1:33:52.385,1:33:55.795


1:33:55.815,1:33:59.205


1:33:59.535,1:34:02.705


1:34:02.705,1:34:05.875


1:34:05.875,1:34:06.875


1:34:07.015,1:34:10.355


1:34:10.355,1:34:13.355


1:34:13.355,1:34:16.395


1:34:16.395,1:34:19.415


1:34:19.415,1:34:22.535


1:34:22.535,1:34:23.625


1:34:24.335,1:34:27.425


1:34:27.425,1:34:30.495


1:34:30.495,1:34:33.525


1:34:33.525,1:34:35.655


1:34:36.255,1:34:38.815


1:34:39.705,1:34:42.695


1:34:42.695,1:34:45.705


1:34:45.705,1:34:48.495


1:34:48.495,1:34:49.955


1:34:50.545,1:34:53.465


1:34:53.465,1:34:56.495


1:34:56.495,1:34:57.635


1:34:59.605,1:35:02.655


1:35:02.655,1:35:05.645


1:35:05.645,1:35:08.605


1:35:08.605,1:35:11.755


1:35:11.755,1:35:14.565


1:35:14.565,1:35:17.635


1:35:17.635,1:35:20.805


1:35:20.805,1:35:23.885


1:35:23.885,1:35:26.935


1:35:26.935,1:35:27.935


1:35:28.775,1:35:31.555


1:35:31.555,1:35:34.545


1:35:34.545,1:35:37.505


1:35:37.505,1:35:40.465


1:35:40.465,1:35:42.385


1:35:43.005,1:35:46.085


1:35:46.705,1:35:49.725


1:35:49.725,1:35:52.725


1:35:52.725,1:35:55.615


1:35:55.615,1:35:58.715


1:35:58.715,1:36:01.885


1:36:01.885,1:36:04.745


1:36:04.745,1:36:07.735


1:36:07.735,1:36:10.525


1:36:10.785,1:36:13.495


1:36:13.495,1:36:16.595


1:36:16.595,1:36:19.205


1:36:19.205,1:36:21.405


1:36:21.935,1:36:22.935


1:36:23.345,1:36:26.425


1:36:26.425,1:36:29.425


1:36:29.535,1:36:32.405


1:36:32.405,1:36:33.735


1:36:34.355,1:36:37.305


1:36:37.305,1:36:40.525


1:36:40.535,1:36:43.485


1:36:43.485,1:36:45.875


1:36:46.515,1:36:49.495


1:36:49.495,1:36:52.815


1:36:53.105,1:36:54.105


1:36:54.175,1:36:57.655


1:36:57.865,1:37:01.095


1:37:01.215,1:37:04.315


1:37:04.315,1:37:07.075


1:37:07.075,1:37:09.345


1:37:10.645,1:37:12.095


1:37:12.815,1:37:15.915


1:37:15.915,1:37:17.695


1:37:18.555,1:37:19.555


1:37:19.675,1:37:22.685


1:37:22.685,1:37:24.515


1:37:24.825,1:37:27.475


1:37:27.475,1:37:30.425


1:37:30.425,1:37:33.425


1:37:33.425,1:37:35.105


1:37:35.685,1:37:38.145


1:37:38.495,1:37:41.095


1:37:41.095,1:37:44.275


1:37:44.275,1:37:47.005


1:37:47.005,1:37:49.945


1:37:50.305,1:37:52.845


1:37:53.385,1:37:56.495


1:37:56.495,1:37:59.355


1:37:59.355,1:38:02.805


1:38:03.825,1:38:06.875


1:38:07.415,1:38:10.235


1:38:10.235,1:38:13.445


1:38:13.445,1:38:14.825


1:38:15.305,1:38:18.575


1:38:18.625,1:38:21.625


1:38:21.625,1:38:24.725


1:38:24.725,1:38:26.965


1:38:26.965,1:38:29.925


1:38:29.925,1:38:32.535


1:38:32.535,1:38:35.875


1:38:35.875,1:38:38.765


1:38:38.765,1:38:41.785


1:38:41.785,1:38:43.615


1:38:44.275,1:38:45.825


1:38:46.215,1:38:48.905


1:38:48.905,1:38:49.905


1:38:50.475,1:38:51.475


1:38:51.755,1:38:54.115


1:38:54.595,1:38:56.165


1:38:56.885,1:38:59.745


1:38:59.745,1:39:02.325


1:39:03.195,1:39:04.195


1:39:05.065,1:39:08.185


1:39:08.185,1:39:11.415


1:39:11.415,1:39:14.455


1:39:14.455,1:39:17.225


1:39:17.225,1:39:19.645


1:39:21.115,1:39:24.215


1:39:24.215,1:39:25.415


1:39:25.945,1:39:28.885


1:39:28.885,1:39:32.015


1:39:32.015,1:39:34.075


1:39:35.695,1:39:38.755


1:39:38.755,1:39:41.925


1:39:41.925,1:39:44.945


1:39:44.995,1:39:47.815


1:39:48.265,1:39:50.985


1:39:50.985,1:39:53.805


1:39:53.805,1:39:56.705


1:39:56.705,1:39:59.115


1:39:59.115,1:40:00.555


1:40:00.885,1:40:04.015


1:40:04.015,1:40:06.635


1:40:06.975,1:40:09.765


1:40:09.765,1:40:12.725


1:40:12.725,1:40:15.955


1:40:15.955,1:40:18.605


1:40:18.605,1:40:21.455


1:40:21.545,1:40:23.805


1:40:24.125,1:40:27.175


1:40:27.175,1:40:30.045


1:40:30.045,1:40:31.465


1:40:32.155,1:40:35.035


1:40:35.035,1:40:37.985


1:40:37.985,1:40:40.895


1:40:40.895,1:40:43.515


1:40:43.515,1:40:46.665


1:40:47.335,1:40:50.445


1:40:50.445,1:40:51.965


1:40:52.605,1:40:55.465


1:40:57.125,1:40:58.125


1:40:58.645,1:41:01.965


1:41:02.485,1:41:05.025


1:41:05.765,1:41:07.465


1:41:07.885,1:41:10.835


1:41:10.835,1:41:12.645


1:41:13.525,1:41:16.425


1:41:16.425,1:41:17.545


1:41:18.065,1:41:20.695


1:41:20.695,1:41:23.775


1:41:23.775,1:41:26.665


1:41:26.665,1:41:29.755


1:41:29.755,1:41:31.115


1:41:31.685,1:41:32.685


1:41:33.485,1:41:36.275


1:41:36.275,1:41:39.345


1:41:39.345,1:41:42.705


1:41:42.785,1:41:43.785


1:41:45.605,1:41:48.605


1:41:48.605,1:41:51.545


1:41:51.545,1:41:54.715


1:41:54.715,1:41:57.945


1:41:58.215,1:42:01.155


1:42:01.155,1:42:02.045


1:42:02.045,1:42:05.065


1:42:05.065,1:42:08.195


1:42:08.735,1:42:11.795


1:42:11.795,1:42:14.665


1:42:14.665,1:42:17.385


1:42:17.385,1:42:20.395


1:42:20.395,1:42:22.955


1:42:23.445,1:42:25.235


1:42:25.975,1:42:27.205


1:42:27.865,1:42:30.625


1:42:30.625,1:42:32.875


1:42:32.875,1:42:35.815


1:42:36.015,1:42:39.095


1:42:39.205,1:42:42.245


1:42:43.085,1:42:44.815


1:42:46.085,1:42:49.125


1:42:49.125,1:42:50.125


1:42:50.335,1:42:53.355


1:42:53.675,1:42:55.235


1:42:55.925,1:42:58.685


1:42:59.115,1:43:02.175


1:43:02.175,1:43:05.315


1:43:05.575,1:43:08.255


1:43:08.255,1:43:09.695


1:43:11.135,1:43:14.335


1:43:16.575,1:43:19.605


1:43:19.605,1:43:22.055


1:43:22.615,1:43:24.035


1:43:24.495,1:43:27.615


1:43:27.615,1:43:30.735


1:43:30.735,1:43:32.045


1:43:32.395,1:43:35.255


1:43:35.545,1:43:39.015


1:43:39.365,1:43:42.575


1:43:42.575,1:43:43.575


1:43:44.185,1:43:45.185


1:43:45.685,1:43:46.685


1:43:47.135,1:43:48.135


1:43:50.255,1:43:53.275


1:43:53.275,1:43:56.365


1:43:56.365,1:43:58.085


1:43:58.425,1:44:01.395


1:44:01.395,1:44:04.565


1:44:04.885,1:44:07.825


1:44:08.085,1:44:10.285


1:44:10.775,1:44:11.775


1:44:11.915,1:44:13.245


1:44:13.985,1:44:16.955


1:44:16.955,1:44:19.815


1:44:19.815,1:44:21.705


1:44:22.455,1:44:25.455


1:44:25.455,1:44:28.565


1:44:28.565,1:44:31.115


1:44:31.185,1:44:34.175


1:44:34.175,1:44:37.225


1:44:37.225,1:44:40.255


1:44:40.255,1:44:43.315


1:44:43.315,1:44:44.515


1:44:45.345,1:44:47.865


1:44:48.345,1:44:50.615


1:44:50.615,1:44:53.695


1:44:53.695,1:44:56.725


1:44:57.475,1:45:00.655


1:45:00.655,1:45:03.825


1:45:03.825,1:45:06.935


1:45:06.935,1:45:10.015


1:45:10.115,1:45:12.915


1:45:12.915,1:45:15.325


1:45:15.865,1:45:18.975


1:45:18.975,1:45:19.955


1:45:19.955,1:45:23.215


1:45:23.625,1:45:24.625


1:45:24.645,1:45:27.645


1:45:27.645,1:45:30.885


1:45:30.885,1:45:33.785


1:45:33.785,1:45:37.005


1:45:37.145,1:45:39.555


1:45:39.555,1:45:41.995


1:45:42.985,1:45:45.935


1:45:45.935,1:45:47.495


1:45:48.865,1:45:51.915


1:45:51.915,1:45:55.245


1:45:55.505,1:45:56.595


1:45:57.615,1:45:59.255


1:46:00.395,1:46:01.395


1:46:03.125,1:46:04.575


1:46:05.275,1:46:08.225


1:46:08.225,1:46:09.435


1:46:09.775,1:46:11.715


1:46:12.155,1:46:13.155


1:46:13.445,1:46:16.475


1:46:16.475,1:46:17.825


1:46:18.435,1:46:21.455


1:46:21.455,1:46:24.875


1:46:24.875,1:46:27.725


1:46:27.725,1:46:30.715


1:46:30.715,1:46:31.715


1:46:31.895,1:46:32.895


1:46:32.975,1:46:34.085


1:46:36.205,1:46:37.625


1:46:38.245,1:46:41.085


1:46:41.085,1:46:42.965


1:46:45.935,1:46:48.465


1:46:49.715,1:46:50.715


1:46:51.645,1:46:54.515


1:46:54.785,1:46:58.155


1:46:58.525,1:47:00.165


1:47:00.735,1:47:04.135


1:47:04.335,1:47:07.435


1:47:07.435,1:47:10.235


1:47:10.355,1:47:12.725


1:47:13.525,1:47:16.495


1:47:16.495,1:47:19.525


1:47:19.525,1:47:22.905


1:47:22.945,1:47:25.805


1:47:30.285,1:47:33.295


1:47:33.295,1:47:36.205


1:47:36.205,1:47:39.275


1:47:39.745,1:47:42.745


1:47:42.745,1:47:45.725


1:47:45.725,1:47:48.575


1:47:48.705,1:47:51.825


1:47:52.015,1:47:55.145


1:47:55.145,1:47:57.795


1:47:57.795,1:48:00.685


1:48:00.685,1:48:03.695


1:48:03.695,1:48:05.505


1:48:07.025,1:48:09.075


1:48:09.075,1:48:11.965


1:48:12.075,1:48:14.895


1:48:14.895,1:48:17.695


1:48:17.695,1:48:20.235


1:48:20.685,1:48:22.225


1:48:22.865,1:48:25.855


1:48:25.855,1:48:29.185


1:48:29.335,1:48:32.225


1:48:32.225,1:48:35.275


1:48:35.275,1:48:38.095


1:48:38.295,1:48:41.285


1:48:41.285,1:48:44.385


1:48:44.655,1:48:47.785


1:48:48.195,1:48:51.375


1:48:51.375,1:48:54.415


1:48:54.415,1:48:57.105


1:48:57.365,1:49:00.375


1:49:00.375,1:49:03.165


1:49:03.375,1:49:06.435


1:49:06.435,1:49:09.165


1:49:09.165,1:49:10.165


1:49:10.595,1:49:13.625


1:49:13.675,1:49:14.675


1:49:15.635,1:49:19.105


1:49:19.135,1:49:21.675


1:49:21.995,1:49:24.395


1:49:24.395,1:49:27.435


1:49:27.855,1:49:29.905


1:49:30.815,1:49:31.815


1:49:32.305,1:49:34.895


1:49:34.895,1:49:35.895


1:49:36.315,1:49:39.425


1:49:39.425,1:49:42.285


1:49:42.285,1:49:45.645


1:49:46.095,1:49:49.105


1:49:49.285,1:49:50.775


1:49:51.415,1:49:52.705


1:49:53.635,1:49:56.645


1:49:56.645,1:49:59.285


1:49:59.285,1:50:01.775


1:50:01.775,1:50:04.765


1:50:04.765,1:50:07.735


1:50:07.805,1:50:10.995


1:50:10.995,1:50:14.045


1:50:14.045,1:50:17.035


1:50:17.035,1:50:20.315


1:50:20.495,1:50:23.265


1:50:23.265,1:50:24.265


1:50:24.845,1:50:27.945


1:50:27.945,1:50:30.965


1:50:30.965,1:50:33.805


1:50:33.975,1:50:36.985


1:50:36.985,1:50:39.935


1:50:39.935,1:50:43.025


1:50:43.025,1:50:44.955


1:50:45.675,1:50:48.645


1:50:48.645,1:50:51.645


1:50:51.725,1:50:54.785


1:50:54.785,1:50:57.825


1:50:57.825,1:50:59.155


1:50:59.835,1:51:03.185


1:51:03.185,1:51:06.205


1:51:06.205,1:51:09.165


1:51:09.165,1:51:10.315


1:51:11.515,1:51:14.535


1:51:14.535,1:51:15.535


1:51:16.955,1:51:19.965


1:51:19.965,1:51:22.975


1:51:22.975,1:51:25.765


1:51:25.765,1:51:29.225


1:51:29.265,1:51:31.665


1:51:31.665,1:51:34.685


1:51:34.685,1:51:36.485


1:51:36.935,1:51:39.355


1:51:41.005,1:51:44.375


1:51:44.625,1:51:46.985


1:51:46.985,1:51:49.495


1:51:49.495,1:51:52.385


1:51:52.385,1:51:53.755


1:51:54.155,1:51:57.195


1:51:57.195,1:52:00.295


1:52:00.295,1:52:03.145


1:52:03.145,1:52:04.565


1:52:05.355,1:52:07.295


1:52:08.825,1:52:10.345


1:52:10.765,1:52:11.765


1:52:12.845,1:52:13.845


1:52:14.425,1:52:16.945


1:52:17.255,1:52:20.295


1:52:20.295,1:52:23.095


1:52:23.095,1:52:26.185


1:52:26.185,1:52:29.115


1:52:29.115,1:52:32.195


1:52:32.195,1:52:33.195


1:52:33.665,1:52:36.485


1:52:36.485,1:52:39.545


1:52:39.545,1:52:42.105


1:52:42.105,1:52:45.375


1:52:46.055,1:52:49.115


1:52:49.115,1:52:52.265


1:52:52.265,1:52:53.825


1:52:54.375,1:52:57.405


1:52:57.405,1:52:59.505


1:52:59.505,1:53:01.955


1:53:01.955,1:53:05.315


1:53:05.445,1:53:08.365


1:53:08.365,1:53:11.335


1:53:11.335,1:53:14.055


1:53:14.055,1:53:16.745


1:53:16.745,1:53:18.675


1:53:19.055,1:53:22.165


1:53:22.165,1:53:25.125


1:53:25.125,1:53:27.685


1:53:28.075,1:53:31.425


1:53:32.115,1:53:35.185


1:53:35.185,1:53:37.215


1:53:37.585,1:53:38.585


1:53:38.585,1:53:41.685


1:53:41.685,1:53:45.105


1:53:45.195,1:53:48.145


1:53:48.145,1:53:50.865


1:53:50.865,1:53:54.235


1:53:54.235,1:53:57.025


1:53:57.025,1:54:00.095


1:54:00.095,1:54:02.615


1:54:02.615,1:54:05.655


1:54:05.655,1:54:07.955


1:54:07.955,1:54:11.115


1:54:11.535,1:54:14.875


1:54:14.875,1:54:17.825


1:54:17.825,1:54:20.145


1:54:20.145,1:54:23.185


1:54:23.185,1:54:26.285


1:54:26.285,1:54:29.205


1:54:29.205,1:54:31.995


1:54:31.995,1:54:35.125


1:54:35.125,1:54:36.125


1:54:36.535,1:54:39.225


1:54:39.225,1:54:40.245


1:54:41.415,1:54:42.855


1:54:43.515,1:54:46.285


1:54:46.285,1:54:49.385


1:54:49.385,1:54:52.365


1:54:52.365,1:54:55.425


1:54:55.425,1:54:56.335


1:54:56.335,1:54:59.295


1:55:02.625,1:55:03.725


1:55:04.115,1:55:07.045


1:55:07.045,1:55:08.365


1:55:10.045,1:55:12.665


1:55:13.895,1:55:16.645


1:55:16.645,1:55:19.205


1:55:19.205,1:55:21.705


1:55:21.705,1:55:24.725


1:55:24.725,1:55:27.805


1:55:27.805,1:55:30.795


1:55:30.795,1:55:32.505


1:55:35.915,1:55:38.955


1:55:38.955,1:55:40.945


1:55:40.945,1:55:44.005


1:55:44.235,1:55:46.765


1:55:46.765,1:55:49.805


1:55:49.805,1:55:52.725


1:55:52.725,1:55:55.775


1:55:55.775,1:55:58.935


1:55:58.935,1:56:01.825


1:56:01.825,1:56:04.875


1:56:04.875,1:56:07.845


1:56:07.845,1:56:10.885


1:56:10.885,1:56:13.825


1:56:14.005,1:56:16.845


1:56:16.845,1:56:19.765


1:56:19.765,1:56:22.545


1:56:22.545,1:56:24.845


1:56:24.845,1:56:27.695


1:56:28.005,1:56:31.175


1:56:31.175,1:56:33.955


1:56:33.955,1:56:36.055


1:56:36.155,1:56:38.625


1:56:38.625,1:56:41.575


1:56:41.575,1:56:44.605


1:56:44.605,1:56:47.935


1:56:48.175,1:56:49.175


1:56:50.615,1:56:53.685


1:56:53.685,1:56:57.025


1:56:58.015,1:57:00.025


1:57:01.765,1:57:03.325


1:57:04.085,1:57:06.195


1:57:06.765,1:57:09.805


1:57:09.805,1:57:11.575


1:57:12.035,1:57:12.945


1:57:12.945,1:57:16.005


1:57:16.005,1:57:19.445


1:57:19.555,1:57:20.555


1:57:21.365,1:57:24.365


1:57:24.365,1:57:27.375


1:57:27.375,1:57:30.115


1:57:30.115,1:57:31.995


1:57:32.335,1:57:35.375


1:57:35.375,1:57:38.205


1:57:38.205,1:57:41.415


1:57:41.415,1:57:42.995


1:57:43.415,1:57:46.235


1:57:46.235,1:57:49.635


1:57:49.635,1:57:52.705


1:57:52.705,1:57:55.715


1:57:56.585,1:57:59.585


1:57:59.585,1:58:02.595


1:58:02.595,1:58:05.285


1:58:05.285,1:58:07.225


1:58:07.225,1:58:10.405


1:58:10.405,1:58:12.515


1:58:12.995,1:58:15.945


1:58:15.945,1:58:19.245


1:58:19.325,1:58:22.475


1:58:22.475,1:58:25.645


1:58:25.645,1:58:29.135


1:58:29.255,1:58:32.075


1:58:32.465,1:58:35.215


1:58:35.215,1:58:37.885


1:58:37.885,1:58:40.645


1:58:40.645,1:58:43.565


1:58:43.565,1:58:45.085


1:58:45.555,1:58:46.555


1:58:46.865,1:58:48.255


1:58:50.355,1:58:52.775


1:58:53.425,1:58:56.765


1:58:56.955,1:58:59.345


1:58:59.345,1:59:01.955


1:59:01.955,1:59:05.105


1:59:05.105,1:59:08.295


1:59:08.295,1:59:11.225


1:59:11.225,1:59:14.245


1:59:14.245,1:59:17.375


1:59:17.375,1:59:20.405


1:59:20.405,1:59:23.335


1:59:23.335,1:59:26.505


1:59:26.505,1:59:29.405


1:59:29.405,1:59:32.575


1:59:32.575,1:59:34.585


1:59:38.685,1:59:41.415


1:59:41.415,1:59:42.415


1:59:42.675,1:59:45.405


1:59:45.405,1:59:48.485


1:59:48.485,1:59:51.405


1:59:51.405,1:59:54.195


1:59:55.405,1:59:56.405


1:59:58.165,2:00:00.935


2:00:00.935,2:00:02.125


2:00:02.435,2:00:05.215


2:00:05.545,2:00:08.785


2:00:08.785,2:00:11.735


2:00:11.895,2:00:13.445


2:00:13.855,2:00:15.915


2:00:15.915,2:00:18.865


2:00:18.865,2:00:21.585


2:00:21.585,2:00:22.985


2:00:23.435,2:00:25.625


2:00:26.105,2:00:29.145


2:00:29.235,2:00:31.745


2:00:32.825,2:00:34.375


2:00:34.795,2:00:38.105


2:00:38.255,2:00:41.285


2:00:41.285,2:00:42.735


2:00:43.045,2:00:45.675


2:00:45.675,2:00:48.915


2:00:48.915,2:00:51.915


2:00:51.955,2:00:54.915


2:00:55.625,2:00:58.555


2:00:58.555,2:01:01.195


2:01:01.195,2:01:04.425


2:01:04.425,2:01:07.315


2:01:07.435,2:01:10.725


2:01:10.725,2:01:13.775


2:01:13.775,2:01:16.855


2:01:16.855,2:01:19.975


2:01:19.975,2:01:21.835


2:01:22.295,2:01:24.815


2:01:24.815,2:01:27.765


2:01:27.765,2:01:30.085


2:01:31.545,2:01:34.395


2:01:34.395,2:01:37.365


2:01:37.365,2:01:40.245


2:01:40.245,2:01:43.135


2:01:43.135,2:01:46.175


2:01:46.175,2:01:49.235


2:01:49.235,2:01:52.185


2:01:52.185,2:01:54.455


2:01:54.855,2:01:57.665


2:01:57.865,2:01:59.085


2:01:59.555,2:02:02.625


2:02:02.625,2:02:05.565


2:02:05.565,2:02:08.625


2:02:08.625,2:02:11.455


2:02:11.455,2:02:14.465


2:02:14.465,2:02:17.455


2:02:17.455,2:02:19.295


2:02:20.115,2:02:21.365


2:02:22.195,2:02:25.285


2:02:25.285,2:02:27.005


2:02:27.005,2:02:29.375


2:02:29.375,2:02:32.275


2:02:32.275,2:02:35.435


2:02:35.435,2:02:38.675


2:02:38.675,2:02:41.355


2:02:44.055,2:02:46.775


2:02:47.535,2:02:49.925


2:02:50.275,2:02:52.895


2:02:52.895,2:02:55.905


2:02:55.905,2:02:58.885


2:02:58.885,2:02:59.885


2:03:00.535,2:03:03.565


2:03:03.565,2:03:06.515


2:03:06.715,2:03:08.185


2:03:09.545,2:03:12.495


2:03:12.495,2:03:15.465


2:03:15.465,2:03:18.715


2:03:19.015,2:03:22.195


2:03:22.195,2:03:25.055


2:03:25.055,2:03:27.625


2:03:28.035,2:03:30.775


2:03:31.115,2:03:32.115


2:03:32.305,2:03:35.655


2:03:35.825,2:03:38.455


2:03:38.455,2:03:41.465


2:03:41.465,2:03:44.405


2:03:44.405,2:03:47.445


2:03:47.445,2:03:50.465


2:03:50.465,2:03:53.555


2:03:53.555,2:03:56.325


2:03:56.325,2:03:59.105


2:03:59.105,2:04:01.365


2:04:01.835,2:04:04.485


2:04:04.485,2:04:07.905


2:04:09.055,2:04:12.015


2:04:12.015,2:04:14.125


2:04:14.125,2:04:17.055


2:04:17.055,2:04:19.925


2:04:19.925,2:04:22.855


2:04:22.855,2:04:25.945


2:04:25.945,2:04:29.145


2:04:29.145,2:04:32.375


2:04:32.375,2:04:35.415


2:04:35.415,2:04:38.645


2:04:38.645,2:04:41.655


2:04:41.655,2:04:44.745


2:04:44.745,2:04:46.025


2:04:46.525,2:04:47.525


2:04:47.645,2:04:50.675


2:04:50.675,2:04:53.965


2:04:53.965,2:04:56.705


2:04:56.705,2:04:59.365


2:04:59.415,2:05:02.455


2:05:02.455,2:05:03.665


2:05:04.065,2:05:06.135


2:05:07.055,2:05:09.295


2:05:09.645,2:05:12.985


2:05:16.775,2:05:18.875


2:05:19.295,2:05:22.455


2:05:22.455,2:05:25.465


2:05:25.465,2:05:27.855


2:05:28.635,2:05:31.685


2:05:31.685,2:05:34.865


2:05:34.865,2:05:36.055


2:05:36.615,2:05:39.045


2:05:39.855,2:05:42.835


2:05:42.905,2:05:45.325


2:05:46.785,2:05:50.035


2:05:50.235,2:05:52.785


2:05:52.785,2:05:55.755


2:05:55.755,2:05:58.465


2:05:58.465,2:05:59.965


2:06:01.855,2:06:02.855


2:06:02.925,2:06:06.005


2:06:06.005,2:06:09.085


2:06:09.085,2:06:11.945


2:06:11.945,2:06:14.925


2:06:14.925,2:06:17.975


2:06:17.975,2:06:20.965


2:06:20.965,2:06:23.485


2:06:24.005,2:06:25.265


2:06:25.865,2:06:27.775


2:06:28.725,2:06:29.995


2:06:30.635,2:06:33.675


2:06:34.395,2:06:35.505


2:06:36.745,2:06:38.205


2:06:38.785,2:06:41.985


2:06:43.295,2:06:46.255


2:06:46.255,2:06:48.785


2:06:48.785,2:06:51.795


2:06:51.795,2:06:54.065


2:06:54.885,2:06:57.695


2:06:57.695,2:07:00.315


2:07:00.815,2:07:02.005


2:07:02.375,2:07:03.515


2:07:04.585,2:07:07.775


2:07:07.775,2:07:10.065


2:07:10.575,2:07:13.815


2:07:13.815,2:07:16.605


2:07:16.605,2:07:19.635


2:07:20.085,2:07:21.475


2:07:21.935,2:07:25.315


2:07:25.315,2:07:27.195


2:07:28.095,2:07:31.015


2:07:31.015,2:07:34.055


2:07:34.055,2:07:36.665


2:07:36.665,2:07:38.815


2:07:40.015,2:07:41.015


2:07:41.315,2:07:42.785


2:07:43.305,2:07:46.295


2:07:46.295,2:07:47.295


2:07:47.785,2:07:50.835


2:07:50.835,2:07:51.835


2:07:52.885,2:07:53.885


2:07:54.565,2:07:57.315


2:07:58.745,2:07:59.745


2:08:00.925,2:08:03.535


2:08:05.055,2:08:08.045


2:08:08.045,2:08:09.185


2:08:10.045,2:08:12.645


2:08:12.645,2:08:15.375


2:08:15.495,2:08:16.695


2:08:17.355,2:08:19.635


2:08:19.635,2:08:21.365


2:08:22.705,2:08:25.215


2:08:26.395,2:08:27.395


2:08:30.465,2:08:31.795


2:08:33.165,2:08:36.195


2:08:36.195,2:08:39.245


2:08:39.395,2:08:42.495


2:08:42.495,2:08:45.205


2:08:45.385,2:08:48.305


2:08:48.305,2:08:51.185


2:08:51.185,2:08:54.275


2:08:54.275,2:08:57.165


2:08:57.165,2:08:58.165


2:09:00.135,2:09:03.165


2:09:03.165,2:09:05.815


2:09:07.165,2:09:09.665


2:09:09.665,2:09:12.825


2:09:14.105,2:09:16.705


2:09:17.095,2:09:20.125


2:09:20.125,2:09:21.775


2:09:22.525,2:09:23.525


2:09:24.445,2:09:27.005


2:09:28.075,2:09:29.075


2:09:29.645,2:09:32.755


2:09:32.755,2:09:35.645


2:09:35.645,2:09:38.795


2:09:39.135,2:09:42.475


2:09:42.475,2:09:44.935


2:09:44.935,2:09:47.605


2:09:47.605,2:09:50.615


2:09:50.615,2:09:52.715


2:09:53.065,2:09:56.005


2:09:56.695,2:09:59.875


2:09:59.875,2:10:02.415


2:10:02.415,2:10:05.775


2:10:08.815,2:10:12.045


2:10:12.045,2:10:14.525


2:10:16.905,2:10:19.885


2:10:19.885,2:10:23.025


2:10:23.025,2:10:25.755


2:10:25.995,2:10:28.695


2:10:29.035,2:10:31.755


2:10:32.355,2:10:35.215


2:10:35.415,2:10:38.265


2:10:38.265,2:10:41.255


2:10:41.255,2:10:43.635


2:10:43.635,2:10:46.645


2:10:46.645,2:10:49.255


2:10:49.885,2:10:51.325


2:10:52.205,2:10:53.205


2:10:55.285,2:10:58.235


2:10:58.235,2:11:01.265


2:11:01.265,2:11:04.295


2:11:04.295,2:11:07.295


2:11:07.295,2:11:09.975


2:11:09.975,2:11:13.015


2:11:13.015,2:11:16.245


2:11:16.245,2:11:19.365


2:11:19.365,2:11:20.435


2:11:20.825,2:11:21.825


2:11:23.545,2:11:26.465


2:11:26.465,2:11:28.965


2:11:28.965,2:11:32.005


2:11:32.005,2:11:35.115


2:11:35.115,2:11:38.085


2:11:38.085,2:11:40.415


2:11:40.415,2:11:43.285


2:11:43.285,2:11:46.605


2:11:46.605,2:11:49.185


2:11:49.275,2:11:52.165


2:11:52.165,2:11:55.435


2:11:55.455,2:11:57.355


2:11:57.705,2:12:00.855


2:12:00.855,2:12:03.375


2:12:03.795,2:12:06.835


2:12:06.835,2:12:09.375


2:12:09.415,2:12:11.795


2:12:12.065,2:12:14.575


2:12:14.575,2:12:17.675


2:12:17.675,2:12:20.585


2:12:20.585,2:12:23.525


2:12:23.525,2:12:26.535


2:12:26.535,2:12:29.375


2:12:29.375,2:12:31.565


2:12:31.565,2:12:34.555


2:12:35.775,2:12:38.755


2:12:38.755,2:12:41.995


2:12:42.235,2:12:45.175


2:12:45.175,2:12:48.525


2:12:48.555,2:12:51.555


2:12:51.555,2:12:54.635


2:12:54.635,2:12:56.185


2:12:56.535,2:12:59.345


2:12:59.345,2:13:02.655


2:13:02.725,2:13:04.875


2:13:06.235,2:13:07.235


2:13:08.015,2:13:09.965


2:13:10.385,2:13:13.685


2:13:13.685,2:13:16.695


2:13:16.695,2:13:19.575


2:13:19.575,2:13:21.215


2:13:21.545,2:13:24.495


2:13:24.495,2:13:27.245


2:13:27.245,2:13:30.095


2:13:30.095,2:13:33.225


2:13:33.225,2:13:34.705


2:13:35.555,2:13:39.015


2:13:39.805,2:13:43.115


2:13:44.255,2:13:47.265


2:13:47.265,2:13:50.135


2:13:50.135,2:13:53.055


2:13:53.055,2:13:55.485


2:13:55.485,2:13:58.315


2:13:58.315,2:14:01.245


2:14:01.245,2:14:02.355


2:14:02.825,2:14:05.925


2:14:05.925,2:14:08.265


2:14:08.655,2:14:11.665


2:14:11.665,2:14:14.755


2:14:14.755,2:14:16.865


2:14:17.715,2:14:20.685


2:14:20.685,2:14:21.915


2:14:23.175,2:14:26.305


2:14:26.305,2:14:29.175


2:14:29.175,2:14:30.395


2:14:31.515,2:14:34.835


2:14:34.835,2:14:37.885


2:14:37.885,2:14:41.105


2:14:41.815,2:14:44.775


2:14:44.775,2:14:47.825


2:14:47.825,2:14:51.135


2:14:51.555,2:14:54.095


2:14:54.095,2:14:55.605


2:14:57.315,2:15:00.125


2:15:00.125,2:15:03.445


2:15:03.905,2:15:06.885


2:15:06.885,2:15:09.895


2:15:09.895,2:15:12.875


2:15:12.875,2:15:13.995


2:15:14.355,2:15:15.565


2:15:16.575,2:15:19.725


2:15:20.795,2:15:24.025


2:15:24.025,2:15:25.335


2:15:25.775,2:15:29.035


2:15:29.345,2:15:30.855


2:15:31.395,2:15:34.305


2:15:34.305,2:15:37.315


2:15:37.315,2:15:38.315


2:15:38.315,2:15:41.565


2:15:41.565,2:15:43.205


2:15:43.525,2:15:46.665


2:15:46.665,2:15:49.665


2:15:49.685,2:15:52.455


2:15:52.455,2:15:55.585


2:15:55.585,2:15:58.535


2:15:58.535,2:16:01.205


2:16:02.045,2:16:05.085


2:16:05.085,2:16:07.985


2:16:08.105,2:16:09.105


2:16:09.225,2:16:11.815


2:16:11.815,2:16:14.915


2:16:14.955,2:16:18.235


2:16:18.235,2:16:21.325


2:16:21.325,2:16:24.335


2:16:24.335,2:16:27.435


2:16:27.435,2:16:30.215


2:16:30.895,2:16:32.365

