0:00:00.040,0:00:03.560
돌아오신걸 환영합니다
보게되어 기쁘군요

0:00:06.960,0:00:11.420
딥러닝 하느라 바쁜 한주 보내셨나요?

0:00:12.945,0:00:15.995
몇 가지 멋진 일들이 일어나고 있더군요

0:00:15.995,0:00:19.075
지난주 처럼, 여러분들의 학우들 중 몇몇이 적은

0:00:19.080,0:00:22.160
몇가지 흥미로운 글들을 소개 해 드리려고 합니다

0:00:24.760,0:00:27.520
Vitaly 라는 분은

0:00:27.520,0:00:31.415
저도 한동안 접해보지 못할 정도의
최고의 글을 적어 주셨습니다

0:00:31.415,0:00:34.575
'차등 학습률'과

0:00:34.575,0:00:37.825
'재시작하는 확률적 경사하강'에 대한
내용을 다루고 있습니다

0:00:37.860,0:00:40.060
가능하면, 꼭 읽어보시기 바랍니다

0:00:40.060,0:00:45.940
이 분이 적은 글은
독자들의 배경지식 수준과 상관 없이

0:00:46.040,0:00:48.965
그 주제들에 대해서

0:00:48.965,0:00:52.095
많은 것을 얻어갈 수 있도록 작성되었습니다

0:00:52.095,0:00:55.115
또한, 좀 더 깊은 내용을 원하는 분들을 위해서

0:00:55.115,0:00:57.785
아카데믹 논문의 링크와

0:00:57.785,0:01:00.545
이 글의 주제에 대한 예제도 제공하고 있습니다

0:01:00.545,0:01:02.835
제 생각에 이 글은

0:01:03.080,0:01:05.740
특히나 나이스하게 잘 적힌 글로

0:01:05.740,0:01:09.175
기술적 커뮤니케이션에 대한
좋은 롤모델도 될 수 있을 것 같습니다

0:01:09.180,0:01:13.180
지난 한 주간 사람들이 적은 글에 대해서

0:01:13.360,0:01:16.300
제가 마음에 들었던 한 가지 점은

0:01:16.300,0:01:19.920
제가 마음에 들었던 한 가지 점은

0:01:20.340,0:01:23.280
포럼에서 여러가지 토론이 진행 되었다는 것입니다

0:01:23.285,0:01:26.445
많은 사람들이 서로를 도와줬는데

0:01:26.445,0:01:29.495
블로그 글의 특정 부분이 잘못 되었으면

0:01:29.500,0:01:33.740
이를 설명 해 주기도 하고

0:01:34.360,0:01:35.360
새로운 아이디어를 서로 배울 수 있는
기회도 되었습니다

0:01:35.725,0:01:38.775
확률적 경사 하강법과 차등 학습률도

0:01:38.775,0:01:41.985
토론의 몇 가지 주제중 하나였습니다.

0:01:42.400,0:01:44.440
확률적 경사 하강법과 차등 학습률도

0:01:44.500,0:01:47.800
토론의 몇 가지 주제중 하나였습니다.

0:01:48.040,0:01:51.200
Anand Saha라는 분도
비슷한 주제에 대한 글을 적었습니다

0:01:51.205,0:01:54.225
왜 이것들이 잘 동작하는지에 대한 설명과

0:01:54.275,0:01:57.295
몇 좋은 그림들과
참조 논문들의 내용을 담았고

0:01:57.295,0:02:00.745
실제로 어떻게 동작하는지를
보여주는 코드 또한 담아냈습니다

0:02:01.835,0:02:05.215
Mark Hoffman 이라는 분도 
똑같은 주제에 대한 글을 적었는데

0:02:05.215,0:02:08.295
약간 입문자들을 위한 수준의 글이고

0:02:08.295,0:02:11.485
많은 직관적인 설명들을 포함하고 있습니다

0:02:11.485,0:02:14.475
Manikanta라는 분은 특별히

0:02:14.475,0:02:17.455
차등 학습률에 대하여 
왜 이주제가 흥미로운지를 설명합니다

0:02:17.455,0:02:19.975
또한, 전이학습(transfer learning) 주제에

0:02:19.980,0:02:22.520
친숙하지 않은 사람들을 위해서

0:02:22.520,0:02:25.660
전이학습이 무엇이고
왜 흥미로운 주제인지를

0:02:25.665,0:02:28.875
잘 설명 해 주고, 그걸 기반으로
왜 차등학습률이 도움이 되는지도

0:02:28.880,0:02:30.920
함께 설명 해 줍니다

0:02:31.700,0:02:33.320
또 다른 분 으로

0:02:33.320,0:02:36.580
Arjun이 적은 글에 대해서
특별히 제가 좋았던 점은

0:02:36.585,0:02:39.505
테크놀로지 관점으로만 글을 적은 것이 아니라

0:02:39.505,0:02:42.680
상업적인 관점에서 가지는 의미를

0:02:42.680,0:02:45.825
함께 설명 해 주고 있습니다

0:02:45.825,0:02:48.595
예를 들어서,

0:02:48.595,0:02:51.595
저희가 지금까지 배워 온 내용이

0:02:51.600,0:02:58.080
실 생활에서 가지는 의미라던지, 등등
설명을 해 주고 있습니다

0:02:58.140,0:03:03.735
어쨋든 온라인에서 멋진 활동들이 일어나고 있었습니다

0:03:03.735,0:03:06.915
모든 분들에게 감사를 드리고 싶어요

0:03:07.795,0:03:10.825
지난주에 말씀 드렸다 시피

0:03:10.825,0:03:13.565
기술 블로그 글을 적어보신 적이 없어서

0:03:13.565,0:03:16.515
어떻게 글을 적는지 잘 모르겠거나

0:03:16.515,0:03:19.555
겁을 먹으셨다면, 그냥 시도해 보세요

0:03:19.555,0:03:22.245
같은 그룹원 들에게 매우 환영 받고,

0:03:22.800,0:03:26.500
용기를 받게도 될 겁니다

0:03:29.060,0:03:30.060


0:03:30.885,0:03:33.685
오늘의 레슨은 약간 흥미롭습니다

0:03:33.685,0:03:36.860
왜냐하면, 많은 여러가지

0:03:36.860,0:03:40.320
어플리케이션 주제를 다룰 것이기 때문입니다

0:03:40.325,0:03:43.355
지금까진, 컴퓨터 비젼에 많은 시간을 할애 했는데

0:03:43.360,0:03:46.960
오늘은 시간이 허락한다면
세 가지 완전히 다른 분야를

0:03:46.980,0:03:52.680
모두  다루게 될 것입니다

0:03:54.060,0:03:57.005
그 시작은
"구조화된 학습" 또는

0:03:57.005,0:04:00.045
"구조화된 데이터 학습" 을

0:04:00.045,0:04:03.005
데이터베이스의 테이블같은

0:04:03.005,0:04:07.260
서로다른 데이터 종류에 대한
컬럼으로 이뤄진 데이터를 위한

0:04:07.260,0:04:10.320
모델을 만들면서 배우게 됩니다

0:04:10.780,0:04:13.860
두 번째 주제는 딥러닝을

0:04:13.865,0:04:16.835
"자연어 처리"에 적용하는 것입니다.

0:04:17.240,0:04:21.100
그리고 세 번째 주제는 딥 러닝을
"추천 시스템"에 적용하는 것입니다.

0:04:21.100,0:04:22.780
이 세가지 주제를

0:04:23.425,0:04:26.395
하이레벨 관점에서 다루게 됩니다

0:04:26.400,0:04:28.260
따라서, 주로 집중할 부분은

0:04:28.260,0:04:30.940
내부적으로 어떻게 동작 하느냐 보단

0:04:30.940,0:04:34.460
이를 수행하기 위해 소프트웨어를 어떻게
사용하는 방법이 됩니다

0:04:34.460,0:04:37.080
그리고 나선, 이후의 다음 세 레슨에서

0:04:37.080,0:04:40.780
내부적으로 어떤일이 일어나는지
구체적으로 파헤쳐 보게 될 겁니다

0:04:40.860,0:04:44.440
또한, 컴퓨터 비젼 주제 중
우리가 건너뛴 부분에 대해서

0:04:44.440,0:04:47.940
훨씬 더 깊이있는 상세한 부분을
공부 할 것입니다.

0:04:48.380,0:04:51.080
어쨋든 오늘 레슨의 초점은

0:04:51.080,0:04:54.540
이 세가지 어플리케이션을 어떻게
수행할 수 있는지에 대한 것이고

0:04:54.540,0:04:57.560
관련된 약간의 컨셉도 포함됩니다.

0:05:00.120,0:05:03.380
일단 시작하기 전에
먼저 말씀드릴 것이 있습니다

0:05:03.380,0:05:07.500
새로운 컨셉인데, dropout 이라는 것입니다

0:05:07.500,0:05:10.320
이미 dropout이 얼마나 중요한지

0:05:10.320,0:05:13.320
많이 들어보셨을 수도 있을 겁니다

0:05:13.320,0:05:15.680
사실상 중요한 컨셉으로

0:05:15.680,0:05:18.455
설명 드리기 위해서 현재 진행 중인

0:05:18.455,0:05:21.385
개 품종 분류 Kaggle 경연을

0:05:21.385,0:05:23.585
다시 한번 살펴 보려고 합니다

0:05:23.925,0:05:26.745
우선 첫번째로 수행한 내용은

0:05:26.745,0:05:29.785
precompute=True
인자값과 함께

0:05:29.785,0:05:32.755
평소처럼 미리학습된 네트워크를 생성 했습니다

0:05:32.755,0:05:35.785
그러면, 마지막 컨볼루션 계층의

0:05:35.785,0:05:38.725
activation을 미리 계산하게 됩니다

0:05:38.725,0:05:42.125
기억하시죠? 
activation은 단순히 단일-숫자 입니다

0:05:42.175,0:05:45.455
상기 시켜 드리겠습니다

0:05:45.455,0:05:48.175
여기에 하나의 activation이 있습니다

0:05:48.175,0:05:49.525
하나의 숫자죠

0:05:49.875,0:05:52.765
구체적으론, activation은

0:05:52.765,0:05:55.320
필터/커널 을 구성하는

0:05:55.320,0:05:59.000
가중치 또는 파라메터라고도 
불리는 것이

0:05:59.000,0:06:02.920
직전 계층의 activation에 적용된 것에
기반하여 계산됩니다

0:06:02.920,0:06:04.955
직전 계층이란 것은

0:06:04.955,0:06:07.615
입력이 될 수도 있고

0:06:07.615,0:06:09.985
또 다른 계산의 결과물이 될 수도 있습니다

0:06:09.985,0:06:14.220
어쨋든 activation 이라고 하면,
계산된 숫자라는 것을 기억해 두시면 됩니다

0:06:14.660,0:06:17.920
몇 activation을 미리 계산한 후에 한 일은

0:06:17.920,0:06:21.000
초기에 무작위로 생성된

0:06:21.000,0:06:24.095
여러개의 fully connected 계층을

0:06:24.095,0:06:27.175
추가 한 것입니다

0:06:27.175,0:06:30.035
단순한 행렬의 곱셈의 수행으로

0:06:30.040,0:06:32.380
엑셀에서 간단히 보여 드리자면

0:06:32.380,0:06:35.480
가장 마지막 부분으로 가서

0:06:35.540,0:06:39.840
이렇게 생긴 행렬을 가지고
행렬의 곱셈을 수행 하였습니다

0:06:41.200,0:06:43.140
그리고 나서

0:06:43.140,0:06:45.800
단순히 learn 객체를 타이핑 해 보면

0:06:46.200,0:06:48.880
어떤 계층들이 들어 있는지와 같은

0:06:48.880,0:06:51.680
learn 객체 중 추가된 마지막 계층을
이루는게 뭔지가 출력되게 됩니다.

0:06:51.780,0:06:54.460
전 레슨들에선
계층을 추가하는 부분의

0:06:54.460,0:06:58.540
설명을 약간 건너뛰었는데,
여기 것들이 실제 learn을 구성하는 계층들 입니다

0:06:58.800,0:07:03.200
BatchNorm은 마지막 레슨에서 다루니까
지금은 일단 신경쓰지 않아도 됩니다

0:07:03.620,0:07:06.940
Linear 계층은 단순히
행렬의 곱셈을 의미합니다

0:07:07.035,0:07:09.975
(2)는 1024개의 열과

0:07:09.980,0:07:12.500
512개의 행으로 이뤄진 행렬입니다

0:07:12.500,0:07:14.920
다른말로 표현 해 보면,

0:07:14.920,0:07:17.860
1024개의 activation을 입력으로 받아들이고

0:07:17.860,0:07:20.660
512개의 출력을 뱉어 낸다고 볼 수 있습니다

0:07:20.960,0:07:25.240
그 다음에는 ReLU 계층이 있습니다.
음수를 0으로 대체한다는 것을 기억해 주세요

0:07:25.580,0:07:28.535
다시 BatchNorm 부분은 건너뛰고,
dropout은 잠시 후에 설명 드리겠습니다

0:07:28.535,0:07:31.715
두 번째 Linear 계층은 
직전 Linear 계층에서 512개의

0:07:31.715,0:07:34.775
activation을 입력으로 받아서

0:07:34.780,0:07:38.160
512x120 행렬과 곱셈을 수행 한 후

0:07:38.160,0:07:41.635
120개의 새로운 activation을 뱉어내게 됩니다

0:07:41.635,0:07:44.535
그리곤 그 결과를 Softmax에 집어 넣게 됩니다

0:07:44.535,0:07:47.645
Softmax를 기억 못하시는 분들을 위해서

0:07:47.645,0:07:50.715
지난 주의 내용을 다시 설명 드리자면,

0:07:50.715,0:07:54.085
activation 값을 가지고 하는 작업으로

0:07:54.085,0:07:56.855
dog를 예로 설명 드리겠습니다

0:07:56.860,0:08:00.160
모든 분류 카테고리에 대한
 EXP(activation) 값을 더한 후,

0:08:00.160,0:08:03.160
더해진 값으로 dog에 대한 
EXP(activation) 값을 나눠 줍니다

0:08:03.160,0:08:06.140
그렇게 모든 카테고리에 대해서 계산된 값을

0:08:06.140,0:08:09.240
더해보면 1이 되게 되고,
각각의 값들은 0~1 범위에 속하게 됩니다

0:08:10.080,0:08:12.745
지금까지 보신 것들이
precompute=True로 설정할 때

0:08:12.745,0:08:15.755
추가되는 계층들 이라고 보시면 됩니다

0:08:15.755,0:08:18.735
여기서, 제가 설명 드리고자 하는건

0:08:18.740,0:08:21.920
dropout이 무엇이고,
값을 선택할때 매우 중요한

0:08:21.920,0:08:25.320
p 가 의미하는게 뭔지에 대한 것입니다

0:08:25.400,0:08:28.360
Dropout(p=0.5) 라는 것을

0:08:28.365,0:08:31.315
엑셀 스프레드시트로 설명 드리겠습니다

0:08:31.340,0:08:34.680
일단, 아무 레이어나 선택해 봅시다

0:08:34.680,0:08:37.935
p=0.5 인 dropout을 Conv2 계층에

0:08:37.935,0:08:40.145
적용해 보겠습니다

0:08:40.700,0:08:42.100
이것의 의미는

0:08:42.140,0:08:45.000
activation들을 지나가면서
50% 확률로

0:08:45.000,0:08:46.520
셀을 선택하는 것입니다

0:08:46.855,0:08:49.825
무작위로 전체 셀의 약 50%를 선택한 후

0:08:49.825,0:08:52.045
이들을 지웁니다

0:08:53.635,0:08:54.635


0:08:54.805,0:08:57.885
이게 dropout이 하는 일입니다

0:08:57.885,0:09:00.400
그러니까, p=0.5의 의미는

0:09:00.400,0:09:02.420
셀을 삭제 할지에 대한

0:09:02.420,0:09:03.740
확률이 됩니다

0:09:04.705,0:09:05.695


0:09:05.700,0:09:08.240
이 셀들을 삭제한 후
결과를 보시면,

0:09:08.240,0:09:11.980
삭제하기 전과 비교해서

0:09:11.985,0:09:14.875
크게 바뀌지 않음을 알 수 있습니다

0:09:14.875,0:09:17.905
아주 약간만이 바뀌었습니다.
특히 Max pooling 계층 이어서

0:09:17.905,0:09:20.915
지워진 값이 4개 값의 그룹 중

0:09:20.915,0:09:23.685
가장 큰 값인 경우만, 변화가 일어납니다

0:09:24.105,0:09:27.165
max pooling이 아닌 컨볼루션 계층이

0:09:27.165,0:09:30.225
수행되는 경우에는 필터에 의해 계산된

0:09:30.225,0:09:32.665
하나의 값으로 부터만 영향이 있습니다

0:09:33.280,0:09:35.300
흥미로운점은

0:09:35.340,0:09:38.895
무작위로 계층의activation들의

0:09:38.900,0:09:42.080
절반을 버리는 이 행위가

0:09:42.080,0:09:45.780
매우 흥미로운 결과를 보여준다는 것입니다

0:09:45.785,0:09:48.745
한가지 중요히 언급할 내용이 있는데

0:09:48.745,0:09:51.040
동일 계층에 대해서,
각 미니배치 마다

0:09:51.140,0:09:54.500
서로다른 절반의 activation들이
버려질 수 있다는 것입니다

0:09:54.805,0:09:57.465
dropout이 의미하는 것은
강제로

0:09:57.465,0:09:59.535
과적합되지 않도록 하는 것입니다

0:10:00.145,0:10:03.075
다른말로 표현해 보면,
만약 특정 activation이

0:10:03.080,0:10:06.320
특정 개나 고양이를 정확히 인식하는데

0:10:06.360,0:10:09.380
학습이 되어 있다면,

0:10:09.460,0:10:12.800
이 activation을 제거 될 때
네트워크 전체는 전처럼

0:10:12.840,0:10:16.160
그 특정 이미지의 인식에 대해서
잘 동작하지 못하게 됩니다

0:10:16.160,0:10:19.980
그러면, 이렇게 제거된 네트워크가
잘 동작하게 하기 위해선

0:10:19.980,0:10:24.220
매번 무작위로 남겨진 절반의 activation만으로도

0:10:24.220,0:10:27.875
여전히 잘 동학하기 위한

0:10:27.880,0:10:31.640
값들을 찾기위한 학습이 수행되어야 합니다

0:10:33.040,0:10:37.555
dropout이 등장한지 약 3~4년 정도 되었습니다.

0:10:37.560,0:10:40.240
그리고 이 아이디어는

0:10:40.280,0:10:43.855
현대의 딥러닝을 잘 동작하게 만드는데

0:10:43.855,0:10:46.855
대단히 중요해져 왔습니다

0:10:46.920,0:10:48.880
그 이유는

0:10:48.880,0:10:52.340
일반화에 대한 문제를 해결해 주었기 때문입니다

0:10:52.340,0:10:54.920
Dropout이 등장하기 전까진

0:10:54.920,0:10:57.375
수 많은 파라메터로 구성된

0:10:57.380,0:11:00.900
모델을 학습시키는 과정에서

0:11:00.920,0:11:04.000
이미 data augmentation을 시도 해 봤고

0:11:04.000,0:11:07.440
이미 할 수 있는한 많은 데이터에
시도 해 봤는데도

0:11:07.440,0:11:11.320
과적합이 일어난다면,
여러가지를 시도해 볼 수 있겠지만

0:11:11.320,0:11:14.300
일반적론 빠져나오기
어려웠습니다

0:11:14.300,0:11:17.915
그리고 나선, Hinton 교수와 몇 동료들이
이 dropout이라는

0:11:17.915,0:11:21.085
아이디어를 생각해 냈습니다.

0:11:21.085,0:11:24.045
이 아이디어는 두뇌가
동작하는 방식과

0:11:24.045,0:11:26.875
Hinton 교수의 은행에서의
경험에 의해

0:11:26.880,0:11:29.120
영감을 받았습니다

0:11:29.880,0:11:33.060
어쨋든 이들은
무작위로 activation을 삭제하는

0:11:33.060,0:11:36.120
이 멋진 아이디어를 생각 해 냈습니다

0:11:36.660,0:11:40.060
만약 p 값이 0.01이라고

0:11:40.060,0:11:42.700
상상해 봅시다.

0:11:42.700,0:11:47.300
그러면, 계층의 activation들 중 1%만을
무작위로 버리게 되는데

0:11:47.300,0:11:52.280
무작위로 별로 큰 변화를 주진 못 할 것입니다

0:11:52.560,0:11:55.760
그 말은 과적합 발생으로 부터

0:11:55.760,0:11:59.160
별로 보호 받을 수 없다는 것입니다.
반면에,

0:11:59.325,0:12:02.495
만약 p 값이 0.99 이라면

0:12:02.495,0:12:05.745
거의 대부분의 모든것을

0:12:05.760,0:12:07.940
버리게 됩니다.

0:12:07.940,0:12:09.540
그러면,

0:12:09.540,0:12:12.000
과적합이 발생하기가 어려워 져서

0:12:12.000,0:12:14.480
일반화에 좋을 수 있지만,

0:12:14.480,0:12:17.080
동시에 좋은 정확도 또한 얻을 수 없게 됩니다

0:12:17.200,0:12:20.635
일종의 트레이드 오프가 있을 수 있다는 이야기죠

0:12:20.640,0:12:23.340
높은 p값은 일반화에 좋지만

0:12:23.360,0:12:26.105
정확도가 낮아지는 반면,

0:12:26.105,0:12:29.185
낮은 p값은 일반화에 좋진 않지만

0:12:29.185,0:12:31.660
더 나은 정확도를 가져다 줍니다

0:12:31.860,0:12:35.135
학습 초기에, 검증 데이터셋의 손실값이

0:12:35.140,0:12:38.500
학습 데이터셋의 손실값보다 나은 경향을

0:12:38.500,0:12:42.135
보인다는 사실에, 
왜 그런지 궁금하신 분들이 있을 겁니다

0:12:42.140,0:12:46.760
미지의 데이터셋(아직 보지 못한)에 대한 손실이

0:12:46.760,0:12:50.580
학습 중인 데이터셋의 손실보다 더 낫다는 것이

0:12:50.585,0:12:53.675
얼핏 보기엔 이상한 일이기 때문입니다.

0:12:53.675,0:12:56.905
그 이유는
검증 데이터셋에 대한 "검증"을 수행할 땐

0:12:56.905,0:12:58.940
dropout 기능을 끄기 때문입니다

0:12:58.940,0:13:01.915
다시 말해보면, inference(추론)을 할 때

0:13:01.915,0:13:04.605
즉, 이미지가 고양이냐 개냐를 판단 할 때

0:13:04.605,0:13:07.615
무작위의 dropout을 사용하고 싶진 않을 것입니다.

0:13:07.615,0:13:10.435
학습된 최상의 모델을 사용하고 싶을 테니까요.

0:13:10.880,0:13:14.220
따라서, 모델에 dropout이 사용된 경우

0:13:14.220,0:13:18.560
특히 학습 초기에
검증 데이터셋의 정확도와 손실이

0:13:18.560,0:13:22.140
더 나은 경향을 보이게 됩니다.

0:13:22.140,0:13:23.680


0:13:23.800,0:13:25.620
(질문)

0:13:25.840,0:13:28.620
activations들이 버려진다는 사실에 대해서

0:13:28.620,0:13:32.380
우리가 특별히 뭔가를 해야 하나요?

0:13:32.380,0:13:34.780
아주 좋은 질문 입니다

0:13:34.840,0:13:37.855
저희(fastai)는 안해도 되지만,
PyTorch는 뭔가를 합니다

0:13:37.860,0:13:41.340
PyTorch의 내부에서는
두 가지 일이 일어나는데

0:13:41.340,0:13:45.660
p=0.5 인 경우,
activation의 절반은 버려 지지만

0:13:45.660,0:13:47.240
또한,

0:13:47.240,0:13:51.180
존재하는 activation을 두 배 증가시킵니다

0:13:51.320,0:13:55.720
따라서, 평균 activation 수는 변함이 없게 됩니다

0:13:56.180,0:13:58.860
꽤나 좋은 방법 이죠

0:13:59.080,0:14:02.100
어쨋든 걱정하실 필요는 없습니다

0:14:02.100,0:14:04.580
내부적으로 핸들링 되니까요

0:14:05.020,0:14:09.280
ps 라는 인자 값을 
설정해 줄 수 있습니다.

0:14:09.280,0:14:12.755
ps 인자는 모든 추가된 
계층에 적용되는 것으로

0:14:12.755,0:14:15.595
fastai 라이브러리를 통해서

0:14:15.600,0:14:19.660
추가된 계층에 어떤 dropout을 넣어주고자 하는지
결정하기 위한 것입니다

0:14:19.660,0:14:21.880
미리학습된 네트워크의

0:14:21.885,0:14:24.845
dropout을 변경하지는 않습니다.

0:14:24.845,0:14:27.705
이미 적당한 수준의 dropout이 적용되어

0:14:27.705,0:14:30.625
있기 때문에, 이를 변경할 필요는 없습니다.

0:14:30.625,0:14:34.300
추가된 계층들에의 모든 dropout의
p값 만을 설정하기 위한 것입니다.

0:14:34.300,0:14:36.760
코드를 보시면
ps=0.5라고 했는데,

0:14:36.960,0:14:41.120
첫 번째와 두 번째의 dropout이 모두
0.5인 것을 보실 수 있습니다

0:14:41.120,0:14:44.100
출력된 추가된 계층 정보의 입력 부분은

0:14:44.100,0:14:47.560
미리 학습된 네트워크의
마지막 컨볼루션 계층의 출력과 연결되어 있습니다

0:14:47.820,0:14:50.735
그래서, 추가된 첫 번째 Linear 계층을 시작하기 전

0:14:50.740,0:14:54.100
그 출력의 절반을 버리게 되는 것입니다.

0:14:54.300,0:14:56.980
그리곤 음수값들을 버리고 (ReLU),

0:14:56.980,0:15:00.600
그로부터 다시 절반을 버리게 되고,
그 결과로 두 번째 Linear 계층을 수행합니다

0:15:00.600,0:15:03.420
그 후, 마지막으로 Softmax를 
수행하게 됩니다

0:15:03.420,0:15:05.775
마이너한 
수의 정밀도에 대한 이유로

0:15:05.780,0:15:08.180
Softmax를 바로 사용하는 것 보다

0:15:08.180,0:15:11.495
Softmax의 LOG를 사용 하는 것이
더 낫다고 밝혀졌습니다.

0:15:11.495,0:15:14.555
이 방법을 사용했기 때문에,

0:15:14.555,0:15:17.925
모델의 예측 결과에 대해서

0:15:18.000,0:15:20.560
np.ex() 를 사용 해야 했던 것입니다.

0:15:20.780,0:15:24.160
왜 그런지의 상세한 내용은
크게 중요하진 않습니다

0:15:24.165,0:15:27.145
만약 droupout을 제거하고 싶다면,

0:15:27.145,0:15:30.060
ps=0 로 값을 설정해 주면 됩니다.

0:15:30.080,0:15:33.360
이 둘을 비교해 보면
droupout이 적용된 첫 번째 에포크엔

0:15:33.365,0:15:36.305
0.76의 정확도가 나왔지만,

0:15:36.365,0:15:39.795
droupout이 미적용된 첫 번째 에포크에선
0.80의 정확도가 나왔습니다.

0:15:40.000,0:15:43.340
즉, droupout을 사용하지 않으면
첫 번째 에포크의 결과는 더 좋습니다

0:15:43.340,0:15:45.600
아무것도 버리지 않기 때문입니다

0:15:45.600,0:15:48.560
하지만, 세번째 에포크를 보시면
dropout이 사용된 경우 0.848를

0:15:48.560,0:15:51.575
그렇지 않은 경우 0.841의 정확도를 얻었습니다

0:15:51.575,0:15:54.575
dropout 대비 시작은 더 좋았지만,

0:15:54.580,0:15:58.940
세 번의 에포크 후엔 그렇지 않게 되었습니다.
보시다시피 매우 과적합 되어 있습니다

0:15:58.940,0:16:02.120
학습 데이터셋에 0.35 손실을 보이고

0:16:02.120,0:16:04.880
검증 데이터셋에는 0.55의 손실을 보입니다

0:16:05.460,0:16:08.545
지금 이 모델의 모습을 보면,

0:16:08.545,0:16:11.485
즉, p값이 0이면, dropout이 추가되지

0:16:11.485,0:16:15.380
않는 것을 확인할 수 있는 것입니다.

0:16:18.520,0:16:22.600
한가지 더 언급할 내용으론

0:16:22.600,0:16:27.780
두 개의 Linear 계층이 추가된 것에 대한 것인데

0:16:27.780,0:16:32.355
꼭 그럴 필요는 없다는 것입니다.

0:16:32.355,0:16:35.335
xtra_fc 라는 인자가 있는데,

0:16:35.340,0:16:38.440
추가 할 fully connected 계층들이

0:16:38.440,0:16:42.580
얼마나 클지에 대한 리스트를 넣어줄 수 있습니다.

0:16:42.580,0:16:45.080
다만, 최소한 하나의

0:16:45.085,0:16:48.145
fully connected 계층은 있어야 합니다.

0:16:48.145,0:16:51.175
컨볼루션 계층의 결과를 받아들여서

0:16:51.175,0:16:53.805
(여기서는 1024의 입력)

0:16:53.805,0:16:57.085
분류 카테고리의 수 만큼의
출력을 계산해야 하기 때문입니다.

0:16:57.085,0:16:59.900
출력의 크기는 고양이/개 분류의 경우 2이며

0:16:59.900,0:17:02.595
개 품종 분류 경우는 120이며

0:17:02.595,0:17:05.935
planet 위성사진 분류는 17개 였습니다

0:17:05.940,0:17:09.660
최소 하나의 Linear 계층이 필요한 이유 입니다
그리고, 분류 목록 개수에 의존적인

0:17:09.660,0:17:12.800
이 계층의 크기는 우리가 선택할 수 없습니다

0:17:12.800,0:17:15.640
하지만, 다른 Linear 계층에 대해선
선택이 가능합니다

0:17:15.685,0:17:18.705
만약 비워진 리스트를 인자로 넣어주면

0:17:18.705,0:17:22.005
꼭 필요한 하나의 Linear 계층 이외의

0:17:22.005,0:17:24.885
어떤 추가 Linear 계층도 추가하지 않게 됩니다.

0:17:24.885,0:17:28.280
ps=0, 그리고 xtra_fx=[ ]로 설정 했기 때문에

0:17:28.400,0:17:31.920
가장 최소한으로

0:17:32.100,0:17:36.220
추가할 수 있는 모델의 형태가 됩니다

0:17:36.460,0:17:43.380
이 처럼 생성한 모델을 사용한
경우의 결과를 보실 수 있는데요,

0:17:43.640,0:17:47.780
이 노트북의 특정 상황에 대해선
꽤 합리적으로 좋은 결과를 보여 줍니다

0:17:47.785,0:17:50.845
별로 오랫동안 학습을 시키지 않았고

0:17:50.845,0:17:54.660
이 특정 모델이, 주어진 특정 상황에
잘 들어맞기 때문이지요

0:17:55.060,0:18:01.420
>> 그러면, 디폴트로 어떤형태를 사용하는게 좋을까요?

0:18:01.800,0:18:05.835
일단, 디폴트로 제공되는 형태는

0:18:05.835,0:18:08.835
첫 번째 Dropout의 p값이 0.25이고

0:18:08.840,0:18:11.500
두 번째는 0.5입니다

0:18:11.500,0:18:16.040
이 정도 설정이 대부분의 상황에서
꽤 잘 동작하는 것 같더군요

0:18:16.040,0:18:17.580
그러니까

0:18:17.580,0:18:20.840
어떤 변화도 주지 않으셔도
좋을 것 같다는 것입니다.

0:18:21.000,0:18:23.800
만약 과적합을 목격 하신다면

0:18:23.800,0:18:26.740
일단 ps=0.5로 설정해서

0:18:26.740,0:18:30.300
두 계층 모두의 p를 0.5로 
설정하는 해 보는 것으로 시작을 해 봅시다

0:18:30.305,0:18:33.265
그래도 여전히 과적합이 일어난다면

0:18:33.265,0:18:36.065
값을 0.7로 증가시켜서
무엇이 알맞은 값인지

0:18:36.065,0:18:39.075
범위를 좁혀 나갈 수 있습니다.

0:18:39.075,0:18:42.095
만약, 희소적합이 발생하는 경우라면

0:18:42.100,0:18:44.560
ps 값을 낮춰볼 수 있겠죠.

0:18:44.860,0:18:48.420
아주 값을 낮출일은 많이 없을 것입니다.

0:18:48.420,0:18:52.120
개/고양이 분류 문제에서 조차도,

0:18:52.200,0:18:55.855
값을 낮출 필요가 없었습니다.

0:18:55.860,0:18:59.360
반면에, 0.6이나 0.7로 값을
증가시킬 상황이 더 자주 있습니다.

0:18:59.460,0:19:02.440
여러분이 직접 여러가지를 시도해 보실 수 있지만,

0:19:02.460,0:19:06.940
제 경우, 대부분 상황에서 디폴트로 제공되는
형태가 잘 동작함을 발견할 수 있었습니다.

0:19:06.940,0:19:09.175
저는 개 품종 분류 문제에서는

0:19:09.180,0:19:12.160
이 값을 증가시킨 적이 있습니다.

0:19:12.160,0:19:14.655
더 큰 모델을 사용했을때,

0:19:14.655,0:19:17.705
두 계층 모두 값을 0.5로 설정 했었는데

0:19:17.705,0:19:20.725
예를 들어서, ResNet34는 비교적 더 적은 수의

0:19:20.725,0:19:23.915
파라메터로 구성 되므로 과적합이 덜 발생합니다.

0:19:23.915,0:19:26.685
하지만, 이후에 ResNet50을 시도하는 경우

0:19:26.685,0:19:29.635
훨씬 더 많은 파라메터가 존재하여
과적합이 발생함을 확인할 수 있고,

0:19:29.635,0:19:32.575
결과적으로 dropout 확률값을 증가 시키게 됐습니다.

0:19:32.580,0:19:36.580
덩치가 더 큰 모델을 사용하면,

0:19:36.580,0:19:40.520
종종 더 많은 dropout / 더 큰 값의 dropout 을
추가해야 하는 경우가 생기게 됩니다.

0:19:41.980,0:19:44.800
>> p 값을 0.5로 설정하게 되면

0:19:44.800,0:19:47.740
>> 정확히 이게 몇 퍼센트인가요?

0:19:47.740,0:19:49.560
50% 입니다.

0:19:49.800,0:19:51.220


0:19:53.365,0:19:56.335
>> 만약 과적합이 발생하면, 이를 알아내기 위한

0:19:56.340,0:19:58.700
>> 특정 방법이란게 있나요?

0:19:59.180,0:20:00.620
네 있습니다.

0:20:00.695,0:20:03.425
여기를 보시면
(스크롤 내리는 중)

0:20:03.700,0:20:07.080
학습 데이터셋에 대한 손실이, 검증 데이터셋에 대한것 보다

0:20:07.080,0:20:10.400
훨씬 낮음으로 확인 할 수 있습니다.

0:20:10.460,0:20:14.560
과적합률이 전혀 없는것이 일반적으로
최적이 아닌 것 처럼

0:20:14.660,0:20:18.800
과적합의 정도가 큰지에 대해선 알 수가 없습니다.

0:20:18.805,0:20:21.745
단지 우리가 해야 할 것은 검증 데이터셋에 대한

0:20:21.745,0:20:24.825
손실값이 더 낮도록 만드는 것입니다.

0:20:24.825,0:20:27.625
결국은 여러가지를 시도해 보고

0:20:27.625,0:20:30.340
그 중 어떤 시도가 검증 데이터셋에 대한

0:20:30.340,0:20:33.220
손실값을 더 낮도록 만들었는지 알아내야 할 것입니다.

0:20:33.380,0:20:36.680
하지만, 특정 문제에 대해서 계속
시도 해 보다 보면,

0:20:36.800,0:20:40.600
과적합이 크게 발생하는게
어떤 것인지에 대한 느낌이 오실 겁니다.

0:20:43.640,0:20:44.700


0:20:45.515,0:20:48.165
지금까지 설명 드린 것이 dropout이고

0:20:48.165,0:20:51.205
앞으로 많이 사용하게 될 것입니다.

0:20:52.455,0:20:54.995
>> 두 가지 질문이 있습니다.

0:20:54.995,0:20:58.025
>> 첫 번째는, dropout 율이 0.5일때,

0:20:58.025,0:21:03.680
>> 각 셀을 지울때 50% 확률로 지우는 것인가요?

0:21:03.860,0:21:10.020
>> 아니면, 50%의 셀을 무작위로 선택하는 건가요?

0:21:10.020,0:21:12.720
전자가 맞습니다.

0:21:13.220,0:21:18.360
>> 두 번째 질문은, 왜 평균 activation이 중요 한가요?

0:21:18.940,0:21:24.980
엑셀로 설명을 드려 보겠습니다.

0:21:25.100,0:21:30.700
예를 들어서, 이 셀의 결과 값은

0:21:33.120,0:21:38.020
직전 계층의 이 9개 셀에, 필터 두 개의 9개 값을

0:21:38.140,0:21:41.520
각각 곱한 후 더한 것입니다.

0:21:41.520,0:21:44.920
만약, 직전 계층의 9개 셀의 절반을 삭제한다면

0:21:44.920,0:21:47.295
결과도 절반으로 줄어들게 되고

0:21:47.295,0:21:50.325
그 다음의 결과들도 바뀌게 되어서

0:21:50.325,0:21:52.265
의미 자체가 변하게 됩니다.

0:21:52.840,0:21:56.020
예를 들어서, activation 값이 0.6일때

0:21:56.020,0:21:58.800
솜털같은 귀라는 인식되던 것이

0:21:58.805,0:22:01.505
이제는 그 값이 0.3일때
솜털같은 귀라고 인식된다면,

0:22:01.505,0:22:04.175
그 의미 자체가 변하게 되는 것입니다.

0:22:04.180,0:22:07.420
dropout의 목표는 의미를 바꾸지 않은 채

0:22:07.420,0:22:10.360
activation들을 삭제하는 것입니다.

0:22:12.400,0:22:15.095
>> 왜 Linear activation을 사용 하나요?

0:22:15.095,0:22:18.005
왜 Linear를 사용 하느냐고요?

0:22:18.260,0:22:21.800
>> 네, 왜 그 특정 activation인가요?

0:22:22.120,0:22:25.475
이 계층들의 집합이 그러한 것입니다.

0:22:25.475,0:22:27.685
미리 학습된 네트워크는

0:22:27.685,0:22:30.625
컨볼루션 네트워크인데, 그 내용은
미리 계산되어 있는 것이라서

0:22:30.625,0:22:33.535
여기선 보이지 않습니다.

0:22:33.535,0:22:36.545
그리고 이 네트워크가 내뱉는 결과는
벡터입니다.

0:22:36.545,0:22:40.420
따라서,  이 시점에서 유일하게 선택 가능한 것은
Linear 계층을 사용하는 것입니다.

0:22:40.700,0:22:45.260
>> 레이어 단위로, 
서로 다른 dropout을 사용 할 수 있나요?

0:22:45.260,0:22:46.925
물론 입니다

0:22:46.925,0:22:49.845
>> 어떻게 할 수 있죠?

0:22:49.925,0:22:53.025
당연히 레이어 마다 서로다른 dropout을
사용할 수 있습니다.

0:22:53.025,0:22:56.095
이 파라메터명이 ps (복수)라고 불리는 이유 입니다.

0:22:56.095,0:22:58.645
배열을 넣어줄 수 있는 것이죠

0:22:59.060,0:23:00.380
예를 들어서,

0:23:00.380,0:23:03.015
ps=[0, 0.2]

0:23:03.015,0:23:05.995
그리고 xtra_fc=[512] 라고 해주면

0:23:05.995,0:23:09.125
첫 번째엔 dropout이 없게되고

0:23:09.125,0:23:11.975
두 번째엔 0.2의 dropout이 추가 됩니다.

0:23:12.200,0:23:14.440
좋은 질문 감사합니다.

0:23:14.940,0:23:17.960
저도 수 년동안 사용해 왔지만,

0:23:17.960,0:23:21.240
dropout 대한 어떤 좋은 직관력은 없습니다.

0:23:21.440,0:23:24.460
초반/후반 계층들이 언제 서로다른

0:23:24.465,0:23:27.425
dropout률을 가져야 한다던지와 같은 주제에 대해서

0:23:27.425,0:23:30.485
저도 아직 여러가지 시도를
해보고 있는 입장입니다.

0:23:30.485,0:23:33.575
그리고, 아직도 어떤 경험적인 
규칙을 찾지 못했습니다.

0:23:33.580,0:23:35.780
여러분이 이 규칙을 찾으신다면

0:23:35.780,0:23:38.800
그 내용을 들어보고 싶군요.
하지만, 확신이 서질 않는다면,

0:23:38.840,0:23:42.760
모든 fully connected 계층에 똑같은
dropout을 사용하셔도 괜찮습니다.

0:23:42.760,0:23:44.715
그 외에 시도 해 보실만한 것은

0:23:44.715,0:23:47.895
종종 사람들이 가장 마지막 Linear 계층에만

0:23:47.945,0:23:50.575
dropout을 추가한다는 사실이 있습니다.

0:23:50.575,0:23:52.575
이 두 가지를 시도 해 볼만 합니다.

0:23:54.180,0:23:57.560
>> 선생님, 정확도 대신에 손실값을

0:23:57.560,0:24:00.840
>> 모니터링 하는 이유가 뭔가요?

0:24:02.260,0:24:07.140
손실이 학습과 검증 데이터셋 모두에서

0:24:07.140,0:24:10.960
확인 가능한 유일한 것이기 때문입니다.

0:24:11.020,0:24:14.000
이 두 데이터셋에 대한 비교를 할 수 있게 해줍니다

0:24:14.180,0:24:17.320
또한, 추후 배울 내용 입니다만

0:24:17.320,0:24:20.800
손실은 알고리즘이 최적화 하는 대상입니다.

0:24:20.800,0:24:24.620
그렇기 때문에 손실을 모니터링 해 보면

0:24:24.660,0:24:28.920
좀 더 의미 파악이 쉬울 수 있습니다.

0:24:30.040,0:24:31.640


0:24:34.580,0:24:37.380
>> dropout을 사용하면, 일종의 랜덤 노이즈를

0:24:37.380,0:24:39.660
>> 매 iteration마다 추가 하는것 같습니다.

0:24:39.780,0:24:43.780
>> 그렇다면, 충분한 학습을 못한다는 의미인 것 같습니다

0:24:43.780,0:24:45.155
맞습니다

0:24:45.160,0:24:49.400
>> 그러면 학습률을  이에 맞춰서 조정 해 줄 필요가 있나요?

0:24:49.400,0:24:51.620
제가 느낀바로는 학습률에

0:24:51.620,0:24:54.100
어떤 영향을 주진 않는 것 같습니다.

0:24:54.340,0:24:56.895
아마도 이론적으론 말씀 하신것이
맞을 수 있다고 생각합니다.

0:24:56.900,0:25:00.040
하지만, 아직까지 그런 상황을
경험해 보진 못했습니다.

0:25:03.920,0:25:04.920


0:25:04.960,0:25:07.140
그러면, 지금부터는

0:25:07.140,0:25:10.020
“구조화된 데이터” 문제를 다뤄보겠습니다.

0:25:10.020,0:25:13.780
상기 차원에서 말하자면, 사용될 예제는

0:25:13.800,0:25:20.060
독일 수퍼마켓 브랜드인 
Rossman에 대한 Kaggle 경연입니다.

0:25:20.500,0:25:24.980
이 파일은 lesson3-rossman.ipynb 이니 참고 해 주세요.

0:25:27.095,0:25:29.795
사용되는 주된 데이터셋은 특정 가게가

0:25:29.800,0:25:35.060
얼마나 많이 판매를 했는지를 예측하기 위한 것입니다.

0:25:36.280,0:25:40.540
이 데이터에는 몇 중요한 정보가 있는데, 
그 중 하나는 날짜(date) 이고

0:25:40.540,0:25:42.720
다른 하나는 그날의 개점 여부입니다.

0:25:42.835,0:25:45.815
또, 그날 프로모션이 있었는지

0:25:45.820,0:25:49.180
주지정 휴일이나

0:25:49.180,0:25:52.540
휴교일 이었는지

0:25:53.520,0:25:59.180
어떤 물건을 파는 
어떤 종류의 가게인지

0:25:59.180,0:26:02.400
경쟁 가게로 부터 얼마나 떨어져 있는지

0:26:02.400,0:26:04.595
등과 같은 것들이 있었습니다.

0:26:04.595,0:26:07.015
이렇게 생긴 데이터셋의 컬럼은

0:26:07.015,0:26:09.935
두 가지 종류로 주로 나눠볼 수 있습니다.

0:26:09.935,0:26:12.945
몇 단계 수준을 가지는 범주형 컬럼이 있을 수 있습니다.

0:26:12.945,0:26:16.300
Assortment가 범주형 컬럼의 한 예로

0:26:16.300,0:26:19.300
a, b, c와 같은 수준의 범주를 가집니다.

0:26:20.325,0:26:22.505
반면에,

0:26:22.505,0:26:25.525
CompetitionDistance 컬럼은 계속형 이라고 부르는데

0:26:25.525,0:26:31.460
차이나 비율등 어떤 의미있는 
숫자 값이 저장될 수 있습니다.

0:26:31.780,0:26:34.040
이 두 가지 종류의 컬럼은 꽤

0:26:34.040,0:26:36.040
서로 다른 방법으로 다뤄져야 합니다.

0:26:36.445,0:26:39.145
어떤 종류든 머신러닝을 해 봤고

0:26:39.145,0:26:42.015
파라메터에 의해서 값이 변화하는

0:26:42.015,0:26:45.005
선형 회귀 문제를 풀어보신 분이라면,

0:26:45.005,0:26:48.320
계속형 컬럼과 친숙하실 것입니다.

0:26:49.460,0:26:52.780
범주형 컬럼은 이 보다는 좀 더 생각해 볼게 많습니다.

0:26:54.215,0:26:57.125
이 코스는 데이터 클리닝을 다루진 않습니다.

0:26:57.125,0:26:59.200
Feature 엔지니어링의 일부로,

0:26:59.200,0:27:02.040
이 과정이 이미 되어 있다고 
가정하고 진행하겠습니다.

0:27:02.480,0:27:05.595
이 Feature 엔지니어링이 끝난 후,

0:27:05.595,0:27:08.785
컬럼들의 목록을 출력 했습니다.

0:27:08.785,0:27:11.280
이 출력 결과에 대해선

0:27:11.280,0:27:14.295
제가 직접 feature 엔지니어링이나

0:27:14.295,0:27:17.135
데이터 클리닝 작업을 한 것은 전혀 없습니다

0:27:17.135,0:27:19.845
이 모든 것은 경연의 3등을 한 분의

0:27:19.845,0:27:21.720
코드를 빌려온 것입니다.

0:27:22.080,0:27:26.060
이 분이 유용하다고 판단한

0:27:26.220,0:27:29.340
여러가지 컬럼들의 리스트인 것입니다.

0:27:29.620,0:27:33.760
그 중 첫번째 컬럼들의 리스트는

0:27:33.760,0:27:37.760
범주형 컬럼으로 다뤄질 것들에 대한 것입니다.

0:27:39.060,0:27:43.400
년(Year), 월(Month), 일(Day) 같은 숫자는

0:27:43.740,0:27:47.840
2003년과 2000년 의 차이가 의미가 있을 수 있는

0:27:47.840,0:27:51.680
계속형으로 다룰 수도 있는 컬럼이지만

0:27:52.080,0:27:56.260
꼭 그럴 필요는 없습니다.

0:27:56.360,0:28:00.940
어떻게 범주형 데이터가 다뤄지는지
곧 보여 드리겠습니다

0:28:00.940,0:28:02.455
하지만, 기본적으론

0:28:02.455,0:28:05.405
어떤 데이터를 범주형으로 분류하게 되면

0:28:05.405,0:28:09.720
2000, 2001, 2002와 같은 
모든 수준의 Year 정보는

0:28:09.720,0:28:15.400
뉴럴넷에서 완전히 다른 형태로 
다뤄질 수 있게 됩니다.

0:28:16.020,0:28:18.420
반면에, 계속형이라면

0:28:18.420,0:28:21.920
계속적인 상관관계를 나타내는

0:28:21.920,0:28:24.560
어떤 함수를 생각해 내야 할 것입니다

0:28:24.965,0:28:28.335
Year와 같은 것은 종종 계속형으로 다뤄집니다

0:28:28.340,0:28:32.660
하지만, 실제로 그렇게 많은 수준이 
존재하진 않는다면

0:28:32.900,0:28:37.380
범주형으로 다뤄질 때, 
더 잘 동작하는 경우가 많습니다.

0:28:37.385,0:28:40.435
0~6 범위로 표현되는 DayOfWeek (월/화..) 가

0:28:40.440,0:28:43.080
하나의 좋은 예가 될 수 있습니다.

0:28:43.540,0:28:46.585
단순히 의미를 가지는 숫자로

0:28:46.585,0:28:51.020
3과 5라면 이틀이라는 의미가 될 수 있습니다.

0:28:51.640,0:28:54.520
하지만, 어떻게 특정 가게의 판매량이

0:28:54.520,0:28:57.440
DayOfWeek에 영향을 받을 수 있는지 생각 해 보면

0:28:57.440,0:29:00.280
토/일요일의 판매량은 이정도가 될 수 있고

0:29:00.280,0:29:03.540
금요일과 수요일은 또 다른 정도로

0:29:03.540,0:29:06.680
각 날에 대해서 질적으로 다르게 동작하게 됩니다.

0:29:06.860,0:29:09.840
그래서, 뉴럴넷이 범주형 데이터를

0:29:09.840,0:29:13.340
그런식으로 다루는 것을 곧 보시게 될 것입니다.

0:29:14.140,0:29:18.455
어떤게 범주형이고 어떤게 계속형인지는

0:29:18.460,0:29:22.940
어느 정도 모델링 할 때 결정되어야 하는 사항입니다.

0:29:23.520,0:29:27.840
어떤 데이터가 a, b, c 또는 Jeremy, Yannet (이름)

0:29:27.840,0:29:31.440
처럼 코드화 되어 있는 경우는

0:29:31.440,0:29:35.840
반드시 범주형으로 다뤄져야 합니다

0:29:36.300,0:29:40.280
이 데이터를 직접 계속형으로 
다룰 수 있는 방법은 없기 때문입니다.

0:29:40.280,0:29:41.620
반면에,

0:29:41.620,0:29:44.455
age, dayOfWeek 같이 계속형 “적인”

0:29:44.460,0:29:48.320
데이터의 경우는 계속형으로 다룰지, 범주형으로 다룰지

0:29:48.340,0:29:51.820
여러분이 직접 결정 하셔야 합니다.

0:29:51.935,0:29:54.815
요약 해 보면, 범주형 데이터는

0:29:54.815,0:29:57.805
모델에게 범주형으로만 다뤄져야 하고

0:29:57.805,0:30:00.285
계속형 데이터는 모델이 이를 계속형으로 다룰지

0:30:00.285,0:30:03.645
범주형으로 변형해서 다룰지를 선택해야 합니다.

0:30:04.135,0:30:07.385
여기서는 그게 뭐가 되었든

0:30:07.425,0:30:10.455
경연의 3등을 한 사람이 한 내용 입니다.

0:30:10.460,0:30:12.620
위에 (cat_vars)는 범주형 데이터이고

0:30:12.620,0:30:15.420
아래 (contin_vars)는 계속형으로 결정 했습니다

0:30:15.460,0:30:17.660
보시면 기본적으로

0:30:18.140,0:30:20.925
계속형으로 선택된 컬럼은 모두

0:30:20.925,0:30:23.535
사실상 실수형의 숫자로 표현되는 것들입니다.

0:30:23.540,0:30:27.900
CompetitionDistance는 소수점으로 표현되는
숫자값을 가지고,

0:30:27.900,0:30:31.740
마찬가지로 Temperature(온도)도 소수점
숫자인 것 처럼 말이죠.

0:30:31.740,0:30:34.320
이런 종류의 데이터를 범주형으로 
변형하기란 어렵습니다

0:30:34.320,0:30:37.020
너무 많은 수준이 존재하기 때문입니다.

0:30:37.120,0:30:40.720
만약 소숫점 다섯 째 까지 표현하는 데이터라면

0:30:40.720,0:30:44.480
거의 모든 데이터가 범주로 다뤄질 수 있겠죠

0:30:45.220,0:30:46.560
그런데

0:30:46.680,0:30:49.440
여기서 사용된“얼마나 많은 수준”이라는 말은

0:30:49.440,0:30:52.160
Cardinality 라고 표현 하도록 하겠습니다.

0:30:52.160,0:30:56.900
제가 Cardinality 라고 말하면,
예를 들어서 DayOfWeek 의 Cardinality는  7입니다.

0:30:56.900,0:30:59.320
7개의 서로 다른 요일이 있기 때문입니다.

0:31:02.600,0:31:07.300
>> 계속형 데이터를 bin(통계학의) 하기도 하나요?

0:31:07.300,0:31:10.840
저는 그렇게 해 본적이 없습니다.

0:31:11.820,0:31:14.055
Max_TemperatureC 같은 데이터를

0:31:14.060,0:31:18.260
0~10, 10~20, 20~30 처럼 그룹핑 해서

0:31:18.260,0:31:21.960
범주형으로 만들 수도 있을 겁니다.

0:31:21.960,0:31:26.040
흥미롭게도, bin을 만드는게 때로는 유용 하다는

0:31:26.040,0:31:30.580
내용의 한 논문이 지난 주에 나왔습니다.

0:31:31.380,0:31:33.960
근데 말 그대로 지난 주에 발표된 것이고

0:31:33.960,0:31:36.980
그 전까진 딥러닝에서 이 기법이 사용된 것을
본 적이 없습니다.

0:31:36.980,0:31:40.800
아직 논문 내용을 자세히 보진 않았습니다.
이번주 까진, 별로 좋지 않은 생각이라고

0:31:40.800,0:31:44.560
말씀 드리고 싶지만, 논문이 발표 되었으니
한번 생각 해 볼 만하다고 말할 수도 있겠습니다

0:31:48.520,0:31:49.520


0:31:49.600,0:31:53.500
>> 만약 Year을 범주형으로써 사용한다면

0:31:53.500,0:31:58.120
>> 학습때는 한번도 본 적 없는 Year가 
테스트에서 사용되면 어떤일이 발생 할까요?

0:31:59.240,0:32:02.260
잠시 후 설명 드릴게요.
간단히만 설명 드리면

0:32:02.265,0:32:05.365
알려지지 않은(unknown) 카테고리로써
다뤄질 것입니다.

0:32:05.365,0:32:09.460
DataFrame 에 사용되는 pandas 에는

0:32:09.480,0:32:13.680
특별한 unknown 이라는 카테고리가 있는데

0:32:13.680,0:32:17.560
한번도 본적이 없는 카테고리에 이를 적용합니다

0:32:18.480,0:32:23.460
그러므로, 딥러닝 모델 입장에선 unknown은 
또 다른 종류의 카테고리가 될 뿐이죠

0:32:25.620,0:32:31.260
>> 만약 학습 데이터셋엔 
unknown 카테고리가 없는데

0:32:31.260,0:32:35.440
>> 테스트 데이터셋엔 unknown이 있으면
 어떻게 되나요?

0:32:35.440,0:32:37.580
그냥 unknwon 으로써

0:32:37.580,0:32:39.800
핸들링 될 것입니다.

0:32:40.140,0:32:43.260
unknown이 0과 같은 값을 가진다고 생각 해 보면,

0:32:43.260,0:32:45.840
여전히 뭔가를 "예측"도 할 것입니다.

0:32:46.020,0:32:49.880
만약 학습 데이터의 많은 
데이터들이 unknown 이라면,

0:32:49.880,0:32:52.760
unknown에 대해서 예측할 수 있는법을 
학습할 것입니다.

0:32:52.760,0:32:56.920
하지만, 학습 데이터에 unknwon 데이터가 없다면,
예측 결과는 랜덤한 추측이 될 것입니다.

0:32:57.100,0:33:00.920
이 코스의 일부로써 이야기 해 볼 만한
학습에 관련된 흥미로운 질문이었습니다.

0:33:00.920,0:33:04.280
물론 포럼에서 토론 해 볼 수 있겠죠?

0:33:06.380,0:33:11.220
어쨋든, 범주형과 계속형 컬럼 목록을 정의 했습니다

0:33:11.220,0:33:12.320
여기선

0:33:12.320,0:33:18.760
Store와 Date 테이블을 Join한 결과의 갯수가
800,000 이상 존재 합니다.

0:33:19.580,0:33:24.580
그러면, 이제는 이 모든 컬럼들을 하나씩

0:33:24.580,0:33:29.320
확인 해 보면서 이들의 데이터 타입을

0:33:29.320,0:33:34.960
category로 변경할 수 있습니다.

0:33:35.680,0:33:38.695
pandas를 이용해서 수행한 것입니다.

0:33:38.695,0:33:41.775
제가 pandas를 가르치진 않을 것이지만,

0:33:41.775,0:33:44.695
유명한 McKinney 저서인 "python for data analysis"

0:33:44.700,0:33:46.960
외의 많은 책들이 있습니다.

0:33:46.960,0:33:49.840
하지만, 이 문법을 전에 본 적이 없더라도, 
여러분이

0:33:49.840,0:33:53.540
직관적으로 코드의 흐름을 파악할 수 있기를
희망 해 보겠습니다.

0:33:53.540,0:33:55.375
어쨋든 범주형 컬럼들의 타입을

0:33:55.380,0:33:57.560
category로 바꾸었습니다.

0:33:57.560,0:33:59.680
그리고, 계속형 컬럼의 데이터 타입은 모두

0:33:59.680,0:34:02.800
32비트의 float형으로 바꾸었습니다

0:34:02.800,0:34:07.180
이렇게 한 이유는 PyTorch에선 모든 것이

0:34:07.180,0:34:11.495
32비트의 float형의 사용이 기대되기 때문입니다.

0:34:11.500,0:34:18.060
계속형 컬럼의 몇몇, 
프로모션이나 휴일정보같은 것들은

0:34:18.060,0:34:21.895
0과 1로 표현이 되었었는데

0:34:21.895,0:34:24.495
이 값들이 소숫점을 가지는

0:34:24.500,0:34:28.680
실수형으로 바뀌게 되는 것입니다.

0:34:33.080,0:34:36.000
저는 가능한한 많은 일을 "적은 데이터셋"으로

0:34:36.000,0:34:39.335
해결 하려고 노력하곤 합니다.

0:34:39.335,0:34:42.415
이미지 관련 일을 한다면, 이미지를

0:34:42.415,0:34:45.795
64x64 또는 128x128 같은 크기로 
조절한다는 의미 입니다.

0:34:45.800,0:34:49.460
하지만, 구조화된 데이터는 이런식의 조절이
불가능 하기 때문에

0:34:49.460,0:34:52.820
전체 데이터셋에서 무작위로
적은 수의 샘플을 채취하는 방식을 사용합니다.

0:34:52.820,0:34:55.060
이렇게 일단 채취된 샘플로 
시작을 해 봅니다.

0:34:55.060,0:34:59.200
그리곤, 전제 검증 데이터셋을 구성하는 것과 
똑같은 방법으로

0:34:59.200,0:35:02.740
전체 데이터셋에서 랜덤하게 데이터를 선택해서

0:35:02.740,0:35:06.120
샘플 데이터를 구성하였는데,

0:35:06.120,0:35:11.080
이 코드 라인이 그 여러 개의 
무작위 수들을 뽑아주는 기능을 합니다.

0:35:12.660,0:35:15.720
전체 크기는 800,000 이었던 것에 비해서

0:35:15.720,0:35:19.440
이렇게 구성된 샘플 데이터셋의 크기는 150,000 입니다.

0:35:20.420,0:35:23.100
그 다음을 진행하기 전, 데이터의 모양을

0:35:23.105,0:35:26.115
살펴보면 이렇게 생긴 것을 알 수 있습니다.

0:35:26.115,0:35:27.905
Boolean형 데이터도 있고,

0:35:28.240,0:35:32.740
다양한 범위의 정수형 데이터도 있고,

0:35:32.740,0:35:36.580
여기 데이터의 Year는 2014 군요.

0:35:36.580,0:35:39.320
또, 문자형 값들도 보입니다.

0:35:39.320,0:35:43.700
이전 단계에서, pandas 에게 이것들을
category 로 만들어 달라고 요청했지만

0:35:43.720,0:35:46.540
pandas가 notebook 에선, 여전히

0:35:46.540,0:35:48.720
문자로서 이를 표현하고 있습니다

0:35:48.720,0:35:52.000
하지만, 내부적으로는 다른식으로 저장되게 됩니다.

0:35:52.220,0:35:54.875
fastai 라이브러리엔 proc_df() 라는

0:35:54.875,0:35:57.555
특별한 함수가 있습니다.

0:35:57.795,0:36:00.855
proc_df()에는 DataFrame과

0:36:00.860,0:36:03.660
의존 컬럼명을 인자로 주면

0:36:03.820,0:36:08.060
그 컬럼을 DataFrame으로 부터 추출 해서

0:36:08.060,0:36:11.080
독립적으로 따로 저장을 하고

0:36:11.080,0:36:14.140
원본 DataFrame에선 이 컬럼을 삭제하게 됩니다.

0:36:14.140,0:36:17.900
이 코드가 수행된 후 리턴되는 df에는 더 이상
Sales라는 컬럼이 존재하지 않게 되는 것입니다.

0:36:17.900,0:36:21.280
반면에, 리턴된 y에는 
Sales 컬럼의 내용이 담기게 됩니다.

0:36:21.280,0:36:23.800
그 외에 proc_df()가 하는 일은

0:36:23.800,0:36:25.160
스케일링 입니다.

0:36:25.360,0:36:30.360
뉴럴넷에 입력되는 데이터들은 표준 편차가 1인

0:36:30.360,0:36:35.960
0 근처의 값이 되는 것이 바람직 합니다.

0:36:35.960,0:36:39.615
데이터 값에서 전체 평균을 뺀 후, 
이를 표준편차로 나눠주면

0:36:39.615,0:36:42.615
이러한 값의 특성을 가지는 값으로 변형 되는데

0:36:42.665,0:36:45.735
do_scale 인자값이 True 이면, 
정확히 이를 수행 해 줍니다.

0:36:45.735,0:36:49.025
그러면, 특별한 객체를 리턴 해 주고 이를 통해서

0:36:49.025,0:36:52.820
정규화에 사용된 표준편차와 평균값 확인이 가능하고

0:36:52.820,0:36:54.515
테스트 데이터셋 구성시에

0:36:54.520,0:36:57.320
이를 활용해서 똑같은 분포의 
데이터를 구성할 수 있게 됩니다.

0:36:57.320,0:37:00.120
또한, 몇 빠진 값들도 핸들링 해 줍니다.

0:37:00.475,0:37:03.485
범주형 데이터에 대해서,

0:37:03.485,0:37:06.455
빠진 값들은 ID의 경우 0

0:37:06.460,0:37:12.000
그 외의 범주형 값들은 1, 2, 3 순으로 
채워지게 됩니다.

0:37:12.060,0:37:15.245
계속형 데이터의 경우엔,

0:37:15.245,0:37:18.640
빠진 값들은 중앙값으로 채워지게 되고

0:37:18.640,0:37:21.085
Boolean 형의 새로운 컬럼을 생성해서

0:37:21.085,0:37:24.145
이 데이터 행에 빠진 값이 있었는지를 명시 해 줍니다.

0:37:24.145,0:37:26.740
머신러닝 코스에서 자세히 다루는 내용이라서

0:37:26.740,0:37:29.520
이 부분의 설명을 아주 빠르게 진행하고 있습니다.

0:37:29.520,0:37:31.975
이 부분에 대해서 어떤 질문이 있으시다면,

0:37:31.975,0:37:34.045
딥러닝에 특화된 내용이 없는

0:37:34.045,0:37:36.480
머신러닝 코스를 수강 해 보면 좋을 것입니다

0:37:36.480,0:37:39.940
proc_df() 수행 후 결과를 보면,

0:37:39.975,0:37:43.175
예를들어 Year의 2014 값이 2로 바뀌었습니다

0:37:43.180,0:37:46.540
범주형 데이터이기 때문에, 각 범주의 값은

0:37:46.540,0:37:49.580
0부터 증가되는 값들이 할당 됩니다.

0:37:49.580,0:37:52.900
이렇게 하는 이유는 나중에 이 값들은

0:37:52.900,0:37:56.460
행렬로써 포장되어야 하는데

0:37:56.460,0:38:00.420
그 행렬이 2014개의 열이 되는 것 보단

0:38:00.420,0:38:04.520
2개의 열로 표현되면 좋기 때문 입니다.

0:38:04.960,0:38:07.760
a, c 였던 값들도 똑같은 방식으로

0:38:07.760,0:38:10.420
1, 3 값으로 대체 되었습니다.

0:38:10.900,0:38:13.985
이렇게 프로세싱된 DataFrame에는

0:38:13.985,0:38:17.005
의존컬럼이 포함되어 있지 않고,

0:38:17.005,0:38:20.155
모든 값들이 숫자입니다

0:38:20.160,0:38:25.740
이 과정이 딥러닝/RandomForest 
수행에 필요한 것이고

0:38:25.740,0:38:29.860
머신러닝 코스에서 다룬다고 했던, 윗 쪽에서

0:38:29.860,0:38:35.260
수행한 여러 작업들은 
딥러닝에 특화된 작업이 아닙니다.

0:38:36.320,0:38:39.335
머신러닝에서 많이 다루는 또 다른 내용은

0:38:39.335,0:38:42.415
검증 데이터셋에 관한 것입니다.

0:38:42.415,0:38:45.805
이 예제에서 예측해야 할 내용은

0:38:45.840,0:38:49.000
단순히 무작위로 판매량을 측정하는 것이 아니라

0:38:49.000,0:38:52.200
다음 2주동안의 판매량 입니다.

0:38:52.200,0:38:55.600
Kaggle 경연 주최자가 요구한 사항이죠.

0:38:55.880,0:38:59.300
그렇기 때문에, 
검증 데이터셋은 학습 데이터셋에서

0:38:59.300,0:39:03.480
마지막 2주 동안의 데이터로 생성 해 줍니다.

0:39:03.480,0:39:07.880
가능한 테스트 데이터와 
비슷하게 만들어 줘야 하기 때문입니다.

0:39:07.880,0:39:11.940
저희 기관의 Rachel이 얼마전 이 내용 관련의

0:39:11.940,0:39:16.120
글을 작성 했었는데,
fast.ai 에서 읽어보시길 권장합니다.

0:39:16.120,0:39:19.480
관련 링크를 레슨 wiki에도 올려 두겠습니다.

0:39:19.480,0:39:21.100
기본적으론

0:39:21.100,0:39:25.740
저희가 최근에 가르친 머신러닝 코스의 요약으로,

0:39:25.740,0:39:28.700
관련 비디오도 시청 가능하지만,

0:39:28.700,0:39:32.840
이 것은 비디오 내용을 글로 요약한 것입니다.

0:39:33.780,0:39:34.780


0:39:35.620,0:39:39.480
Rachel과 저는 어떻게 학습/검증/테스트 데이터셋이

0:39:39.480,0:39:43.560
다뤄져야 하는지를 생각하는데 많은 시간을 보냈습니다.

0:39:43.600,0:39:46.360
그 글에는 저희의 생각이 담겨져 있는데

0:39:46.360,0:39:48.920
다시 말하지만,
 딥러닝에 특화된 내용은 없습니다.

0:39:49.020,0:39:52.900
어쨋든, 이제부터는 딥러닝을 해 보도록 하겠습니다.

0:39:52.900,0:39:57.180
이 특정 경연 뿐만 아니라, 다른 경연이나

0:39:57.180,0:40:01.500
다른 종류의 머신러닝 프로젝트에서는

0:40:01.500,0:40:05.560
사용하는 metric에 대한
확고한 이해를 해야만 합니다.

0:40:05.560,0:40:08.600
결과가 어떻게 평가될지에 대한 것이죠.

0:40:08.600,0:40:11.800
Kaggle에서는 비교적 쉽게, 
어떻게 평가될지를 명시 해 줍니다.

0:40:11.800,0:40:16.680
이 경연은 RMSPE (Root Mean Squared 
Percentage Error) 로써 평가 될 것입니다.

0:40:16.680,0:40:22.960
이 함수는 실제 레이블값과 예측 결과값의

0:40:22.960,0:40:28.960
차이에 대한 평균을 백분율로 구하는 것입니다.

0:40:29.160,0:40:36.180
여러분이 Log에 대해서 잘 알아야 한다고
충고 해 드린 적이 있습니다.

0:40:36.180,0:40:40.400
여기선 예측 결과를 실제 결과로 나눈 것의

0:40:40.400,0:40:47.420
평균을 구해야 하는데,

0:40:47.740,0:40:54.860
PyTorch에는 RMSPE 라는 metric이 없습니다.

0:40:54.860,0:41:00.020
그러면, 소스를 확인 해 보면 
한줄 정도로 구현이 가능해서

0:41:00.020,0:41:04.180
직접 간단하게 이를 만들어 볼 수 있을 것입니다.

0:41:04.180,0:41:10.260
그러나, 아마도 가장 쉬운 방법은

0:41:10.340,0:41:14.040
만약 a'/b 라는 것이 있을 때

0:41:14.040,0:41:18.260
이것을 ln(a'/b')로 바꾸면

0:41:18.260,0:41:22.940
ln(a') - ln(b')처럼
뺄셈 형태로 표현이 가능 해 집니다.

0:41:23.260,0:41:26.580
Log의 단순한 법칙을 적용한 것입니다.

0:41:26.580,0:41:30.555
이런 법칙을 잘 모르시면, 관련 내용을
확인 해 보시기 바랍니다.

0:41:30.560,0:41:32.880
아주 유용할 겁니다.

0:41:32.880,0:41:38.940
어쨋든 여기선, 제가 미리 Notebook에 작성 해 뒀는데

0:41:38.940,0:41:44.440
데이터 값에 Log를 취해 주기만 하면,

0:41:44.440,0:41:50.280
별다른 노력 없이
RMSE로 RMSPE를 구할 수 있게 됩니다.

0:41:51.040,0:41:53.820
단, 출력 할 땐

0:41:53.820,0:41:58.520
Log가 취해진 값을 EXP()에 넣어줘야 합니다.

0:41:58.520,0:42:03.320
그래야 Log가 벗겨지고 
실제 백분율 값을 구할 수 있게 됩니다.

0:42:03.320,0:42:08.860
지금 보고계신 코드가 이 모든 것을 수행하는데
딥러닝에 특화된 내용은 전혀 아닙니다.

0:42:09.200,0:42:12.715
그 다음 셀에서, 
딥러닝 관련 부분이 '드디어' 시작됩니다.

0:42:12.720,0:42:17.000
여느 때와 같이, 오늘 보시게 될 모든 것은

0:42:17.000,0:42:20.360
지금까지 봐오던 것과 정확히 똑같을 것입니다.

0:42:20.360,0:42:21.720
우선 처음엔,

0:42:21.780,0:42:25.780
학습/검증/테스트(옵션) 데이터셋이 포함된

0:42:25.840,0:42:29.260
모델 데이터 객체를 만들게 됩니다.

0:42:29.260,0:42:32.580
그리고 나선, learn 객체를 만들고

0:42:32.580,0:42:35.465
learn.lr_find()를 수행하고

0:42:35.465,0:42:38.345
learn.fit()을 수행하는데

0:42:38.345,0:42:42.440
파라메터등 모든 것은
전에 보신 것과 동일한 형태로 설정 됩니다.

0:42:42.860,0:42:46.180
다른점도 있긴한데,

0:42:46.180,0:42:49.835
ImageClassifierData 를 이용한

0:42:49.840,0:42:54.100
이미지 때와는 다른 방식으로 
데이터를 가져와야 합니다.

0:42:54.100,0:42:57.780
열과 컬럼으로 구성된 이런 데이터에 대해선

0:42:57.780,0:43:01.040
ColumnarModelData 라는 것을 사용하는데

0:43:01.040,0:43:05.395
이것에 의해 리턴된
객체는 이미 친숙하신 형태의 API를 제공합니다.

0:43:05.400,0:43:08.780
그리고, 이미지때 사용된, 
from_paths(), from_csv() 대신에

0:43:08.780,0:43:13.280
여기서는 from_data_frame()이 사용 되었고
몇 가지 인자값이 전달 되어야 합니다.

0:43:13.420,0:43:16.380
PATH라는 것은 모델등 관련 파일이

0:43:16.385,0:43:19.425
어디에 저장될 지를 명시 합니다.

0:43:19.425,0:43:21.955
나중에 저장할 때,

0:43:21.960,0:43:25.120
어디에 파일을 넣어둘지를 정하는 것입니다.

0:43:25.380,0:43:28.915
val_idx는 앞서 생성한 
검증 데이터셋으로써 사용하고자 하는

0:43:28.915,0:43:32.225
데이터들의 인덱스값에 대한 리스트 입니다.

0:43:32.540,0:43:38.920
df는 DataFrame 객체이고,

0:43:39.160,0:43:42.960
yl은 proc_df() 수행 후, 떨어져 나온 (Sales)

0:43:42.960,0:43:46.840
의존 컬럼 데이터값에 Log를 취한 값들 입니다.

0:43:46.840,0:43:50.280
그래서, yl 이라고 명명 하였고

0:43:50.520,0:43:54.760
모델 데이터를 만들 때,
의존 데이터가 뭔지도 알려줬습니다

0:43:54.940,0:43:59.000
다시 종합 해 보면
검증 데이터셋에 사용될 데이터 목록,

0:43:59.000,0:44:01.005
비의존 데이터와

0:44:01.005,0:44:03.965
의존 데이터가 뭔지를 명시 해 줬습니다.

0:44:03.965,0:44:08.180
그리고, 어떤 컬럼을 범주형으로
다뤄야 하는지도 알려줘야 합니다.

0:44:08.180,0:44:12.320
최종 변형된 DataFrame에선

0:44:12.360,0:44:15.000
모든 것이 숫자 이기 때문에,

0:44:15.000,0:44:18.760
모든 것을 계속형 데이터처럼 다루게 될 수 있고

0:44:18.760,0:44:21.380
그러면 의미 없는 결과가 나올 수 있습니다.

0:44:21.480,0:44:24.840
그래서, 뭐가 범주형 으로써 다뤄져야 하는지를
알려줄 필요가 있는 것이고,

0:44:25.040,0:44:30.860
여기서는 이미 정의된, 그것들의 목록을 
인자로써 전달 해 줬습니다.

0:44:30.940,0:44:34.440
그러면, 나머지 다른 많은 파라메터들은

0:44:34.440,0:44:36.240
예전과 동일합니다.

0:44:36.240,0:44:39.120
예를 들어서 bs로 
배치크기를 지정 해 줄 수 있을겁니다.

0:44:39.485,0:44:42.105
여기까지 수행하면,

0:44:42.105,0:44:46.560
일종의 표준적인 모델 데이터 객체가
만들어 지고

0:44:46.560,0:44:51.720
이미지 관련 데이터 객체에서 본 것 다르지 않게,

0:44:51.720,0:44:58.900
이 객체에도 
train_dl,  val_dl, train_ds, val_ds,  길이 속성 값 등이

0:44:58.900,0:45:03.120
존재 합니다.

0:45:04.200,0:45:07.820
그러면, 이번엔 모델 또는 learner라고 불리는 것을
만들어 줘야 합니다.

0:45:07.820,0:45:11.180
그 부분이 나오는 부분 까지 약간 건너 뛰어 보겠습니다

0:45:11.340,0:45:16.340
이미 꽤 친숙한 내용을 보실 수 있을 겁니다.

0:45:16.340,0:45:21.960
모델 데이터 객체의 
.get_learner() 로 상황에 적합한 모델을 만들어 줍니다.

0:45:22.420,0:45:26.760
이 함수엔 몇 가지 정보들을 넘겨줘야 하는데,

0:45:26.760,0:45:30.760
초반 계층에서 어느 정도의 dropout을 사용할지,

0:45:30.760,0:45:34.020
각 계층당 사용 할 activation의 갯수,

0:45:34.020,0:45:37.640
후반 계층들에서 어느 정도의 droupout을 사용할지

0:45:37.640,0:45:39.300
같은 것들이 포함됩니다.

0:45:39.800,0:45:43.220
그리고 그 외에도 몇 가지가 있는데, 이 것에 대해서
배워 보겠습니다

0:45:43.220,0:45:46.320
구체적으로 임베딩 이라고 불리는 (emb_szs 부분) 
것을 배워 볼 것입니다.

0:45:46.720,0:45:52.800
임베딩은 오늘 새로 배울 할 가장 중요한 컨셉 입니다.

0:45:56.280,0:46:00.180
(잘못 말함)

0:46:01.460,0:46:04.440
일단, 잠시만 범주형 데이터에 대한 것을 잊고

0:46:04.440,0:46:06.880
계속형 데이터에 대한 것만을 생각 해 봅시다.

0:46:07.240,0:46:11.340
계속형 데이터에 대해서 해야하는 건,

0:46:11.440,0:46:14.360
이 데이터들을 모두 들고와서

0:46:14.640,0:46:16.640
(정정)

0:46:16.900,0:46:25.440
이것은 모든 계속형 데이터에 대한 하나의 큰 배열 입니다.

0:46:25.740,0:46:28.460
첫 번째 요소는 최저온도, 
두 번째는 최고온도 …

0:46:28.485,0:46:31.735
가장 가까운 경쟁 가게의 거리등과 같이 말입니다.

0:46:31.735,0:46:35.215
리스트의 값들은 모두 소숫점 
실수형 값으로 채워져 있습니다.

0:46:35.420,0:46:37.280
이 것에 대해서,

0:46:37.280,0:46:41.100
뉴럴넷이 하게 되는 것이 뭐냐하면
이 1차원 배열를 입력 받게 되는데,

0:46:41.405,0:46:44.515
한편, 1차원 배열과 벡터,

0:46:44.520,0:46:49.680
그리고 1차원 텐서는 같은 말입니다.

0:46:49.840,0:46:52.180
이 1차원 텐서를 입력 받아서

0:46:52.180,0:46:55.440
행렬의 곱셈을 수행하게 됩니다.

0:46:55.915,0:46:58.435
예를 들어서, 그림의 배열은

0:46:58.665,0:47:01.655
20개의 계속형 데이터로 구성되어 있다면

0:47:01.655,0:47:04.635
이것과

0:47:04.640,0:47:10.880
20개의 열, 우리가 지정 해 줄수 있는 행 크기의
행렬을 곱셈 해 줍니다.

0:47:10.880,0:47:13.795
행의 크기를 100 정도로 설정 해 보겠습니다.

0:47:13.800,0:47:16.440
이 행렬 곱셈의 출력은 길이가 100인

0:47:16.440,0:47:20.500
새로운 1차원 텐서를 만들어 낼 것입니다.

0:47:20.820,0:47:25.820
이 결과는 단순히 행렬의 곱셈 공식에 의해서 나타난 것이고,

0:47:25.820,0:47:29.940
딥러닝의 선형적 계층에 대한 정의 이기도 합니다.

0:47:30.240,0:47:32.400
그리고 나서,

0:47:32.400,0:47:35.200
이 결과를 ReLU 함수에 넣어주게 됩니다.

0:47:35.400,0:47:38.200
음수 값들을 버리는 작업을 수행하게 된다는 의미 입니다.

0:47:38.200,0:47:40.975
ReLU의 결과를 가지고,

0:47:40.980,0:47:44.860
또 다시 또 다른 행렬과의 행렬 곱셈을
수행하게 할 수 있습니다.

0:47:44.860,0:47:47.680
이번에 곱해지는 행렬의 열의 크기는 100,

0:47:47.680,0:47:50.960
행의 크기는 우리가 원하는 만큼으로 정할 수 있습니다.

0:47:50.960,0:47:55.100
이번 계층이 마지막 계층이라고 가정 해 보죠. 
그러니까, 이 다음으로 하게 되는건

0:47:55.100,0:47:59.120
판매량을 측정하게 된다는 것으로,
따라서, 행의 크기를 1로 정해볼 수 있을 겁니다.

0:47:59.120,0:48:02.320
이것과 행렬의 곱을 수행하면,

0:48:02.320,0:48:06.300
그 결과로는 단일-숫자가 튀어나오게 됩니다.

0:48:06.640,0:48:14.820
이 과정은 단일계층으로 이뤄진 뉴럴넷 정도로
생각 해 볼 수 있을 것입니다.

0:48:15.100,0:48:20.000
현실적으론, 단일 계층만의 
뉴럴넷을 만들진 않습니다.

0:48:20.320,0:48:25.220
그렇기 때문에, 행의 크기를 50 정도로 바꿔 보겠습니다.

0:48:25.300,0:48:29.800
그러면, 그 결과는 크기가 50인 벡터가 되고,

0:48:29.980,0:48:38.420
이것에다가 50x1인 행렬을 곱해줄 수 있을 겁니다.

0:48:38.420,0:48:42.020
그러면, 마지막엔 단일-숫자가 예측 값으로 출력됩니다.

0:48:42.020,0:48:48.400
이렇게 바꾼 또 다른 이유는 마지막 계층에는 
보통 ReLU를 포함시키지 않기 때문입니다.

0:48:48.480,0:48:53.300
즉, 마지막의 음수 값들은 버리지 않겠다는 것인데

0:48:53.320,0:48:57.260
Softmax에는 낮은 확률 값들을 생성하는데

0:48:57.320,0:49:02.680
음수 값이 필요하기 때문입니다.

0:49:02.680,0:49:07.040
마이너한 내용이지만, 
한번 생각 해 보기에 유용 할 것입니다.

0:49:13.960,0:49:20.720
Fully Connected 뉴럴넷을 간단한 시각으로 설명 해 보면

0:49:20.860,0:49:26.780
1차원 텐서를 입력으로 받아서,

0:49:26.840,0:49:30.640
선형 계층 형태로 출력을 하고

0:49:30.900,0:49:33.380
이 결과에 활성화 계층인 ReLU를 적용하고,

0:49:33.440,0:49:36.140
또 다른 선형 계층을 만들어 냅니다.

0:49:36.300,0:49:39.320
그리고, Softmax를 수행하게 되면,

0:49:39.500,0:49:41.580
최종 출력 결과를 얻게 됩니다.

0:49:43.200,0:49:48.720
즉, 더 많은 선형 계층들을 추가 해 볼 수도 있습니다

0:49:48.800,0:49:51.980
또한, dropout을 추가 해 볼 수도 있을 것 입니다.

0:49:52.260,0:49:56.060
뭘 얼마나 추가 할 지에 대한 결정을 해야 하는 것이죠.

0:49:56.380,0:49:59.475
하지만 엄청나게 복잡한 아키텍쳐는 아니라서,

0:49:59.480,0:50:02.820
아주 많은 종류의 계층들 조합을 
시도 해 볼 내용은 없습니다.

0:50:02.820,0:50:05.175
코스 후반에 이미지 모델 부분을

0:50:05.175,0:50:08.265
다시 배울 때 즈음, 좀 더 복잡한,

0:50:08.265,0:50:10.915
ResNet 이나 Inception 네트워크 같은

0:50:10.920,0:50:13.240
모든 것들을 배우게 될 것입니다.

0:50:13.240,0:50:17.240
하지만, 이 fully connected 네트워크는 
아주 간단한 모양으로

0:50:17.280,0:50:19.960
행렬의 곱셈인 선형 계층,

0:50:19.960,0:50:22.600
ReLU와 같은 활성화 계층,

0:50:22.600,0:50:26.100
그리고 마지막엔 Softmax 가 배치된 형태 입니다.

0:50:26.960,0:50:29.940
그리고 지금 다루는 문제인 판매량의 예측처럼

0:50:29.940,0:50:32.800
분류(classification) 문제가 아니라면,

0:50:32.800,0:50:34.920
Softmax 조차 사용되지 않습니다.

0:50:34.980,0:50:38.380
예측 결과가 0~1이 되는것이 아니기 때문입니다.

0:50:38.380,0:50:40.900
그래서, 마지막에 있는 활성화 계층(Softmax)을

0:50:40.980,0:50:43.080
제거 해 볼 수 있습니다.

0:50:43.280,0:50:47.240
시간이 허락 됐다면, 
이때 사용 가능한 몇 방법을 소개 해 드릴 수 있었겠지만

0:50:47.240,0:50:49.760
일단은 이정도만 알고 넘어 가도록 하겠습니다.

0:50:50.260,0:50:52.795
지금까지 보여드린 것들은 모든것이

0:50:52.800,0:50:55.320
계속형 데이터라는 가정 하에 이뤄진 것입니다.

0:50:55.320,0:50:57.640
그러면, 범주형 데이터는 어떨까요?

0:50:58.520,0:51:03.500
예를 들어서 DayOfWeek 이라는 변수가 있는데

0:51:03.520,0:51:07.560
이 것을 범주형 데이터로 다룬다고 가정 해 봅시다.

0:51:07.560,0:51:11.720
0 - 토요일, 1 - 일요일, 2 - 월요일

0:51:11.920,0:51:16.380
6 - 금요일

0:51:17.080,0:51:19.200
어떻게 이 데이터를 입력으로 줘야

0:51:19.200,0:51:21.920
Fully connected 계층의 시작인

0:51:21.920,0:51:25.300
실수값으로 채워진 1차원 텐서 형태가 될 수 있을까요?

0:51:25.560,0:51:27.860
그 방법은 이러합니다.

0:51:27.860,0:51:30.940
일단 새로운 행렬을 만드는데,

0:51:31.100,0:51:34.680
7개의 열과

0:51:34.840,0:51:37.620
원하는 만큼의 행의 갯수로 이루어져 있습니다.

0:51:37.620,0:51:39.800
여기선 4로 설정 해 보겠습니다.

0:51:40.140,0:51:44.200
7개의 행과 …

0:51:44.200,0:51:47.700
4개의 컬럼이 있는 것이죠.

0:51:47.840,0:51:52.860
그 다음으로는 범주형 데이터들을 
마지막에 추가 해 줍니다.

0:51:52.860,0:51:56.580
예를 들어서, 데이터들의 첫 번째는 Sunday(1) 라고 해 보죠.

0:51:56.800,0:51:59.805
그러면, 이 Sunday를 표현하는 값이 뭔지를

0:51:59.805,0:52:04.000
방금 만든 행렬에서 찾아봐야 합니다.

0:52:04.000,0:52:07.660
이 1번 열 전체를 선택하게 되는 것이죠.

0:52:07.660,0:52:11.700
이 행렬은 기본적으로 실수 값들로 채워져 있고

0:52:12.080,0:52:17.260
결국 이 중 4개 실수 값의 부분집합을 선택 하였고,

0:52:17.440,0:52:21.560
1번 열, 즉, Sunday 를 위한 실수 값들이 들어 있을 겁니다.

0:52:21.560,0:52:23.920
이 방법으로

0:52:23.920,0:52:29.860
Sunday를 실수 값들로 채워진 1차원 텐서로
만들 수 있는 것이죠.

0:52:29.860,0:52:38.980
그리고, 최초엔 이 행렬의 모든 실수값들이 
무작위로 채워집니다.

0:52:39.080,0:52:43.920
선택된 4개의 숫자를 Sunday 대신에 집어 넣는 방식으로

0:52:43.920,0:52:49.080
이 랜덤 값들을 뉴럴넷에 집어 넣게 되고,

0:52:49.080,0:52:56.440
그러면 범주형 데이터를 실
수형 벡터로 변형한 것이 되는 것입니다

0:52:57.040,0:53:01.100
이렇게 뉴럴넷에 집어 넣고,

0:53:01.100,0:53:04.380
마지막에 손실을 측정하고

0:53:04.380,0:53:09.735
어느쪽이 하강 방향인지 알아내게 되고 (경사)

0:53:09.735,0:53:12.805
그 방향으로 경사 하강을 수행하면서

0:53:12.805,0:53:15.905
이 4개의 숫자까지 그 과정이 도달하게 될 겁니다.

0:53:15.905,0:53:18.905
그러면, 예를 들어서, 이 4개 숫자 값이

0:53:18.905,0:53:22.055
별로 좋지 않다고 판단될 때, 
첫 번째값은 약간 증가

0:53:22.055,0:53:25.165
두 번째도 증가, 세 번째는 감소, 네 번째는 증가

0:53:25.165,0:53:27.540
시켜야 한다는 계산이 이루어 지고

0:53:27.540,0:53:29.880
행렬의 4개 숫자 값들을 업데이트 하게 됩니다.

0:53:29.980,0:53:33.000
이 과정을 계속해서 반복, 반복, 반복하게 되면

0:53:33.005,0:53:35.875
이 숫자들이 어느 시점엔, 
더이상 무작위의 값들이 아니라

0:53:35.875,0:53:38.845
Sunday/Friday 등 각 요일에 대해서

0:53:38.845,0:53:41.835
가장 잘 동작할 수 있는 값들로

0:53:41.840,0:53:45.340
바뀌어 있을 것입니다.

0:53:45.800,0:53:48.540
다르게 표현 해 보면,

0:53:48.740,0:53:53.460
이 행렬은 결국 뉴럴넷에 있는 또 다른 
일종의 가중치 라고 볼 수 있습니다.

0:53:53.900,0:53:57.380
이런 종류의 행렬은

0:53:57.380,0:54:01.640
임베딩 행렬 이라고 불립니다.

0:54:03.920,0:54:06.940
임베딩 행렬은

0:54:06.940,0:54:15.300
0 부터 해당 범주 범위의 값으로 만들어지는데

0:54:15.300,0:54:18.515
이 행렬에서 특정 열의 번호는

0:54:18.515,0:54:21.975
해당 카테고리(범주)를 찾기위해 인덱스로 활용합니다.

0:54:22.040,0:54:25.260
만약 카테고리가 1일 때, 첫 번째 열을 선택하게 되고,

0:54:25.320,0:54:31.280
이 것을 계속형 데이터의 배열 끝에 추가 해 줍니다.

0:54:31.880,0:54:36.860
그러면 결국 변형된 계속형 데이터화 된 
새로운 벡터로 가공 됩니다.

0:54:36.880,0:54:40.620
다른 예로, 우편번호에 대해서도 
비슷한 것을 할 수 있습니다.

0:54:40.640,0:54:43.565
5,000개의 우편번호가 있다고 가정 해 보면,

0:54:43.565,0:54:46.695
행렬의 열의 개수는 5,000가 되고,

0:54:46.695,0:54:49.565
행의 크기는 우리가 결정해야 하는 것으로

0:54:49.565,0:54:52.385
50정도로 해볼 수 있습니다.

0:54:52.925,0:54:55.955
그리고 94003라는 우편번호에 대한

0:54:55.960,0:54:58.080
인덱스 번호가 4라면

0:54:58.080,0:55:01.995
행렬의 4번째 열을 선택해서, 
50개의 숫자를 선택할 수 있고

0:55:01.995,0:55:05.465
이것을 계속형 데이터 벡터의 
마지막에 추가 해 볼 수 있습니다.

0:55:05.775,0:55:07.885
그러면, 그 다음 과정은 동일합니다.

0:55:07.885,0:55:12.360
선형 계층 =>  ReLU => 선형 계층 => 등등 처럼 말이죠

0:55:13.140,0:55:16.145
>> 이 4개의 숫자가 표현하는게 뭔가요?

0:55:16.145,0:55:19.175
좋은 질문입니다. 
이 내용은 협동필터를 공부할 때,

0:55:19.175,0:55:20.995
좀 더 배우게 될 것입니다.

0:55:21.400,0:55:23.100
일단 지금은

0:55:23.100,0:55:29.380
뉴럴넷에 존재하는 다른 파라메터들과
전혀 다를 것이 없는 것입니다.

0:55:29.920,0:55:32.275
학습 종반엔 좋은 손실값을 줄 수 있어야 하는

0:55:32.275,0:55:35.275
학습되어야 하는 파라메터 라는 것입니다.

0:55:35.280,0:55:39.060
나중에 다뤄지겠지만, 
이 파라메터들은 꽤 흥미로운 구석이 있는데

0:55:39.060,0:55:42.040
종종 사람이 해석할 수 있는 형태인 경우가 있습니다.

0:55:42.040,0:55:46.040
하지만, 이런 특성은 이 것들이 가지는 부작용이 됩니다.

0:55:46.380,0:55:49.220
일단 지금은, 단순히 학습되어야 하는

0:55:49.220,0:55:53.240
4개의 숫자들의 집합이라고만 이해하시면 됩니다.

0:55:56.980,0:56:00.600
>> 임베딩 행렬의 차원 (행의 크기)를 
결정하기 위한 좋은 방법이 있나요?

0:56:00.600,0:56:02.520
>> 여기선 왜 4인가요?

0:56:02.520,0:56:05.560
물론 있습니다.

0:56:09.800,0:56:12.980
제가 처음에 했던 것은

0:56:12.980,0:56:15.940
모든 범주형 테이터와 이것들에 대한 cardinality의

0:56:15.940,0:56:18.180
목록을 만든 것입니다.

0:56:18.520,0:56:21.265
Rossman 에는

0:56:21.265,0:56:26.260
1116개의 등록된 가게들이 있으며

0:56:26.920,0:56:29.900
DayOfWeek 에는 8개 범위로 되어 있는데

0:56:29.905,0:56:33.005
월~일 이 7개고, 
나머지 하나는 unknwon 을 가리킵니다

0:56:33.005,0:56:36.085
unknown 값들이 원본 데이터셋에 없더라도,

0:56:36.085,0:56:39.115
저는 일반적으로 unknown 카테고리를 추가 해 줍니다.

0:56:39.120,0:56:42.500
테스트 데이터셋에서 아직 보지못한 
뭔가가 발견될지도 모르기 때문이죠

0:56:42.960,0:56:45.755
마찬가지로 Year에는 3개 종류의 년 + 하나는 unknown

0:56:45.755,0:56:48.595
이런 식으로 되어 있습니다.

0:56:48.600,0:56:50.420
그래서,

0:56:50.420,0:56:53.360
저의 경험적인 법칙은 이렇습니다.

0:56:54.160,0:56:59.300
각 범주형 데이터 마다의 Cardinality 값을 
2로 나눠주고

0:56:59.320,0:57:02.020
나눠진 값을 50 이하로 만들어 줍니다.

0:57:04.620,0:57:07.940
이렇게 해서 구성된 임베딩 행렬입니다.

0:57:07.940,0:57:13.060
Store에 대한 행렬은 1,116개의 열로 구성되고,

0:57:13.060,0:57:17.200
이 열을 인덱스로 삼아서 가게를 검색합니다.

0:57:17.200,0:57:22.180
가게를 찾게 되면, 그 가게에 대한 크기가 50인
1차원 텐서를 가져오는 것입니다.

0:57:22.180,0:57:24.955
마찬가지로, DayOfWeek은
8개의 종류에서 검색을 한 후,

0:57:24.960,0:57:28.760
크기가 4인 1차원 텐서를 가져오게 되죠.

0:57:30.700,0:57:36.040
>> 각 범주형 컬럼들에 대해서, 
임베딩 행렬을 만드는 것이 일반적인가요?

0:57:36.040,0:57:39.600
네.
지금까지 한 것을 정리 해 보면,

0:57:40.660,0:57:42.840
for c in cat_vars 로,

0:57:42.840,0:57:45.740
각 범주형 컬럼 마다

0:57:45.760,0:57:48.460
cardinality를 구했습니다.

0:57:49.140,0:57:52.125
그리곤, cardinality 정보로,

0:57:52.125,0:57:54.895
이런 내용을 만들어 냈는데

0:57:55.325,0:57:58.535
임베딩 크기 목록 (emb_szs) 이라고 불립니다.

0:57:58.540,0:58:03.660
이 임베딩 크기 정보는 모델을 만들 때, 
첫 번째 인자값으로 대입 해 줘야 합니다.

0:58:03.840,0:58:06.740
그러면, 만약 범주형 데이터가 포함된 경우

0:58:06.760,0:58:10.100
그 범주형 데이터에 대한 
임베딩 행렬을 만들기 위한 정보로써 활용 됩니다.

0:58:13.735,0:58:16.915
>> 무작위의 초기화 이외에

0:58:16.920,0:58:21.040
>> 임베딩 행렬을 초기화 하는 다른 방법이 있나요?

0:58:23.420,0:58:26.680
그럴 수도, 아닐 수도 있습니다. 두 가지 방법이 있거든요.

0:58:26.680,0:58:28.740
하나는 무작위 방식이고,
다른 하나는 미리 학습된 방식 입니다.

0:58:28.940,0:58:32.180
아마도 미리 학습된 방식에 대해서 
좀 더 설명을 드리게 될 것입니다.

0:58:32.185,0:58:35.045
기본적인 컨셉은 만약 누군가가 Rossman 데이터에 대해서

0:58:35.045,0:58:37.975
뉴럴넷을 이미 학습 시켰을 수 있다는 것입니다.

0:58:37.975,0:58:40.805
개나 고양이를 구분하기 위해 ImageNet에 대해서

0:58:40.805,0:58:43.835
미리 학습된 모델을 사용하는 것과

0:58:43.835,0:58:46.815
Rossman 에서의 치즈 판매량을 예측하기 위해서

0:58:46.820,0:58:52.620
Rossman 에서의 술 판매량에 대한 
임베딩 행렬을 활용하는 것과 유사합니다.

0:58:52.620,0:58:58.460
Pinterst와 Instacart 에서 이 방식이 사용되고 있습니다.

0:58:58.720,0:59:03.580
Instacart는 쇼핑객들의 경로를 위해서,

0:59:03.580,0:59:08.080
Pinterest는 웹페이지에 뭐를 보여줄지 결정하기 위해서
이 방식을 사용하고 있습니다.

0:59:08.100,0:59:14.020
Instacart는 기관 내부에, 상품과 가게에 대한 
임베딩 행렬을 보유/공유하고 있기 때문에,

0:59:14.020,0:59:20.000
이것을 학습시켜서 새로 만들 필요가 없습니다.

0:59:23.860,0:59:27.620
>> 임베딩 크기에 대해서,

0:59:27.620,0:59:32.240
>> 원-핫 방식을 사용하지 않는 이유는 뭔가요?

0:59:32.340,0:59:36.080
>> 알려주신 방식의 이점이 뭔가요?

0:59:36.080,0:59:37.660
좋은 질문입니다.

0:59:37.660,0:59:40.060
지적하신 것 처럼,

0:59:40.100,0:59:45.040
이 각 범주마다 4개의 숫자를 사용하지 말고,

0:59:45.040,0:59:48.720
전체 범주를 통틀어 모든 값이 0, 그러나 해당 범주의 값은 1인

0:59:48.725,0:59:51.795
길이 7의 배열을 사용 할 수도 있습니다.

0:59:51.795,0:59:54.755
물론, 이때 값들은 실수 입니다.

0:59:56.855,0:59:59.825
일반적으로 말하면,

0:59:59.825,1:00:02.905
이 방식이 그동안 범주형 데이터가
통계학에서 오랫동안 사용 되어 온,

1:00:02.905,1:00:06.840
더미 데이터 인코딩 이라고 불리는 것이었습니다.

1:00:06.840,1:00:08.720
이 방식에서 찾을 수 있는 문제점은

1:00:08.840,1:00:12.680
Sunday 라는 개념은

1:00:12.680,1:00:17.440
오직 하나의 실수값과 조합(연관)이 가능하다는 것입니다.

1:00:17.740,1:00:19.500
그러니까,

1:00:19.500,1:00:23.215
Sunday가 한가지 기준으로만 비교되어 
크거나 작거나가 판단되는 것 같이,

1:00:23.220,1:00:26.740
일종의 선형적인 동작 모양새를 보여주게 됩니다.

1:00:27.200,1:00:30.075
반면, 임베딩의 개념을 사용하면,

1:00:30.075,1:00:33.165
Sunday가 4차원적 공간으로 표현되게 됩니다.

1:00:33.165,1:00:38.280
이때 이 임베딩 행렬들은 더 풍부한 개념을 담아내는

1:00:38.280,1:00:42.060
경향이 있다는 것을 알게 되었습니다.

1:00:42.060,1:00:47.160
예를 들어서, 만약 주말(Fri, Sat, Sun)이 다른 날과 비교해서

1:00:47.160,1:00:51.060
다른 결과를 보여주는 경향이 있다면,

1:00:51.065,1:00:54.105
이 주말을 담아내는 숫자들 중 몇몇이

1:00:54.105,1:00:57.115
다른 것들보다 더 높은 값을 가지게 될 수 있을 겁니다.

1:00:57.115,1:01:00.065
또는, 한 주의 특정 날들이

1:01:00.065,1:01:05.820
기름이나 우유같이 일상적인

1:01:05.820,1:01:13.500
어떤 상품의 높은 판매율과 연관성이 있거나

1:01:13.520,1:01:21.080
와인과 같이 휴일이나 주말에 주로 판매되는

1:01:21.160,1:01:28.240
어떤 상품에 대한 판매율과 연관성이 있거나 할 수 있는데

1:01:28.240,1:01:31.620
임베딩 행렬의 특정 부분이

1:01:31.700,1:01:34.595
이러한 의미를 담아내서

1:01:34.595,1:01:37.635
사람들이 쇼핑을 하는지 안하는지를

1:01:37.640,1:01:40.240
파악하게 될 수 있겠죠

1:01:40.240,1:01:43.855
단일 숫자 대신에 이런 고차원적인 벡터를 사용하면,

1:01:43.855,1:01:46.915
딥러닝 네트워크로 하여금

1:01:46.915,1:01:49.875
더 풍부한 표현을 배울 수 있는

1:01:49.875,1:01:52.815
기회를 제공 해 주게 됩니다.

1:01:52.815,1:01:55.465
임베딩이란 아이디어는

1:01:55.465,1:01:59.480
뉴럴넷에서의 가장 기초적인 컨셉 중 하나인

1:01:59.480,1:02:03.400
분산표상 (distributed representation) 이라고도 불립니다.

1:02:04.060,1:02:07.500
분산표상이란 뉴럴넷이

1:02:07.545,1:02:10.935
고차원적인 표현법을 가진다는 것인데,

1:02:11.320,1:02:16.180
이런 벡터의 숫자들은 한가지의 의미만 가지는게 아니라

1:02:16.180,1:02:23.000
각 숫자들의 크고 작음에 따라서 
다른 의미를 내포할 수 있기 때문에

1:02:23.000,1:02:28.200
종종 매우 해석되기가 매우 어렵습니다.

1:02:29.060,1:02:35.260
이런 풍부한 표현법이 아주 흥미롭게도 
입력과 예측결과간의

1:02:35.280,1:02:39.520
관계를 학습할 수 있게 해 주는 것입니다.

1:02:48.380,1:03:08.420
>> 임베딩 행렬이라는 것에 알맞은 
특정 종류의 데이터가 있나요?

1:03:08.420,1:03:11.560
임베딩은 어떤 종류든 
범주형 데이터면 알맞습니다.

1:03:12.560,1:03:17.720
단, cardinality가 아주 크면,

1:03:17.780,1:03:20.835
잘 동작하지 않을 수 있습니다.

1:03:20.835,1:03:24.025
만약, 600,000개의 데이터가 있는데

1:03:24.025,1:03:28.220
극단적으로 범주의 갯수도 600,000개가 존재한다면,

1:03:28.220,1:03:33.280
이 데이터들은 범주형으로썬 유용하지 않을 수 있습니다.

1:03:37.680,1:03:40.560
하지만, 이 경연의 3등을 차지하신 분이 한

1:03:40.560,1:03:43.600
데이터 엔지니어링 결과를 보면

1:03:43.600,1:03:47.020
너무 큰 cardinality가지는 데이터가 
없다고 판단 하였고,

1:03:47.025,1:03:49.935
이들을 모두 범주형으로 다루기로 결정 하였습니다.

1:03:49.935,1:03:53.025
경험적인 법칙으로, 
만약 범주형 데이터를 만들 수 있다고 생각되면

1:03:53.025,1:03:55.105
그렇게 하시라는 것입니다.

1:03:55.105,1:03:58.055
왜냐하면 아주 풍부한 표현이 
학습되는 방법이기 때문입니다.

1:03:58.055,1:04:01.205
반면에, 계속형 데이터로써 다루게 된다면,

1:04:01.205,1:04:05.800
단일 함수 형태를 찾는 것 밖에 하지 못하게 될 것입니다.

1:04:07.040,1:04:10.080
>> 차원을 늘려야 한다고 말씀하셨는데,

1:04:10.085,1:04:13.105
>> 사실상 대부분 상황에선

1:04:13.105,1:04:18.120
>> 원-핫 인코딩을 사용할 것 같다고 생각합니다

1:04:18.120,1:04:24.160
>> 왜냐하면, 원-핫 인코딩의 차원이 사실상 더 크기 때문이죠.
(카테고리 개수가 50개 이상이면, 더 커짐)

1:04:24.160,1:04:28.360
좋은 지적인 것 같습니다.

1:04:29.145,1:04:31.885
원-핫 인코딩이

1:04:31.885,1:04:35.165
사실상 고차원이긴 하지만,

1:04:35.280,1:04:38.800
어떤 의미가 있는 고차원은 아닙니다. 
한가지 요소를 제외하면 모두 값이 0이기 때문이죠.

1:04:38.800,1:04:41.215
>> 제가 이 말을 하는건,

1:04:41.220,1:04:46.100
>> 원-핫 인코딩을 사용하면
메모리 사용량같은 것들이 줄어들 것 같아서 입니다.

1:04:46.100,1:04:50.135
맞는 말입니다.
행렬에 대한 선형대수를 처리하는

1:04:50.135,1:04:53.075
내부 동작원리를 설명드려 보겠습니다.

1:04:53.080,1:04:57.120
이 설명이 잘 이해 안된다면, 그냥 건너 뛰셔도 되지만,

1:04:57.120,1:04:59.800
몇 분들에겐 꽤 도움이 될 것 이라고 생각합니다.

1:05:01.080,1:05:05.620
Sunday 라는 것으로 시작할 때,

1:05:05.660,1:05:10.560
이 것을 원-핫 인코딩된 벡터로 표현할 수 있습니다.

1:05:11.320,1:05:14.405
그러니까, Sunday의 위치는 여기 쯤이면,

1:05:14.405,1:05:18.160
이 부분만 값이 1이고
나머지는 모두 0으로 채워지는 형태입니다.

1:05:19.140,1:05:24.440
그리고, 오른쪽에는 임베딩 행렬을 그려 보겠습니다.

1:05:24.720,1:05:28.960
8개의 열과, 4개의 행으로 구성된다고 생각 해 보죠.

1:05:32.100,1:05:35.960
여기서 행렬의 곱셈을 떠올려 볼 수 있을 겁니다.

1:05:35.960,1:05:38.480
그 전에, 원핫 인코딩 벡터를 일종의 색인 용도로

1:05:38.480,1:05:40.520
사용 가능하다고 말씀 드렸었죠

1:05:40.660,1:05:42.700
그러면, 임베딩 행렬에서

1:05:42.700,1:05:45.480
색인 번호에 해당하는 부분의 
배열을 찾을 수 있습니다.

1:05:45.760,1:05:49.340
근데 한번 생각 해 보면, 
이 과정은 원-핫 인코딩된 벡터와

1:05:49.340,1:05:52.440
임베딩 행렬간의 행렬의 곱셈을 수행한 것과

1:05:52.440,1:05:55.460
완전히 일치하는 결과를 출력합니다.

1:05:55.760,1:05:59.280
0번째 것과 여기 줄을 곱해주고,

1:05:59.280,1:06:01.080
1번째 것을 여기 줄에 곱해주고,

1:06:01.100,1:06:04.045
2번째 것을 여기 줄에 곱해주고

1:06:04.045,1:06:09.800
이 둘의 행렬 곱셈은 색인 해서 
찾아가는 과정과 동일합니다.

1:06:10.440,1:06:14.660
예전에 몇 사람들은 실제로 원-핫 인코딩 후,

1:06:14.660,1:06:19.500
임베딩 행렬과의 곱셈을 수행했고,

1:06:19.500,1:06:24.440
여전히 많은 머신러닝 메소드들이 이와 같은
방법을 사용합니다.

1:06:24.860,1:06:29.880
하지만, 아주 비-효율적인 부분이 발견 되었고,

1:06:29.880,1:06:33.960
모든 현대 라이브러리들은 색인 번호를 가지고

1:06:34.020,1:06:38.100
바로 임베딩 행렬의 해당 부분을 
찾아 가도록 구현되었습니다.

1:06:38.100,1:06:42.000
수학적으로는 이것이 행렬의 곱셈이라는 것을
알게 됐다는 것이 좋은 점 인데,

1:06:42.000,1:06:45.480
경사의 흐름을 명확히 
이해할 수 있게 해 주기 때문입니다.

1:06:45.480,1:06:47.880
확률적 경사하강을 수행할 때,

1:06:47.920,1:06:51.780
이것을 단순히 
또 다른 선형 계층으로 생각 해 볼 수 있게 해 줍니다.

1:06:52.960,1:06:55.875
약간 마이너한 내용이지만,

1:06:55.880,1:06:57.640
몇 분들에겐 도움이 되었길 바랍니다.

1:06:59.160,1:07:02.165
>> 날짜(date)와 시간(time)을 범주형 으로서 다루는 법과

1:07:02.165,1:07:05.175
>> 그때, 계절의 변동에 어떻게 
영향을 미치는지 알려주실 수 있나요?

1:07:05.180,1:07:08.780
물론이죠. 좋은 질문 감사합니다.

1:07:12.740,1:07:16.635
Date 관련 내용은 머신러닝 코스에서 아주 많이 다룹니다.

1:07:16.640,1:07:20.700
하지만, 간단하게 여기서도 언급하는게 의미 있을 것 같군요.

1:07:23.140,1:07:24.940
fastai 라이브러리 중에,

1:07:24.940,1:07:27.640
add_datepart 라는 함수가 있는데

1:07:28.140,1:07:31.500
Data Frame과, 
컬럼 명을 파라메터로 전달 해 줘야 합니다.

1:07:31.540,1:07:34.440
그리고 그 컬럼 명은, 
여기선 Date가 되어야 합니다.

1:07:34.960,1:07:37.755
drop 값이 false가 아니라면,

1:07:37.755,1:07:40.885
Data Frame에서 그 컬럼을 삭제 합니다.

1:07:40.885,1:07:44.035
그리고, 삭제된 컬럼을 그 Date에 대해서

1:07:44.040,1:07:47.240
유용한 정보를 포함하는
여러개의 컬럼으로 대체합니다.

1:07:47.240,1:07:48.465
예를 들어서,

1:07:48.465,1:07:51.685
DayOfWeek (요일), DayOfMonth (월간일),
MonthOfYear (연간월)

1:07:51.685,1:07:54.735
사분기 시작점인지, 사분기 종료시점인지 등의

1:07:54.735,1:07:57.085
추가 컬럼들이 포함됩니다.

1:08:00.580,1:08:05.100
그 결과를 feature 목록에서 확인 해 볼 수 있습니다.

1:08:05.440,1:08:11.620
이 목록들이 add_datepart 에 의해서 생성 된 것들입니다.

1:08:12.300,1:08:16.740


1:08:17.200,1:08:21.520
DayOfWeek 에 대해서,

1:08:21.520,1:08:27.000
8열/4행 크기의 임베딩 행렬이 만들어 지게 되는데,

1:08:27.320,1:08:30.175
개념적으로 보면,

1:08:30.175,1:08:33.075
모델로 하여금 아주 흥미로운

1:08:33.075,1:08:35.985
시계열적 모델을
생성하도록 해 줍니다.

1:08:35.985,1:08:41.300
만약 어떤일이 7일을 주기로 일어난다면,

1:08:41.360,1:08:45.420
예를 들어서, 낮 시간동안 베를린에서만 
월요일에는 상승했다가

1:08:45.620,1:08:49.100
수요일에는 감소하는 뭔가가 
있을 수 있을 것입니다.

1:08:49.100,1:08:52.895
여기 데이터가 그런 것을 감지하기 위한
모든 정보를 포함하고 있고요.

1:08:52.900,1:08:57.860
이 방법은 시계열을 다루기에 
아주 좋은 방법으로

1:08:57.860,1:09:01.260
질문을 해 주셔서 매우 감사합니다.

1:09:01.260,1:09:04.840
단 한가지 확실히 해 둬야 하는 것은

1:09:04.845,1:09:07.875
시계열에 대한 시간의 주기는
하나의 컬럼으로 표현이 되어야 합니다.

1:09:07.880,1:09:11.720
만약 DayOfWeek 이라는 컬럼이 없다면,

1:09:11.720,1:09:15.940
뉴럴넷으로 하여금 직접 이와 비슷한 값을 계산해서

1:09:16.000,1:09:21.180
임베딩 행렬을 색인하기란 
매우 어려운 일이 될 것입니다.

1:09:21.180,1:09:23.125
불가능은 아니더라도,

1:09:23.125,1:09:25.205
매우 많은 계산량이 필요한데다

1:09:25.205,1:09:27.105
완벽하게 해내기도 어려울 것입니다.

1:09:27.700,1:09:35.020
한가지 생각해보면 좋은 예로는 
Holiday(휴일) 을 생각 해 볼 수도 있을 겁니다

1:09:35.020,1:09:41.200
또는, 샌프란시스코의 음료수 판매량을 예측하고 싶다면

1:09:41.200,1:09:47.120
AT&T 경기장에서의 야구게임이 있는 날들의 
목록이 필요 할 지도 모릅니다.

1:09:47.120,1:09:51.660
그 사실이 얼마나 많은 사람들이 
맥주를 마실지에 대한 영향이 있기 때문입니다.

1:09:51.660,1:09:54.515
어쨋든, 시간의 주기를 나타내는 데이터가

1:09:54.520,1:09:58.680
데이터셋에 포함되어 있는지를
확실시 해 주시기 바랍니다.

1:09:58.680,1:10:00.420
그렇기만 한다면,

1:10:00.425,1:10:03.435
뉴럴넷은 이 정보를 사용해서 학습하게 될 것입니다.

1:10:04.380,1:10:09.740
.. 딥러닝 관련 없는 부분들을 약간 건너 뛰려고 합니다.

1:10:11.820,1:10:16.560
Data Frame으로 부터 만든 모델 데이터를 만들고,
이를 이용해서 모델을 만들 때

1:10:16.560,1:10:21.640
임베딩 행렬의 크기 리스트 정보가 필요 합니다 (emb_szs)

1:10:22.000,1:10:32.400
또한, 계속형 컬럼이 몇개나 있는지 정보도 필요한데,

1:10:32.540,1:10:43.460
전체 컬럼 갯수 - (빼기) 범주형 컬럼 계산으로
이 갯수로 구할 수 있습니다

1:10:43.460,1:10:45.220
이 갯수 정보로,

1:10:45.280,1:10:48.275
뉴럴넷은 계속형과 범주형 데이터를 담아내기 위한

1:10:48.280,1:10:52.720
내부적인 포맷을 만들어 낼 수 있게 됩니다.

1:10:55.020,1:10:58.500
임베딩 행렬은 독자적인 dropout을 가질 수 있습니다.

1:10:58.520,1:11:01.840
세번째 파라메터로 이 dropout에 대한 값이 주어져야 합니다.

1:11:02.020,1:11:05.320
다섯째 파라메터의 배열 첫 번째 요소는 
첫 번째 선형 계층의 activation 갯수,

1:11:05.320,1:11:08.180
두 번째 요소는 두 번째 선형 계층의 
activation 갯수를 나타냅니다.

1:11:08.220,1:11:12.360
여섯째 파라메터에는 각 선형 계층의 
dropout들의 리스트가 주어져야 합니다.

1:11:12.360,1:11:15.300
마지막 파라메터는 일단 
지금은 다루지 않겠습니다.

1:11:15.300,1:11:17.640
네번째 파라메터는 최종 출력 계층의 
출력 갯수를 의미합니다.

1:11:17.720,1:11:22.040
여기선 판매량의 단일-숫자 값을 예측하고자 하기 때문에,

1:11:22.040,1:11:25.040
값을 1로 설정 하였습니다.

1:11:26.800,1:11:30.895
이렇게 모델/learner를 생성 할 수 있었고,

1:11:30.900,1:11:36.040
이 객체에서 lr_find() 를 호출하여, 
사용 할 학습률을 알아 봅니다.

1:11:36.440,1:11:38.240
그리고나선,

1:11:38.240,1:11:43.860
전에 본 API와 동일한 형태를 사용해서 
모델에 대한 학습을 시작하였습니다.

1:11:43.860,1:11:46.360
모두 똑같은 형태 입니다.

1:11:46.835,1:11:49.765
전에 보셨는지 모르겠지만,

1:11:49.765,1:11:52.765
fit() 호출시 커스텀 metric을
파라메터로 넘길 수 있습니다.

1:11:52.765,1:11:55.785
그러면, 명시한 metric 함수를 호출하여

1:11:55.785,1:12:00.160
매 epoch 마다 제일 마지막 부분에 
함수의 리턴된 숫자를 출력하게 됩니다.

1:12:00.320,1:12:05.680
metric 함수는 앞에서 정의한 것으로,

1:12:05.740,1:12:11.360
RMSPE를 계산하는 함수였습니다.
(Root Mean Squared Percentage Error)

1:12:11.940,1:12:14.980
학습 자체에 어떤 변화를 주는 것이 전혀 아니라,

1:12:14.980,1:12:17.500
단순히 뭔가를 출력하기 위한 것입니다.

1:12:17.740,1:12:21.300
약간 학습을 진행 하였는데,

1:12:21.320,1:12:24.840
대회 3등의 사람들은 fastai 에서 제공되는

1:12:24.840,1:12:29.260
재시작하는 확률적 경사하강법 같은 것들에 의한

1:12:29.260,1:12:32.980
성능향상을 하지 못했기 때문에,

1:12:33.080,1:12:37.560
결과를 비교 해 보면 좋을 것입니다.

1:12:40.655,1:12:43.445
비록 우리가 사용한 검증 데이터셋이

1:12:43.445,1:12:46.595
테스트 데이터셋과 동일 하지는 않ㅂ지만,

1:12:46.600,1:12:50.340
학습 데이터셋의 마지막 2주간의 
데이터로 구성한 것이라서

1:12:50.340,1:12:52.915
매우 유사 할 것입니다.

1:12:52.920,1:12:56.660
학습의 결과를 보시면, 
 0.09711 가 나왔는데

1:12:56.940,1:13:01.800
비교 해 보기 위해서, 
Kaggle 의 리더보드로 이동 해 보겠습니다.

1:13:05.900,1:13:14.840
(점수 보는 중..)

1:13:14.840,1:13:18.060
약간 신기한데요, 
Public과 Private 리더보드 사이엔

1:13:18.060,1:13:20.520
큰 차이점이 있어 보이는 군요.

1:13:20.520,1:13:23.780
Private 리더보드에선 
상위권에 오를 수 있는 결과지만

1:13:23.780,1:13:26.555
Public 리더보드에선 
상위 30~40위권 정도의 결과가 되는군요

1:13:26.560,1:13:28.660
왜 그런지 확실치는 않지만,

1:13:28.840,1:13:33.700
어쨌든 우리의 결과가
꽤 상위권에 랭크 될 수 있는걸 알 수 있습니다.

1:13:34.120,1:13:38.200
저는 사실 3등한 분의 코드를 구해서 직접 돌려 봤습니다.

1:13:38.280,1:13:42.600
그 결과는 0.1 보다 약간 컸는데
이 결과로 보면

1:13:42.600,1:13:47.220
아마도 Private 리더보드의
3등을 하신 분의 것 같습니다.

1:13:48.420,1:13:51.640
어찌되었든, 여러분은

1:13:51.640,1:13:55.155
시계열 데이터와 구조화된 데이터를 다루기 위한

1:13:55.160,1:13:58.700
방법이 있다는 것을 배우셨습니다.

1:13:58.700,1:14:03.140
3등을 차지한 분들은 결과에 대한 논문을 적었는데,

1:14:03.140,1:14:06.400
해당 링크가
Notebook에 포함되어 있습니다.

1:14:06.920,1:14:08.580
이들의 결과를,

1:14:08.580,1:14:11.860
경연의 우승자와 
2등한 사람들과 비교 해 보면,

1:14:11.860,1:14:15.440
1/2등은 훨씬 더 많은 Feature 엔지니어링을 
수행 했었습니다.

1:14:15.520,1:14:22.080
우승자는 사실상 판매량 예측 분야의 전문가인데,

1:14:22.080,1:14:27.580
수 많은 feature를 
만들어 내기 위한 코드를 작성해서 사용 했습니다.

1:14:27.645,1:14:30.545
또한 매우 유사한 모델을 만든 Pinterest의

1:14:30.545,1:14:34.480
추천 시스템을 개발한 엔지니어와
이야기 해 본 적이 있습니다.

1:14:34.480,1:14:39.380
이들은 Gradient Boosting machine에서 
딥러닝으로 갈아탔는데,

1:14:39.820,1:14:44.580
그 이유는 훨씬 더 적은
Feature 엔지니어링의 노력이 필요했던점,

1:14:44.580,1:14:49.060
훨씬 간단한 모델을 만들 수 있었던점,
더 적은 유지/보수 노력이 드는점이라고 하더군요.

1:14:49.060,1:14:51.860
최신예 수준의 결과를 얻음과 동시에,

1:14:51.860,1:14:54.640
훨씬 적은 작업량을 요하는 이러한 이유들이

1:14:54.640,1:14:58.320
딥러닝의 사용 이점이라고 볼 수 있을 것입니다.

1:15:02.540,1:15:07.580
>> fit() 수행시에, 시계열 데이터를 사용했나요?

1:15:07.680,1:15:10.635
간접적으로 사용 했습니다. 
당연히 사용 했습니다.

1:15:10.640,1:15:16.300
DayOfWeek, MonthOfYear 같이
아까 보신 모든 컬럼을 사용했고

1:15:16.580,1:15:20.200
이 컬럼들의 대부분은 범주형으로써 다뤄 졌었습니다.

1:15:20.200,1:15:28.540
이 컬럼 데이터를 가지고, 1월/2월/… 등등에 대한 
분산표상이 만들어 집니다.

1:15:28.540,1:15:33.140
전통적인 시계열 기법을 사용하는게 아니라,

1:15:33.140,1:15:39.620
오직 뉴럴넷의 fully connected 계층만을 사용 했습니다.

1:15:40.420,1:15:44.040
>> (질문 못 알아 들음)

1:15:44.040,1:15:49.440
그렇습니다. 다른 어떤 표준적인 시계열 기법보다

1:15:49.480,1:15:54.680
임베딩 행렬은 DayOfWeek 같은 것들의 주기성을

1:15:54.680,1:15:58.760
더 풍부한 형태로 다룰 수 있습니다.

1:15:59.280,1:16:06.560
>> CNN 에서 사용된 행렬(커널) 같은 것이

1:16:06.560,1:16:13.520
>> fit()의 파라메터로 전달되지 않나요?

1:16:14.080,1:16:18.400
여기서도 fit()에는 아무것도 전달하지 않았습니다.

1:16:18.400,1:16:23.080
단지 학습률, cycle 갯수, 
metric 리스트등만을 전달했습니다.

1:16:23.740,1:16:30.820
다만, get_learner()를 호출할때엔 다른점이 있습니다.

1:16:31.280,1:16:34.040
이미지 처리에서는

1:16:34.040,1:16:39.780
pretrained() 에 데이터를 전달 해서
모델을 만들어 줬었습니다.

1:16:40.320,1:16:48.540
그런데, 모델이라는 것은 사실상
데이터에 어떤 의존성을 가집니다.

1:16:48.540,1:16:52.595
그래서, 구조화된 데이터에 대한 모델을 만들땐

1:16:52.595,1:16:55.605
어떤 임베딩 행렬을 사용할 것인지 같은 정보가 
별도로 필요했던 것입니다.

1:16:55.605,1:16:59.480
그리고 여기서는 데이터 모델 객체가 
모델을 생성하기 위한 방법을 제공합니다.

1:16:59.480,1:17:02.380
이미지 모델 생성과는 정 반대의 사용법이죠.

1:17:04.780,1:17:10.700
>> 약간 혼동되어서 요약 해 보려고 하는데요,

1:17:10.700,1:17:13.635
>> 이 경우엔,

1:17:13.635,1:17:16.645
>> 일종의 구조화된 데이터를 가지고

1:17:16.645,1:17:19.445
>> feature 엔지니어링을 해서

1:17:19.445,1:17:23.280
>> 컬럼으로 표현되는 데이터를 만들어 냈고

1:17:23.500,1:17:26.780
그냥 pandas의 Data Frame 이라고 하셔도 됩니다.

1:17:26.780,1:17:29.100
>> 네. Data Frame을 만들어 냈습니다.

1:17:29.100,1:17:32.600
>> 그리고 이 Data Frame과 딥러닝을 매핑하는데 …

1:17:32.600,1:17:36.120
임베딩 행렬을 사용 했죠.
범주형 데이터에 대해서 말이죠.

1:17:36.120,1:17:38.935
계속형 데이터는 그냥 넣어줬죠.

1:17:38.940,1:17:43.620
>> 그러니까, feature 엔지니어링이 되어 있다는 가정하에

1:17:43.660,1:17:48.920
>> 이 데이터를 딥러닝에 매핑하기 위해 제가 해야 하는건

1:17:48.920,1:17:53.360
>> 어떤것을 범주형으로 다뤄야 할지를 
생각해야 하는것 뿐인가요?

1:17:53.360,1:17:56.400
좋은 질문입니다.
네 맞습니다.

1:17:56.400,1:18:00.035
만약 여러분 자신만의 데이터셋에 적용 해 보고 싶으시면

1:18:00.040,1:18:05.600
첫 번째 단계는, 
범주형 컬럼과 계속형 컬럼의 목록을 만드는 것입니다.

1:18:05.640,1:18:09.900
그리고 이것들을 pandas의 Data Frame에 넣어줘야 합니다.

1:18:10.100,1:18:13.780
두 번째 단계는

1:18:13.800,1:18:19.080
검증 데이터셋으로 사용 할 
인덱스의 목록을 만드는 것입니다.

1:18:19.920,1:18:22.320
세 번째 단계는,

1:18:22.320,1:18:25.200
ColumnarModelData.from_data_frame 코드를 
수행하는 것입니다.

1:18:25.240,1:18:29.240
지금 보시는 라인을 그대로 사용하셔도 됩니다.

1:18:29.760,1:18:31.680
네 번째 단계는,

1:18:31.680,1:18:36.040
각 컬럼당 임베딩 행렬의 크기에 대한 
리스트를 만드는 것입니다.

1:18:36.720,1:18:39.820
다섯번째 단계는, 
get_learner()를 호출하는 것입니다.

1:18:39.840,1:18:43.560
여기 보시는 파라메터를 그대로 사용하셔도 됩니다.

1:18:43.560,1:18:47.780
만약 과적합 또는 희소적합이 발생하면, 
약간 값들을 바꿔보시면 될 것입니다.

1:18:47.780,1:18:51.540
마지막 단계는, fit()을 수행하는 것입니다.

1:18:51.540,1:18:56.620
보여드린 거의 모든 코드의 내용이 
거의 바뀔 필요 없이 사용 되어도 괜찮을 것입니다.

1:18:59.600,1:19:02.660
>> 두 가지 질문이 있는데요,

1:19:02.660,1:19:04.260
>> 첫 번째 질문은

1:19:04.260,1:19:07.940
>> data augmentation이 이 경우에서는 어떻게 사용될 수 있을까요?

1:19:07.940,1:19:13.400
>> 두 번째 질문은 여기서의 dropout 역할은 뭔가요?

1:19:13.620,1:19:17.820
Data augmentation 관련해서는 
매우 흥미로운 질문이긴 하지만,

1:19:17.820,1:19:20.460
저도 잘 모르겠습니다.

1:19:20.740,1:19:24.360
제 생각엔 문제마다 
도메인에 특화된 방식이 사용되어야 할 것 같은데,

1:19:24.360,1:19:27.120
아직까진 구조화된 데이터를 사용한 딥러닝에서는 
어떤 논문도, 산업의 다른 분들도

1:19:27.120,1:19:28.900
data augmentation을 하는걸 본 적이 없습니다.

1:19:29.680,1:19:33.220
어떤 수가 있을 수도 있겠지만, 
아직 그런걸 본 적은 없습니다.

1:19:33.240,1:19:35.660
dropout의 역할이 뭐냐구요?

1:19:35.660,1:19:38.700
전과(이미지) 정확히 똑같습니다.

1:19:38.840,1:19:44.220
각 지점에서…

1:19:44.320,1:19:50.940
각 선형 계층들의 출력은 
단순히 1차원 텐서입니다.

1:19:51.000,1:19:56.580
dropout은 이 텐서의 명시된 일부분의 
activations을 버리는 역할을 하게 됩니다.

1:19:57.020,1:20:00.195
그리고, 가장 첫번째인 
임베딩 행렬에 대한 dropout은

1:20:00.200,1:20:04.680
말 그대로 임베딩 행렬에 대해서,

1:20:04.760,1:20:10.520
부분 부분의 activations들을 버리는 일을 하게 됩니다.

1:20:11.920,1:20:17.340
잠시 쉬는시간을 가지겠습니다.

1:20:19.400,1:20:23.140
(휴식을 가진 후)

1:20:23.140,1:20:27.860
모두들 감사합니다.

1:20:27.960,1:20:34.200
그러면 지금부터 뭔가 흥미로운 것을 배울텐데요,

1:20:34.200,1:20:38.780
그 전에 쉬는시간에 몇 가지 좋은 질문을 받았는데

1:20:38.780,1:20:40.940
이에 대해서 이야기 해 볼까 합니다.

1:20:41.320,1:20:44.460
거의 누구도 제가 보여드린 방법을 
사용하진 않습니다.

1:20:44.460,1:20:48.440
그러면 이 방법의 단점은 무엇일까요?

1:20:49.280,1:20:52.180
전에 논의된 것 처럼,

1:20:52.180,1:20:56.680
대학에서 누구도 이 방법에 대해서

1:20:56.680,1:21:01.940
연구를 하지 않고 제출되는 논문도 많이 없습니다.

1:21:02.900,1:21:05.920
그런 이유로 이 방법이 잘 동작 하는지에 대해서

1:21:05.920,1:21:08.740
사람들이 볼 기회가 없었기에,

1:21:08.740,1:21:12.580
이 방법을 어떤 회사가 구현하고자 한 적도 없는등
이 방법을 사용한 좋은 사례가 없습니다.

1:21:12.580,1:21:17.420
하지만, 또 중요한 사실은 
지금까지 fastai 라이브러리를 사용하는 것 처럼

1:21:17.480,1:21:22.055
이 것을 이만큼 편리하게 할 수 있는 방법이 없었습니다.

1:21:22.055,1:21:25.095
만약 이러한 모델들을 구현하고 싶었다면, 관련된

1:21:25.100,1:21:28.700
모든 코드를 직접 작성 해야 했습니다.

1:21:28.700,1:21:31.740
반면에, fastai를 사용하면

1:21:31.740,1:21:36.215
말씀드린 것 처럼 6개의 단계 만으로

1:21:36.220,1:21:40.520
거의 6줄의 코드 만으로  이게 가능 해 집니다.

1:21:41.740,1:21:44.755
이 말씀을 드리는 이유는

1:21:44.755,1:21:47.725
상업적으로나 과학적으로나

1:21:47.725,1:21:50.645
아직까지 해결되지 못한 문제들에 대해서

1:21:50.675,1:21:53.575
이 방법을 사용 할 수 있는

1:21:53.575,1:21:56.425
많은 기회들이 있을 것이라는 겁니다.

1:21:56.740,1:22:00.800
만약 여러분들 중 누군가 이런 시도를 해본다면

1:22:00.800,1:22:04.420
저는 그 소식에 대해서 누구보다 흥미로울 것 같습니다.

1:22:04.840,1:22:07.875
과거에 있었던 Kaggle 경연들 중에서,

1:22:07.875,1:22:10.725
이 방법을 사용 했더라면 
우승할 수 있었을 것이라던지

1:22:10.725,1:22:16.120
GBM이나 Random Forest 같은 것을 적용하고 계신

1:22:16.140,1:22:20.460
데이터에 대해서 이 방법을 사용 해서 
도움이 된다던지 말이죠.

1:22:21.460,1:22:24.395
저도 이 쪽으로는 사실 얼마 되진 않았습니다.

1:22:24.400,1:22:27.300
2018년 초에 구조화된 데이터에

1:22:27.300,1:22:31.840
딥러닝을 적용하는 강의를 준비하면서 시작 했으니까요.

1:22:31.860,1:22:36.780
그래서 저도 이 방식이 실패할만한 상황을 
접할 기회가 많이 없었습니다.

1:22:36.780,1:22:40.660
지금까지 시도한 문제들에 대해서는 
실패한 적이 없었거든요

1:22:41.020,1:22:44.920
이번 수업에서 처음으로

1:22:44.920,1:22:50.620
약 6명정도가 그룹으로 이 문제에 대해서 
작업을 하고 있는데,

1:22:50.620,1:22:55.480
여러분들이 뭔가 만들고 시도해 보면 
흥미로울 것이라고 생각합니다.

1:22:55.480,1:23:00.360
해 보신 것들에 대해서 
기술 블로그를 적어보시는 것도 좋을 것이구요

1:23:00.980,1:23:05.540
staccator 에서 진행된 연구 관련 글과

1:23:05.540,1:23:10.320
Pinterest 에서 O’Reiliy AI 비디오에 대한 
연구 관련 글이 있는데

1:23:10.320,1:23:13.200
이 내용과 비슷한 것입니다.

1:23:13.200,1:23:15.500
그리고 관련 내용의 두 편의 논문도 있는데

1:23:15.500,1:23:18.360
모두 Kaggle 경연의 우승자들이 작성한 것입니다.

1:23:18.360,1:23:22.040
그 중 하나는 Yoshua Bengio 그룹이 작성한 것인데

1:23:22.040,1:23:25.960
택시 목적지를 예측하는 경연에서 우승을 차지했고

1:23:25.960,1:23:29.360
다른 하나는 여기서 다룬 Rossman 경연에 관한 것으로

1:23:29.360,1:23:31.840
Notebook 에 링크가 있습니다.

1:23:32.220,1:23:35.460
약간의 백그라운드 내용을 설명 드렸고,

1:23:35.460,1:23:39.060
지금부터는 자연어 처리(NLP)를 
다뤄 보도록 하겠습니다.

1:23:39.100,1:23:46.260
NLP는 딥러닝에서 가장 유망한 분야 중 하나로,

1:23:46.260,1:23:50.520
컴퓨터 비젼에 비해서 약 2~3년 정도 늦게

1:23:50.520,1:23:54.380
딥러닝 적용 연구가 시작되었습니다.

1:23:54.380,1:23:58.200
하지만 딥러닝이 유명해지게 해 준 
두 번째 분야 입니다.

1:23:58.720,1:24:05.200
컴퓨터 비젼 분야의 최신예 기술의 대부분은

1:24:05.280,1:24:11.160
2012, 2014년도에 등장했습니다.

1:24:11.160,1:24:15.920
하지만 딥러닝의 다른 많은 분야와 마찬가지로

1:24:15.920,1:24:19.980
NLP 분야는 아직 최신예 수준에 
도달하지 못했습니다.

1:24:19.980,1:24:22.680
알게 되실 내용이지만,

1:24:22.720,1:24:25.800
컴퓨터 비젼 분야에 비해서

1:24:25.800,1:24:30.640
다른 분야들의 소프트웨어와 개념들은 
훨씬 덜 성숙되어 있습니다.

1:24:30.645,1:24:33.435
그래서, 일반적으로

1:24:33.445,1:24:36.255
저희가 컴퓨터 비젼 다음으로 다루는

1:24:36.255,1:24:38.655
어떤 내용도 컴퓨터 비젼만큼

1:24:38.655,1:24:41.545
안정적으로 정착되어 있는 것은 없습니다.

1:24:41.545,1:24:44.960
NLP에서 흥미로운 것 중 하나는 지난 몇 달동안

1:24:44.960,1:24:47.945
컴퓨터 비젼에서 발전된 좋은 아이디어들이

1:24:47.945,1:24:50.920
처음으로 NLP 분야에 영향을 주기 시작했고,

1:24:50.920,1:24:53.900
큰 발전을 가져다 주고 있다는 것입니다.

1:24:53.900,1:24:57.520
그래서, 여러분들이 NLP에서 보게 될 
많은 내용들은 꽤 새로운 것들입니다.

1:24:57.520,1:25:01.960
일단, 특정 NLP 관련 문제로 시작을 해보려고 합니다.

1:25:01.960,1:25:05.240
NLP 에서는 해결 가능한 특정 문제들이 있고,

1:25:05.240,1:25:08.240
이 문제들에는 특정 이름이 있습니다.

1:25:08.240,1:25:12.500
그래서, NLP 에 있는 특정 문제 중, 
언어 모델링(language modeling) 이라는 것이 있는데

1:25:12.500,1:25:15.295
아주 구체적인 문제에 대한 정의를 가지고 있습니다.

1:25:15.300,1:25:19.440
그 내용은 몇 단어로 이뤄진 문장이 주어졌을 때,

1:25:19.440,1:25:23.280
그 다음 단어를 예측 가능한 모델을 생성하는 것입니다.

1:25:24.300,1:25:27.640
예를 들어, 스마트폰을 사용해서 문자를 작성할 때,

1:25:27.640,1:25:31.600
스페이스 키를 누르면
 그 다음에 사용될 단어를 알려주곤 하는데,

1:25:31.600,1:25:34.480
SwiftKey(어플) 라는게 비슷한 일 아주 잘 합니다.

1:25:34.480,1:25:37.720
그리고, SwiftKey도 사실상 딥러닝을 사용하고 있습니다.

1:25:37.820,1:25:41.260
이런 것이 언어 모델링 이라는 것입니다.
아주 구체적인 의미를 가지죠.

1:25:41.265,1:25:44.265
언어 모델링 이라는 것에 대해서 이야기할 때,

1:25:44.265,1:25:47.740
문장의 다음 단어를 예측하는
모델을 만드는 것을 의미한다는 것입니다.

1:25:47.860,1:25:51.040
또 다른 예를 한가지 말씀드리겠습니다.

1:25:51.040,1:25:56.380
저는 아카이브(arvix)에서 
18개월치의 논문을 다운로드 했었습니다.

1:25:56.600,1:26:01.880
아카이브가 뭔지 모르시는 분들을 위해 보여드리면,

1:26:01.880,1:26:08.400
가장 인기있는 견본인쇄 서버로 
수 많은 논문들이 보관 되어 있습니다.

1:26:09.200,1:26:12.020
저는 각 논문들에 대한

1:26:12.020,1:26:15.520
초록(abstract)과 주제(topics)를 수집 했습니다.

1:26:15.525,1:26:18.575
여기서 그 중의 한 예를 보여주고 있습니다.

1:26:18.580,1:26:20.940
이 논문의 카테고리 <cat> 은 csni로,

1:26:20.940,1:26:23.960
Computer Science NetworkIng 
라는 의미 이고

1:26:23.960,1:26:27.180
요약내용 <summ> 은 논문의 초록 부분입니다.

1:26:27.220,1:26:31.260
(초록 부분을 읽음)

1:26:32.020,1:26:38.860
이 것이 (<cat>, <summ> 포함) 언어 모델에 사용되는 
텍스트의 한 예 라고 볼 수 있습니다.

1:26:39.360,1:26:43.840
저는 다운로드된 아카이브 데이터에 대한 
언어 모델을 학습 시켰고

1:26:44.100,1:26:47.940
그리고선, 간단한 테스트 코드를 만들었는데,

1:26:48.020,1:26:53.000
일종의 기폭 텍스트 (트리거) 를 통해서 작동합니다.

1:26:53.000,1:26:59.200
<CAT> csni <SUMM> algorithms that 
이라는 글귀로 시작하는

1:26:59.200,1:27:05.380
텍스트라는 단서를 가지고, 
모델에게 초록을 적어달라고 요청하게 됩니다.

1:27:05.820,1:27:08.840
그 결과를 보시면....

1:27:09.060,1:27:17.260
(모델이 생성한 텍스트를 읽고 있음..)

1:27:17.260,1:27:19.240
라고 생성 되었습니다.

1:27:19.560,1:27:22.020
모델은 아카이브 데이터에서

1:27:22.020,1:27:26.460
누군가 
<cat> csni <summ> algorithms that 라고 적은

1:27:26.460,1:27:32.960
논문들을 읽으면서 학습 되었습니다.

1:27:32.980,1:27:37.480
영어를 전혀 모르는 채로 시작 했다는 것을
알아 두셔야 합니다.

1:27:37.485,1:27:40.495
사실상 영어의 모든 단어에 대하여
무작위로 초기화된

1:27:40.500,1:27:43.140
임베딩 행렬을 가지고 시작을 한 것입니다.

1:27:43.140,1:27:46.720
그리고, 아카이브의 
수 많은 논문들을 읽으면서

1:27:46.720,1:27:49.780
그 다음으로 나타날 단어들이 
뭐가 될지를 학습을 하였습니다.

1:27:49.780,1:27:54.980
이번엔, <cat> cscv <SUMM> algorithms that 라는 글귀로
(Computer Science Computer Vision)

1:27:54.980,1:27:57.600
텍스트 생성 요청의 결과를 확인 해 봤습니다.

1:27:57.660,1:28:10.500
(모델이 생성한 텍스트를 읽고 있음..)

1:28:10.500,1:28:16.300
보시다 시피 전의 결과와 거의 동일한 문장입니다.

1:28:16.340,1:28:21.660
하지만, 그 내용이 컴퓨터 네트워크 대신 
컴퓨터 비젼에 대한 것으로 채워져 있습니다.

1:28:21.880,1:28:28.160
이번에는 <cat> cscv <SUMM> algorithms. <TITLE> on 이라는 텍스트를 주고,

1:28:28.160,1:28:34.400
타이틀인 <TITLE> on 다음에 오는 
텍스트를 생성 해 보았습니다.

1:28:34.920,1:28:38.660
(모델이 생성한 텍스트를 읽고 있음..)
‘이미지 분류를 위한 딥러닝의 성능에 대해서...’

1:28:38.660,1:28:41.420
<eos> 는 end of sentence 로, 
타이틀의 끝을 나타냅니다.

1:28:41.420,1:28:44.640
<cat> csni <SUMM> algorithms. <TITLE> on 
의 경우는

1:28:44.645,1:28:47.685
(모델이 생성한 텍스트를 읽고 있음..)
‘무선 네트워크의 성능에 대해서..’

1:28:47.685,1:28:50.565
<cat> cscv <SUMM> algorithms. <TITLE> towards 
의 경우는

1:28:50.565,1:28:53.375
(모델이 생성한 텍스트를 읽고 있음..)
‘이미지 분류에 대한 새로운 접근법을 향해서...’

1:28:53.375,1:28:56.335
<cat> csni <SUMM> algorithms. <TITLE> towards 
의 경우는

1:28:56.340,1:28:59.380
(모델이 생성한 텍스트를 읽고 있음..)
’무선 네트워크의 분석에 대한 새로운 접근법을 향해서...’

1:28:59.520,1:29:02.160
제 생각엔 꽤나 신나는 결과물인 것 같습니다.

1:29:02.160,1:29:09.220
전혀 미리학습되지 않은 무작위 값의 행렬을 가지고

1:29:09.280,1:29:13.880
18개월치의 아카이브 논문을 학습 시켰더니

1:29:13.880,1:29:17.895
영어 문장을 꽤 잘 적어내는 방법 뿐만 아니라,

1:29:17.900,1:29:21.220
convolutional neural networks 같은 말 
다음의 ( ) 괄호 속에

1:29:21.220,1:29:24.380
cnn 이라는 약어도 넣어주고

1:29:24.440,1:29:29.340
컴퓨터 비젼이나

1:29:29.340,1:29:39.340
네트워크라는 상황에 맞는 문장을 만들어 줬습니다.

1:29:39.340,1:29:42.580
언어 모델의 내부는

1:29:42.580,1:29:48.240
엄청나게 심오하고 미묘한 것일 수 있습니다.

1:29:48.400,1:29:52.300
저희가 만들 언어 모델이 그러한 것입니다.

1:29:52.300,1:29:54.840
하지만 이 예제의 결과물을 위해서가 아니라,

1:29:54.840,1:29:57.420
미리 학습된 모델을 만들어내서,

1:29:57.420,1:30:00.340
다른 문제들에도 적용하기 위한 모델을 만들 것입니다.

1:30:00.340,1:30:03.200
실제로 우리가 해결하고자 하는 문제는
IMDB의 영화 리뷰를 보고,

1:30:03.205,1:30:06.305
그 리뷰가 긍정인지 부정인지를 알아내는 것입니다.

1:30:06.560,1:30:10.700
일종의 고양이/개 분류 문제와 유사합니다.

1:30:10.700,1:30:16.700
이미지 대신에, 리뷰에 대한 텍스트가 대상이 되는 것입니다.

1:30:17.640,1:30:20.405
그러니까 이를 위해선, 적어도 영어를 읽을 줄 아는

1:30:20.405,1:30:25.720
미리 학습된 모델을 사용해서 
이 문제를 시작 하고 싶은 것입니다.

1:30:26.700,1:30:31.940
여기서, 영어를 읽을 줄 안다는 것의 의미는

1:30:31.940,1:30:35.220
문장의 다음 단어를 
예측할 수 있어야한다는 것입니다.

1:30:35.320,1:30:39.380
그렇다면 언어 모델을 미리 학습시키고,

1:30:39.380,1:30:42.660
그 미리 학습된 언어 모델을 사용하면 
어떻게 된다는 걸까요?

1:30:42.660,1:30:45.500
컴퓨터 비전 에서와 마찬가지로,

1:30:45.500,1:30:49.460
몇 새로운 계층들을 마지막에 추가한 후
다음 단어를 예측하는 것 대신에

1:30:49.460,1:30:53.240
문장의 긍정/부정을 구분 해 볼 수 있을 겁니다.

1:30:53.580,1:30:56.460
제가 이 방식에 노력을 하고 있던 당시,

1:30:56.460,1:30:58.680
이는 사실상 새로운 아이디어 였습니다.

1:30:58.680,1:31:01.195
불행히도, 지난 몇 달간 제가 작업하는 동안,

1:31:01.200,1:31:04.880
몇 사람들이 이 아이디어에 대해서 
논문을 발표 해버렸지요.

1:31:04.880,1:31:10.480
완전히 새로운 아이디어에서, 
약간 신선한 아이디어 정도가 된 것입니다.

1:31:11.000,1:31:14.675
지금부터 저희가 배우게 될 것은

1:31:14.680,1:31:18.040
언어 모델을 학습 시켜서,

1:31:18.040,1:31:22.220
분류 문제를 위한 미리 학습된 모델을 
만들어 내는 것입니다.

1:31:22.220,1:31:23.840
그리고 이 내용은

1:31:23.905,1:31:26.965
강력한 분류 모델을 만들기 위해서

1:31:26.965,1:31:30.275
미세 조정 하는 법을 공부 했던

1:31:30.280,1:31:34.340
컴퓨터 비젼에서 배운 내용으로 부터 
영향을 받게 될 것입니다.

1:31:35.300,1:31:43.020
>> 왜 직접적으로는 왜 안하나요? 잘 동작하지 않는가요?
(미리-학습된 모델)

1:31:44.320,1:31:47.995
경험적으로 그렇지 않습니다.

1:31:48.000,1:31:53.060
그리고 그렇지 않은데는 몇 가지 이유가 있습니다.

1:31:53.480,1:31:55.600
우선, 아시다 시피

1:31:55.600,1:32:00.300
미리 학습된 네트워크를 
미세조정하는 것은 매우 효과적 입니다.

1:32:00.300,1:32:05.740
만약 관련된 첫 번째 작업에 대해서 학습을 시키면,

1:32:05.740,1:32:08.540
그 학습된 모든 정보를 사용해서

1:32:08.560,1:32:11.960
두 번째 작업을 풀어나가는데 
도움을 줄 수 있을 겁니다.

1:32:12.220,1:32:15.220
또 다른 이유는

1:32:15.260,1:32:17.940
IMDB 영화 리뷰는

1:32:17.940,1:32:21.380
1,000개 이상의 단어로 이뤄진 긴 문장들 입니다.

1:32:21.380,1:32:31.120
영어 언어의 구조나 단어들의 의미를 전혀 모른 상태에서

1:32:31.120,1:32:34.840
이 많은 단어들을 읽은 후의 결과는

1:32:34.840,1:32:39.740
1(긍정) 또는 0(부정)이라는 정수값이 될 텐데

1:32:39.740,1:32:43.400
영어 문장 전체를 배우고,

1:32:43.400,1:32:46.860
이것이 긍정이냐 부정이냐를 단일 숫자로
표현하는 법을

1:32:46.860,1:32:50.055
바로 배우라는 것은 너무 지나친 기대일 것입니다.

1:32:50.060,1:32:54.280
먼저 언어 모델을 만듦으로써,

1:32:54.300,1:32:58.980
일종의 영화 리뷰의 영어를 
이해할 수 있는 뉴럴넷을 만들 수 있게 되고

1:32:59.060,1:33:02.040
이 언어 모델에서 학습된 내용이

1:33:02.040,1:33:07.100
긍정과 부정을 판단하는데 
유용할 것이라고 기대해 볼 수 있는 것입니다.

1:33:07.240,1:33:09.280
좋은 질문 이었습니다.

1:33:11.020,1:33:16.500
>> Karpathy가 보여준 Char-RNN 과 유사한가요?
(Karpathy는 딥러닝 연구자)

1:33:16.760,1:33:21.880
네. Karpathy가 보여준 유명한 Char-RNN과 
유사한 면이 있습니다.

1:33:21.880,1:33:25.580
Char-RNN은 이전에 등장한 몇 개의 문자를 가지고

1:33:25.580,1:33:29.320
다음 문자를 예측하는 것입니다.

1:33:29.320,1:33:32.540
언어 모델은 일반적으로, 
단어 단위로 동작하긴 하지만

1:33:32.540,1:33:34.860
꼭 그럴 필요는 없습니다.

1:33:34.860,1:33:36.900
하지만 단어 단위로 동작하는 것이

1:33:36.900,1:33:39.740
더 효과적일 수 있을 것입니다.

1:33:39.740,1:33:43.440
따라서 이 코스에서 저희는 
단어 단위의 모델링만을 다룰 예정입니다.

1:33:44.020,1:33:49.680
>> 생성된 단어/문장들이 어느 정도로

1:33:49.680,1:33:54.780
>> 학습 데이터셋에 있는 것을 
그대로 복사해 오는 것인가요?

1:33:54.780,1:34:00.180
>> 완전히 랜덤한 것인가요?

1:34:00.640,1:34:02.740
좋은 질문입니다.

1:34:02.740,1:34:05.740
문자 단위가 아니기 때문에,

1:34:05.740,1:34:09.260
단어들은 분명히 학습 때 본 것들로 
이루어 질 것입니다.

1:34:09.260,1:34:11.860
전에 본 단어들 만을 포함시킬 수 있다는 거죠.

1:34:11.960,1:34:15.075
문장의 경우엔, 이를 엄격하게 
판단하는 방법이 있긴 하지만

1:34:15.075,1:34:18.265
가장 쉬운 방법은 결과의 예를 
들여다 보는 것입니다.

1:34:18.265,1:34:21.195
두 가지 다른 <CAT>로 된 문장의 시작에 대해서,

1:34:21.195,1:34:24.425
매우 유사한 컨셉을 생성했지만

1:34:24.425,1:34:27.705
각 상황에 맞게 잘 섞어줬다는 걸 알 수 있는 것 처럼요.

1:34:28.300,1:34:36.180
문장이 생성된 후, 
전에 생성된 것들을 다시 훑어보고

1:34:36.180,1:34:44.700
동일하거나 유사한 문장이 있었는지 
확인 해 볼 수 있을 겁니다

1:34:44.960,1:34:49.235
또 다른 방법은 언어 모델을 학습 시킬 때,

1:34:49.240,1:34:52.500
우리는 검증 데이터셋을 가지고 있습니다.

1:34:52.500,1:34:55.260
즉, 예측 단계에서,

1:34:55.260,1:34:58.180
아직 전에 보지 못한 단어의 
다음 단어를 예측하게 될 수 있는데

1:34:58.185,1:35:01.395
만약 그 결과가 좋다면 
텍스트를 생성능력이 좋다고 판단 가능합니다.

1:35:01.500,1:35:06.160
하지만, 여기서의 목적은 
텍스트를 생성하는게 아니기 때문에

1:35:06.280,1:35:10.360
그 부분을 너무 많이 다루진 않을 생각입니다.

1:35:10.360,1:35:15.340
하지만, 어떤 소설을 학습 시킨 다던지 해서

1:35:15.340,1:35:18.840
직접 해 보실 순 있겠죠.

1:35:19.460,1:35:22.540
사실, 언어 모델을 사용해서

1:35:22.540,1:35:25.760
텍스트를 생성하기 위한 몇 기법이 있는데

1:35:25.760,1:35:28.315
여기서 사용되진 않지만 아주 간단한 것이어서

1:35:28.315,1:35:31.475
포럼에서 관련된 이야기를 나눠볼 수 있을 것입니다.

1:35:31.480,1:35:34.960
제가 초점을 맞추려는 내용은
“분류” 문제 라는 것이고

1:35:34.960,1:35:38.840
엄청나게 효과적인 분야라는 것을 알아 주세요.

1:35:39.140,1:35:41.660
텍스트 분류 관련의 예로,

1:35:41.700,1:35:44.940
헤지펀드를 한다고 가정 해 보죠.

1:35:44.940,1:35:49.160
로이터나, 트위터 등에서 
모든 기사들이 발행될 때 마다, 이를 읽어보고

1:35:49.160,1:35:51.255
즉각적으로

1:35:51.260,1:35:55.780
과거에 어떤 주식 폭락이 있었는지 
식별하고 싶을 수 있을 겁니다.

1:35:55.780,1:35:58.220
이것은 “분류” 문제 입니다.

1:35:58.605,1:36:01.475
또 다른 예로는

1:36:01.475,1:36:04.385
고객 서비스 센터에 들어오는 질문(불만)들이

1:36:04.385,1:36:07.295
다음 달, 멤버쉽 탈퇴하는 사람들과의

1:36:07.295,1:36:10.215
어떤 연관성이 있는지를 식별하고 싶을 수 있습니다.

1:36:10.220,1:36:14.300
이 또한 “분류” 문제 입니다.

1:36:15.100,1:36:18.560
또한, 문서를 분류하려고 할 때,

1:36:18.560,1:36:25.400
이 문서가 적법한 자료인지 안닌지를 분류하는 등

1:36:25.400,1:36:33.620
이런 문제들에 대해서 매우 효과적입니다.

1:36:33.780,1:36:38.080
이젠, 대충 텍스트 분류가 뭔지를 아시겠죠?

1:36:38.400,1:36:42.260
이제 코드 레벨로 들어가서 보면,
몇 가지 라이브러리를 임포트 하고 있습니다.

1:36:42.260,1:36:47.720
그 중 torchtext 라는 것에서 
여러개를 임포트 하고 있는데,

1:36:47.820,1:36:53.040
torchtext 라는 건 
PyTorch에 있는 NLP를 위한 라이브러리 입니다.

1:36:53.040,1:36:56.380
그리고, fastai 는 torchtext와 연관되어 동작하도록

1:36:56.380,1:36:58.460
디자인 되어 있습니다.

1:36:58.760,1:37:02.020
다음으로는 fastai 에서 정의한 텍스트 처리를 위한

1:37:02.020,1:37:05.440
라이브러리의 몇 부분들을 임포트 하고 있습니다.

1:37:05.740,1:37:08.745
지금부터 IMDB 의 거대한 
영화 리뷰 데이터를 가지고

1:37:08.745,1:37:11.145
작업을 하게 될 것입니다.

1:37:11.440,1:37:15.380
이 데이터는 학계에서 아주 많이 연구가 된 것이고

1:37:15.380,1:37:20.220
수 년동안 수 많은 사람들이 연구를 해 왔습니다.

1:37:20.360,1:37:23.520
이 데이터는 50,000개의 리뷰를 포함하고 있으며

1:37:23.520,1:37:26.840
이 리뷰들은 정서에 따라서 극으로(긍정/부정)

1:37:26.840,1:37:29.980
갈려서 분류되어 있습니다.

1:37:30.420,1:37:35.220
우선은 언어 모델을 만들게 될 것입니다.
즉, 일단은 텍스트의 정서적 내용은 무시 할 것입니다.

1:37:35.220,1:37:38.895
그저 고양이/개 분류 문제에서 처럼, 
한가지 일을 위해서 모델을 미리 학습 시킨 후

1:37:38.900,1:37:42.140
다른 작업을 시키기 위한 미세 조정을 하게 될 것입니다.

1:37:42.380,1:37:47.420
NLP에서 이런 아이디어 (미리학습)는 매우 새로운 것이어서

1:37:47.420,1:37:50.600
다운로드 가능한 모델이 없어서

1:37:50.600,1:37:53.580
우리 스스로가 모델을 만들어야 합니다.

1:37:54.100,1:37:57.440
여기 있는 링크를 이용해서 
데이터를 다운로드 하였다면,

1:37:57.440,1:37:59.980
데이터의 PATH를 지정 하고

1:37:59.980,1:38:03.720
학습, 검증 데이터의 PATH를 지정하는 등
일상적인 작업을 해 줘야 합니다.

1:38:04.080,1:38:08.380
보시면 알겠지만, 컴퓨터 비젼에서 본 것과 
꽤나 유사한 형태로

1:38:08.380,1:38:12.875
train/ test/ 라는 디렉토리가 있는 걸 알 수 있습니다.

1:38:12.875,1:38:15.985
단, 여기서는 test/와 val/을 
별도로 구분짓진 않았습니다.

1:38:16.075,1:38:19.275
그리고 컴퓨터 비젼에서와 마찬가지로

1:38:19.280,1:38:22.980
train 디렉토리 안에는 
많은 파일이 들어 있습니다.

1:38:22.980,1:38:25.515
단지 이 파일들이 이미지가 아니라,

1:38:25.520,1:38:28.000
영화 리뷰라는 것이 다른 점입니다.

1:38:28.440,1:38:31.860
그 파일들 중 하나의 내용을 들여다 보면 
(cat 명령어 사용)

1:38:31.860,1:38:36.040
지금 보시는 것은 좀비 관련 영화에 대한 
리뷰로 그 내용이 ...

1:38:36.040,1:39:02.620
… (읽고 있음)…

1:39:02.940,1:39:05.960
기본적인 UNIX 명령어를 사용해서,

1:39:05.960,1:39:09.820
전체 데이터에서 단어 수(유니크는 아님)가 
몇 개나 있는지를 알아 봅니다.

1:39:09.820,1:39:13.455
학습 데이터셋에서 총 17,000,000 이상의 
단어가 발견 되었고

1:39:13.460,1:39:18.280
테스트 데아터셋에선 총 5,600,000 이상의 
단어가 발견 되었습니다.

1:39:22.400,1:39:25.445
IMDB에서 가져온 데이터로,

1:39:25.445,1:39:28.475
무작위의 아무개가 작성한 내용 입니다.

1:39:28.480,1:39:32.660
제가 아는 한 뉴욕타임즈 같이 
선정된 리뷰가 아닙니다.

1:39:36.080,1:39:39.460
텍스트를 가지고 뭔가를 해보기 전에,

1:39:39.465,1:39:42.445
일단은 텍스트를
토큰들의 목록으로 만들어 줘야 합니다.

1:39:42.445,1:39:45.675
토큰이란 기본적으로 단어 같은 것 입니다.

1:39:45.675,1:39:48.215
최종적으로는 숫자들의 목록을 
만들어 줘야 하는데,

1:39:48.215,1:39:50.865
그러기 위한
첫 번째 단계로 단어들의 목록을 만들어야 하고

1:39:50.865,1:39:53.835
NLP에서는 이를 tokenization(토큰화) 한다고 표현합니다.

1:39:53.840,1:39:57.340
NLP에는 엄청나게 많은 용어가 존재하는데,
시간이 지나면서 조금씩 배우게 될 것입니다.

1:39:57.340,1:40:01.960
토큰화를 할 때, 한 가지 까다로운 점이 있습니다.

1:40:02.280,1:40:07.620
여기에 제가 토큰화 한 후, 각 토큰들을 
공백문자로 연결시킨 결과를 보실 수 있는데요

1:40:07.620,1:40:12.155
was n't 이 두 개의 토큰이 되어 있는걸 
보실 수 있을 겁니다.

1:40:12.160,1:40:15.020
was 와 n’t 는 다른 두 가지라서,

1:40:15.020,1:40:17.220
이 결과는 납득이 되는 것 입니다.

1:40:17.900,1:40:21.840
". . . " 의 경우는 하나의 토큰이 되어 있는데 반면

1:40:21.860,1:40:25.920
! ! ! 같은 느낌표는 각각이 토큰이 되어 있습니다.

1:40:25.920,1:40:29.220
좋은 토큰화 알고리즘은 문장을 이루는 부분들을

1:40:29.220,1:40:33.220
인지하는 능력이 좋다고 볼 수 있습니다.

1:40:33.520,1:40:37.100
분리된 각 구두점의 부분들이 분리되고,

1:40:37.105,1:40:39.965
여러 부분으로 구성되는 단어들의

1:40:39.965,1:40:42.840
각 부분들도 적절하게 분리 될 것입니다.

1:40:43.360,1:40:46.315
spacy는 오스트레일리아의 
한 개발자가 만든 소프트웨어로

1:40:46.315,1:40:49.335
수 많은 NLP 관련 내용을 포함하고 있고,

1:40:49.415,1:40:52.525
제가 아는 한 
최고의 토큰화 알고리즘을 포함 합니다.

1:40:52.525,1:40:55.555
그리고 fastai는 torchtext 와 마찬가지로

1:40:55.555,1:40:58.655
spacy의 토큰 알고리즘과 함께 
잘 동작하도록 디자인 되어 있죠.

1:40:58.655,1:41:01.625
여기 보시는게 그 토큰화의 한 예 입니다.

1:41:02.005,1:41:05.095
그 다음으로 torchtext 를 이용하게 되는데

1:41:05.100,1:41:08.120
우선 Field 라는 것을 생성하게 됩니다.

1:41:08.120,1:41:12.680
Field 는 텍스트를 어떻게 
전처리 할 것인가에 대한 정의로,

1:41:12.680,1:41:15.460
여기 보시는 코드 라인이
 Field의 한 예를 보여줍니다.

1:41:15.460,1:41:19.240
소문자 텍스트를 원하고,

1:41:19.260,1:41:24.200
토큰화 알고리즘으로는 
spacy_tok 를 사용한다고 명시 하였습니다.

1:41:24.360,1:41:28.260
Field를 정의만 하고, 
아직 수행은 하지 않았습니다.

1:41:28.380,1:41:34.980
어떻게 전처리를 해라 라는 “정의”를 
TEXT 라는 변수에 저장을 한 것입니다.

1:41:35.440,1:41:39.940
이 부분은 fastai와 관련이 없고,

1:41:39.940,1:41:43.460
torchtext 라이브러리의 일 부분입니다.

1:41:43.460,1:41:46.960
torchtext는 생긴지 얼마 안된 것으로,
많은 내용이 있지는 않지만

1:41:47.065,1:41:50.040
torchtext 홈페이지에서 관련 문서를 
찾아보실 수 있긴 합니다.

1:41:50.040,1:41:54.620
하지만 아마도, 가장 자세한 정보는 
이 레슨을 통해서 얻을 수 있겠지만 말이죠.

1:41:55.660,1:41:58.575
Field 정의를 한 후, 해야 할 것은

1:41:58.580,1:42:03.600
평소에 하던 것 처럼 fastai의 
"모델 데이터" 객체를 만드는 것입니다.

1:42:04.060,1:42:06.405
"모델 데이터" 객체를 만들기 위해서,

1:42:06.405,1:42:09.105
몇 가지 정보를 제공 해 줘야 하는데

1:42:09.105,1:42:12.515
그 전에 우선, 
학습/검증/테스트 데이터셋의 위치 정보를

1:42:12.520,1:42:16.480
먼저 묶어서 관리 할 필요가 있습니다.
(dict 사용)

1:42:16.480,1:42:20.440
이 예제에서는 
독립적인 검증/테스트 데이터셋이 없으므로

1:42:20.460,1:42:24.260
두 가지에 대해서 동일하게
검증 데이터셋을 명시 해 줬습니다.

1:42:24.660,1:42:27.600
이제 "모델 데이터" 객체를 만들 수 있는데,

1:42:27.600,1:42:31.600
첫 번째 파라메터는 
전과 동일하게 PATH 정보를,

1:42:31.600,1:42:38.060
두 번째 파라메터에는 
전처리의 “정의”인 Field가 담긴 TEXT를,

1:42:38.080,1:42:41.340
세 번째 파라메터에는 직전에 만들어둔

1:42:41.340,1:42:44.840
데이터셋 들의 위치가 담긴 딕셔너리 정보를,

1:42:44.840,1:42:48.860
네 번째 파라메터는 
배치 사이즈를 넘겨 주면 됩니다.

1:42:48.860,1:42:53.200
마지막에는 특별한 두 가지 파라메터가 있습니다.

1:42:53.420,1:42:57.040
그 중 마지막 것은 
NLP에서 아주 일반적으로 사용되는 것인데

1:42:57.040,1:42:59.880
minimum frequency(최소 빈도) 
라는 것입니다.

1:43:00.180,1:43:04.240
잠시 후, 모든 유니크한 단어들에

1:43:04.240,1:43:08.720
정수 값의 인덱스를 부여하게 될 텐데

1:43:09.120,1:43:12.880
이때, 이 최소 빈도 값 보다 덜 등장하는 단어는

1:43:12.880,1:43:15.800
unknown 으로 취급되게 됩니다.

1:43:15.800,1:43:18.600
여기서는 최소 빈도 값이 10으로,

1:43:18.600,1:43:21.900
10번 이하로 등장하는 단어들은 
단어로서 취급하지 않겠다는 의미가 됩니다.

1:43:21.900,1:43:24.255
다섯번째 파라메터는 bptt로,

1:43:24.260,1:43:27.520
back propagation through time 의 약어인데

1:43:27.780,1:43:31.560
얼마나 긴 문장을 한번에

1:43:31.560,1:43:35.880
GPU에 올릴 지를 결정하기 위한 것입니다.

1:43:35.880,1:43:41.580
여기서는 70개의 토큰 이하로 
이루어진 문장을 대상으로 하고 있습니다.

1:43:41.580,1:43:45.980
이 두 가지 파라메터의 더 자세한 내용은 
잠시 후에 다시 배우게 될 것입니다.

1:43:45.980,1:43:50.760
"모델 데이터" 객체를 만들면 내부적으로

1:43:50.800,1:43:57.480
TEXT 의 vocab 이라는 속성의 
내용이 채워지게 됩니다.

1:43:57.580,1:44:01.515
이는 NLP의 매우 중요한 개념 중 하나입니다.

1:44:01.515,1:44:04.525
너무 많은 NLP 개념들을 막 소개 해 드려서 죄송하지만,

1:44:04.525,1:44:07.715
몇 차례에 걸쳐서 계속 설명 해 드리겠습니다.

1:44:07.765,1:44:10.715
vocab 는 vocabulary(어휘 목록) 입니다.

1:44:10.715,1:44:13.715
NLP에서 어휘 목록은 
주어진 텍스트에서 등장하는

1:44:13.720,1:44:17.700
유니크한 단어들의 목록이라는 매우 특별한 의미를 가집니다.

1:44:17.700,1:44:21.680
즉, 각 유니크한 단어들에는 
그들만의 인덱스 값이 부여 되는 것이죠.

1:44:21.880,1:44:23.180
한번 봅시다.

1:44:23.180,1:44:27.200
fastai와는 연관이 없는 torchtext에 대한

1:44:27.200,1:44:33.200
TEXT.vocab.itos 에서, 
itos 는 int to string 으로

1:44:33.200,1:44:36.660
정수 0은 <unk>,

1:44:36.680,1:44:40.080
정수 1은 <pad>,
정수 2는 the,

1:44:40.080,1:44:44.220
같은 방식으로 나머지인
, . and a of to … 이 저장되어 있습니다.

1:44:44.500,1:44:48.320
여기서 보이는 것은 IMDB 영화 리뷰에 대한

1:44:48.320,1:44:53.340
vocab 배열의 12번째 까지의 것만들 보여줍니다.

1:44:53.340,1:44:57.120
그리고 이 목록들은 처음 두 개의 
특별한 토큰을 제외하곤,

1:44:57.120,1:45:00.280
frequency(빈도)에 의해서 정렬되어 있습니다.

1:45:00.600,1:45:03.220
정 반대의 방식으로도 접근이 가능합니다.

1:45:03.220,1:45:05.160
stoi 는 string to integer 라는 의미입니다.

1:45:05.160,1:45:08.200
itos 결과에서, the 의 인덱스는 2 였습니다.

1:45:08.200,1:45:11.300
그래서, stoi에서 이를 확인 해 본 결과 
2가 나옴을 알 수 있습니다.

1:45:11.300,1:45:14.780
즉, vocab 이라는 것은 
단어들을 정수값에 매핑하거나

1:45:14.780,1:45:18.620
정수 값에 단어들을 매핑할 수 있도록 
해 주는 것입니다.

1:45:19.460,1:45:22.820
이 vocab을 가지고 할 수 있는 것은

1:45:22.840,1:45:26.960
예를 들어서, 
영화 리뷰의 처음 12개의 토큰에 대해서

1:45:26.960,1:45:31.240
이 토큰들을 12개의 정수값으로 치환하는 작업이 있습니다.

1:45:31.240,1:45:35.980
‘of’, ‘the’ 가 각각 7과 2로 치환 되었고,

1:45:35.980,1:45:40.040
실제로 이 단어의 인덱스가 
7과 2임을 확인 할 수 있습니다.

1:45:40.040,1:45:44.720
우리는 이렇게 치환된 형태의 
데이터를 가지고 작업을 하게 될 것입니다.

1:45:47.360,1:45:51.995
>> stemming이나 lemmatization을 하는게 일반적인가요?
(stemming은 단어의 끝 부분의 복수형을 나타내는 부분 같은 것을 잘라내는것, Lemmatization 복수등 변형된 단어를 단어의 원 형태로 만드는 것)

1:45:51.995,1:45:55.015
그렇진 않습니다.

1:45:55.020,1:45:58.560
일반적으로 토큰화가 저희가 원하는 모든 것입니다.

1:45:58.560,1:46:03.840
언어 모델은 다음에 오는 단어가 뭔지 예측 할 때,
가능한 한 일반적 이어야 하는데,

1:46:03.840,1:46:08.380
미래형, 과거형, 단수, 복수등과 같은 단어 형태 중에서,

1:46:08.380,1:46:12.760
무엇이 다음 단어로 예측 될지 알기가 어렵습니다.

1:46:12.760,1:46:21.340
따라서, 되도록 그냥 있는 그대로 놔두는게 
가장 좋다는게 일반적입니다.

1:46:23.960,1:46:27.235
지금 내용은 상대적으로 
꽤나 새로운 것이라고 말씀 드렸습니다

1:46:27.240,1:46:32.600
그래서, 몇 연구자들이 발견한 
매우 유용한 전처리 방법이 있더라도

1:46:32.600,1:46:37.640
제가 아직 이걸 알지 못할 수도 있다는걸 알려 드립니다.

1:46:38.820,1:46:43.460
>> 자연어 처리에서, 문맥이 중요하지 않나요?

1:46:43.460,1:46:45.140
아주 중요하죠

1:46:45.140,1:46:50.460
>> 왜 토큰화를 하고 개별의 단어들을 들여다 보고 있는거죠?

1:46:50.460,1:46:54.700
아니에요, 저희는 단지 단어를 개별적으로 
들여다 보고 있는게 아닙니다.

1:46:54.700,1:46:57.120
지금 보신 토큰들은

1:46:57.120,1:47:00.080
순서대로 저장되어 있는 것이에요.

1:47:00.080,1:47:03.300
I 라는 단어를 12 숫자로 치환 했을 뿐이어서

1:47:03.300,1:47:07.120
인덱스로 치환된 데이터도
 문장의 순서대로 구성 되어 있는 것이죠

1:47:07.780,1:47:10.795
“bag of words” 라는 자연어를 다루기 위한

1:47:10.795,1:47:13.445
또 다른 방식도 있는데

1:47:13.445,1:47:16.325
이 방식은 문맥이나 순서등을 신경쓰지 않습니다.

1:47:16.325,1:47:19.365
머신러닝 코스에서, 
“bag of words” 표현에 대해서 배우지만

1:47:19.365,1:47:21.720
제 생각에는

1:47:21.720,1:47:24.780
이 방식은 더이상 유용하지 않다거나

1:47:24.785,1:47:27.915
그렇게 되어 가고 있는 중이라고 생각 됩니다.

1:47:27.920,1:47:37.460
딥러닝을 이용해서 어떻게 문맥을 더 적절하게 
활용할 수 있는지 알아가고 있기 때문입니다.

1:47:38.660,1:47:44.380
배치 크기(bs)와 BPTT 에 대한 값들을 
설정 해 준다는 설명으로 돌아가 보겠습니다.

1:47:44.380,1:47:48.040
근데 이 개념들이 뭔지 모호 할 것입니다.

1:47:48.340,1:47:53.020
(창 전환 하는 중)

1:47:53.220,1:48:01.720
일단, 긴 텍스트가 있다고 가정 해 봅시다. 

1:48:01.720,1:48:05.380
그러면, 이 문장은 여러개의 단어로 구성될 것입니다.

1:48:05.580,1:48:08.615
비록 수 많은 영화 리뷰(파일)가 있더라도

1:48:08.615,1:48:11.635
사실상 언어 모델 내부에서는

1:48:11.635,1:48:14.665
이 모든 리뷰들이 모두 이어 붙여져서 

1:48:14.665,1:48:17.725
하나의 긴 텍스트로서 다뤄 집니다.

1:48:17.725,1:48:20.725


1:48:20.725,1:48:23.635


1:48:23.635,1:48:26.365


1:48:26.365,1:48:29.505


1:48:29.505,1:48:30.505


1:48:30.845,1:48:32.635


1:48:33.185,1:48:36.615


1:48:36.885,1:48:39.555


1:48:39.555,1:48:42.575


1:48:42.975,1:48:44.595


1:48:45.015,1:48:48.055


1:48:48.055,1:48:50.985


1:48:50.985,1:48:53.555


1:48:53.755,1:48:56.335


1:48:56.665,1:48:59.325


1:49:00.185,1:49:02.175


1:49:04.045,1:49:07.005


1:49:07.935,1:49:09.805


1:49:11.645,1:49:13.355


1:49:13.665,1:49:14.785


1:49:16.105,1:49:19.425


1:49:20.125,1:49:21.545


1:49:28.785,1:49:29.995


1:49:31.485,1:49:34.495


1:49:34.495,1:49:37.395


1:49:37.395,1:49:40.385


1:49:40.415,1:49:43.405


1:49:43.405,1:49:46.495


1:49:46.495,1:49:47.495


1:49:47.595,1:49:49.805


1:49:50.165,1:49:53.195


1:49:53.425,1:49:56.775


1:49:57.065,1:49:58.665


1:49:59.045,1:50:01.005


1:50:01.855,1:50:05.135


1:50:05.605,1:50:08.605


1:50:08.605,1:50:12.055


1:50:12.415,1:50:15.475


1:50:15.475,1:50:17.315


1:50:17.665,1:50:20.335


1:50:20.815,1:50:23.875


1:50:23.875,1:50:26.885


1:50:26.885,1:50:29.605


1:50:29.605,1:50:32.215


1:50:32.215,1:50:33.685


1:50:34.115,1:50:37.275


1:50:37.335,1:50:40.345


1:50:40.345,1:50:42.295


1:50:42.605,1:50:45.525


1:50:45.525,1:50:47.885


1:50:47.915,1:50:51.055


1:50:51.055,1:50:52.055


1:50:52.285,1:50:54.585


1:50:54.585,1:50:57.335


1:50:57.335,1:50:59.855


1:51:00.305,1:51:03.165


1:51:03.875,1:51:07.325


1:51:07.685,1:51:10.275


1:51:11.055,1:51:12.585


1:51:13.165,1:51:16.215


1:51:16.215,1:51:18.215


1:51:18.215,1:51:20.285


1:51:20.895,1:51:23.815


1:51:23.815,1:51:26.855


1:51:26.855,1:51:29.735


1:51:30.285,1:51:31.695


1:51:32.655,1:51:35.325


1:51:35.325,1:51:37.805


1:51:37.805,1:51:40.835


1:51:40.835,1:51:43.805


1:51:43.805,1:51:46.795


1:51:46.795,1:51:47.795


1:51:48.535,1:51:51.685


1:51:55.125,1:51:56.125


1:51:56.495,1:51:59.285


1:51:59.285,1:52:02.405


1:52:02.405,1:52:05.245


1:52:05.245,1:52:08.285


1:52:08.285,1:52:09.285


1:52:09.405,1:52:12.395


1:52:12.395,1:52:15.395


1:52:15.395,1:52:18.155


1:52:18.155,1:52:21.395


1:52:21.435,1:52:24.505


1:52:24.505,1:52:27.885


1:52:29.035,1:52:30.035


1:52:32.305,1:52:33.825


1:52:34.485,1:52:37.495


1:52:37.885,1:52:38.885


1:52:39.225,1:52:42.225


1:52:42.225,1:52:45.035


1:52:45.035,1:52:47.805


1:52:47.805,1:52:50.885


1:52:50.885,1:52:53.485


1:52:53.655,1:52:56.665


1:52:56.665,1:52:59.625


1:52:59.625,1:53:01.685


1:53:02.105,1:53:03.325


1:53:03.775,1:53:06.885


1:53:06.925,1:53:09.595


1:53:09.595,1:53:12.585


1:53:12.585,1:53:15.235


1:53:15.655,1:53:18.415


1:53:18.415,1:53:21.445


1:53:21.445,1:53:24.585


1:53:24.585,1:53:27.665


1:53:27.665,1:53:30.515


1:53:30.515,1:53:33.665


1:53:33.665,1:53:36.585


1:53:37.785,1:53:38.785


1:53:41.515,1:53:44.695


1:53:44.695,1:53:47.765


1:53:48.055,1:53:51.145


1:53:51.145,1:53:53.975


1:53:53.975,1:53:56.835


1:53:56.835,1:54:00.205


1:54:00.935,1:54:03.955


1:54:03.955,1:54:06.025


1:54:06.025,1:54:09.005


1:54:09.005,1:54:11.675


1:54:11.675,1:54:14.385


1:54:14.385,1:54:17.385


1:54:17.385,1:54:20.715


1:54:20.715,1:54:21.715


1:54:22.575,1:54:24.345


1:54:25.465,1:54:28.125


1:54:28.125,1:54:31.135


1:54:31.135,1:54:34.155


1:54:34.155,1:54:37.225


1:54:37.225,1:54:38.225


1:54:38.385,1:54:41.435


1:54:41.435,1:54:44.135


1:54:47.535,1:54:48.535


1:54:50.885,1:54:53.945


1:54:53.945,1:54:56.765


1:54:56.765,1:54:59.775


1:54:59.775,1:55:02.885


1:55:02.885,1:55:05.295


1:55:05.325,1:55:08.325


1:55:08.325,1:55:09.325


1:55:09.875,1:55:13.175


1:55:13.175,1:55:16.205


1:55:16.205,1:55:19.265


1:55:19.265,1:55:22.395


1:55:22.395,1:55:25.385


1:55:25.385,1:55:28.355


1:55:28.355,1:55:31.615


1:55:31.795,1:55:33.735


1:55:34.075,1:55:36.915


1:55:36.915,1:55:40.345


1:55:40.415,1:55:43.145


1:55:43.445,1:55:46.115


1:55:46.765,1:55:49.655


1:55:49.655,1:55:52.825


1:55:52.825,1:55:55.995


1:55:55.995,1:55:56.995


1:55:57.245,1:56:00.525


1:56:00.525,1:56:03.645


1:56:04.705,1:56:07.335


1:56:08.575,1:56:11.645


1:56:12.395,1:56:15.305


1:56:15.305,1:56:18.515


1:56:18.515,1:56:21.155


1:56:21.155,1:56:22.645


1:56:23.205,1:56:26.135


1:56:26.135,1:56:29.175


1:56:29.175,1:56:31.135


1:56:31.975,1:56:34.945


1:56:34.945,1:56:37.945


1:56:37.945,1:56:40.875


1:56:40.875,1:56:43.895


1:56:43.895,1:56:46.975


1:56:46.975,1:56:49.895


1:56:49.895,1:56:52.375


1:56:52.905,1:56:53.905


1:56:55.315,1:56:57.855


1:56:58.165,1:57:01.235


1:57:02.085,1:57:05.175


1:57:05.175,1:57:08.175


1:57:08.175,1:57:09.925


1:57:10.335,1:57:12.325


1:57:12.905,1:57:15.885


1:57:15.885,1:57:18.625


1:57:18.625,1:57:19.625


1:57:20.715,1:57:23.195


1:57:25.765,1:57:28.845


1:57:28.845,1:57:32.085


1:57:32.085,1:57:35.005


1:57:35.145,1:57:36.145


1:57:36.365,1:57:37.365


1:57:37.465,1:57:40.495


1:57:40.495,1:57:41.495


1:57:41.595,1:57:44.425


1:57:44.425,1:57:45.745


1:57:46.235,1:57:47.395


1:57:47.765,1:57:50.815


1:57:51.165,1:57:52.345


1:57:53.145,1:57:56.255


1:57:56.255,1:57:59.375


1:57:59.375,1:58:02.475


1:58:02.475,1:58:05.535


1:58:05.535,1:58:08.525


1:58:08.525,1:58:11.525


1:58:11.725,1:58:14.815


1:58:14.815,1:58:17.835


1:58:17.835,1:58:20.895


1:58:20.895,1:58:22.095


1:58:22.775,1:58:25.665


1:58:25.665,1:58:28.235


1:58:28.725,1:58:31.955


1:58:31.955,1:58:34.875


1:58:34.925,1:58:38.025


1:58:38.025,1:58:39.255


1:58:39.655,1:58:40.655


1:58:40.945,1:58:41.825


1:58:41.825,1:58:45.125


1:58:45.125,1:58:48.275


1:58:48.275,1:58:50.695


1:58:51.085,1:58:53.025


1:58:53.655,1:58:54.655


1:58:55.105,1:58:57.655


1:58:57.655,1:59:00.745


1:59:00.745,1:59:03.335


1:59:03.335,1:59:06.115


1:59:06.765,1:59:07.765


1:59:08.235,1:59:11.225


1:59:11.225,1:59:14.285


1:59:14.285,1:59:17.365


1:59:17.365,1:59:20.305


1:59:20.305,1:59:23.255


1:59:23.255,1:59:26.435


1:59:26.435,1:59:29.505


1:59:30.545,1:59:33.555


1:59:33.555,1:59:36.665


1:59:36.735,1:59:39.125


1:59:39.535,1:59:42.435


1:59:42.435,1:59:45.495


1:59:45.495,1:59:48.585


1:59:49.135,1:59:52.215


1:59:52.215,1:59:55.225


1:59:55.225,1:59:58.155


1:59:58.155,2:00:01.455


2:00:01.515,2:00:04.345


2:00:04.505,2:00:07.465


2:00:07.465,2:00:08.465


2:00:08.995,2:00:12.015


2:00:12.015,2:00:15.075


2:00:15.075,2:00:18.095


2:00:18.095,2:00:21.175


2:00:21.175,2:00:24.155


2:00:24.155,2:00:27.145


2:00:27.145,2:00:30.105


2:00:30.105,2:00:32.515


2:00:32.515,2:00:35.375


2:00:35.375,2:00:38.575


2:00:38.575,2:00:41.575


2:00:41.575,2:00:42.605


2:00:43.515,2:00:46.085


2:00:46.085,2:00:49.245


2:00:49.245,2:00:51.765


2:00:52.565,2:00:55.155


2:00:55.155,2:00:58.125


2:00:58.125,2:01:01.085


2:01:01.085,2:01:04.195


2:01:04.195,2:01:05.715


2:01:07.475,2:01:10.495


2:01:10.495,2:01:13.215


2:01:13.215,2:01:16.285


2:01:16.635,2:01:19.505


2:01:19.505,2:01:22.385


2:01:22.385,2:01:25.325


2:01:25.325,2:01:28.405


2:01:28.405,2:01:29.405


2:01:29.445,2:01:31.015


2:01:31.345,2:01:34.025


2:01:34.025,2:01:37.105


2:01:37.385,2:01:39.985


2:01:40.005,2:01:42.875


2:01:42.875,2:01:45.555


2:01:45.555,2:01:48.595


2:01:48.595,2:01:52.005


2:01:52.005,2:01:53.005


2:01:54.215,2:01:57.025


2:01:57.025,2:01:58.585


2:01:58.995,2:02:02.025


2:02:02.025,2:02:04.755


2:02:04.755,2:02:06.255


2:02:07.435,2:02:10.145


2:02:10.175,2:02:11.665


2:02:12.175,2:02:14.935


2:02:14.935,2:02:18.125


2:02:18.675,2:02:20.705


2:02:21.035,2:02:22.035


2:02:22.245,2:02:24.815


2:02:29.325,2:02:31.915


2:02:31.915,2:02:34.705


2:02:34.705,2:02:37.775


2:02:37.775,2:02:40.885


2:02:40.885,2:02:42.695


2:02:43.455,2:02:45.845


2:02:45.845,2:02:48.855


2:02:48.855,2:02:51.945


2:02:52.565,2:02:55.545


2:02:55.805,2:02:58.005


2:02:58.465,2:03:01.645


2:03:01.645,2:03:04.575


2:03:04.575,2:03:07.555


2:03:07.555,2:03:10.665


2:03:10.665,2:03:12.685


2:03:13.425,2:03:14.425


2:03:15.115,2:03:18.215


2:03:18.515,2:03:19.685


2:03:20.355,2:03:21.355


2:03:21.715,2:03:22.715


2:03:24.105,2:03:26.755


2:03:26.755,2:03:29.685


2:03:29.685,2:03:32.775


2:03:32.775,2:03:35.765


2:03:35.765,2:03:38.815


2:03:38.815,2:03:41.925


2:03:41.925,2:03:43.285


2:03:43.835,2:03:46.775


2:03:46.775,2:03:49.815


2:03:49.815,2:03:52.815


2:03:52.815,2:03:55.745


2:03:55.745,2:03:58.645


2:03:58.645,2:04:00.325


2:04:01.775,2:04:04.815


2:04:04.815,2:04:07.775


2:04:07.775,2:04:10.345


2:04:10.345,2:04:11.845


2:04:12.235,2:04:13.235


2:04:16.435,2:04:17.465


2:04:17.995,2:04:20.945


2:04:20.945,2:04:23.985


2:04:23.985,2:04:25.975


2:04:26.745,2:04:29.825


2:04:29.825,2:04:32.755


2:04:32.755,2:04:35.795


2:04:35.795,2:04:38.185


2:04:38.315,2:04:41.215


2:04:41.215,2:04:44.285


2:04:44.335,2:04:47.285


2:04:47.285,2:04:50.345


2:04:50.345,2:04:53.255


2:04:53.525,2:04:56.385


2:04:56.385,2:04:59.425


2:04:59.425,2:05:02.265


2:05:02.265,2:05:05.715


2:05:05.725,2:05:07.905


2:05:09.215,2:05:12.195


2:05:12.745,2:05:15.165


2:05:15.505,2:05:18.585


2:05:18.585,2:05:21.455


2:05:21.455,2:05:24.635


2:05:26.305,2:05:29.375


2:05:29.375,2:05:30.975


2:05:31.295,2:05:34.235


2:05:34.235,2:05:37.185


2:05:37.185,2:05:40.255


2:05:40.255,2:05:43.495


2:05:43.495,2:05:46.525


2:05:46.525,2:05:48.985


2:05:48.985,2:05:51.175


2:05:51.565,2:05:52.565


2:05:53.685,2:05:56.275


2:05:56.275,2:05:59.565


2:06:00.095,2:06:01.095


2:06:01.175,2:06:04.305


2:06:04.535,2:06:07.535


2:06:07.535,2:06:10.585


2:06:10.585,2:06:13.515


2:06:13.515,2:06:16.185


2:06:16.375,2:06:19.345


2:06:19.345,2:06:22.345


2:06:22.345,2:06:24.025


2:06:24.875,2:06:27.915


2:06:27.955,2:06:30.165


2:06:30.945,2:06:33.915


2:06:33.915,2:06:36.865


2:06:36.865,2:06:39.845


2:06:39.845,2:06:42.685


2:06:42.685,2:06:45.795


2:06:45.795,2:06:48.785


2:06:48.785,2:06:51.525


2:06:51.525,2:06:54.215


2:06:54.215,2:06:57.265


2:06:57.265,2:07:00.165


2:07:00.165,2:07:03.175


2:07:03.175,2:07:06.015


2:07:06.015,2:07:08.975


2:07:08.975,2:07:12.085


2:07:12.085,2:07:14.895


2:07:14.895,2:07:17.985


2:07:18.305,2:07:21.085


2:07:21.695,2:07:23.895


2:07:24.395,2:07:25.455


2:07:25.895,2:07:27.145


2:07:27.555,2:07:30.495


2:07:30.495,2:07:31.495


2:07:31.555,2:07:34.635


2:07:34.635,2:07:37.675


2:07:37.675,2:07:40.625


2:07:40.625,2:07:43.505


2:07:43.505,2:07:46.215


2:07:46.215,2:07:49.135


2:07:49.135,2:07:52.105


2:07:52.285,2:07:55.275


2:07:55.275,2:07:58.475


2:07:58.485,2:08:01.565


2:08:02.105,2:08:05.025


2:08:05.025,2:08:06.535


2:08:06.895,2:08:10.025


2:08:10.025,2:08:13.015


2:08:13.015,2:08:15.975


2:08:15.975,2:08:17.405


2:08:18.155,2:08:19.155


2:08:19.325,2:08:22.235


2:08:22.435,2:08:25.535


2:08:25.535,2:08:28.695


2:08:28.695,2:08:31.855


2:08:31.985,2:08:35.095


2:08:35.095,2:08:37.625


2:08:37.625,2:08:40.375


2:08:40.375,2:08:43.485


2:08:43.835,2:08:46.075


2:08:46.515,2:08:49.515


2:08:49.675,2:08:52.375


2:08:52.375,2:08:55.425


2:08:55.505,2:08:57.435


2:08:57.745,2:08:58.745


2:08:58.865,2:09:02.255


2:09:02.635,2:09:05.205


2:09:05.205,2:09:08.285


2:09:08.285,2:09:10.525


2:09:11.305,2:09:14.225


2:09:14.225,2:09:17.155


2:09:17.155,2:09:19.945


2:09:19.945,2:09:22.775


2:09:22.775,2:09:25.855


2:09:25.855,2:09:29.205


2:09:29.715,2:09:32.915


2:09:32.915,2:09:35.155


2:09:35.725,2:09:36.725


2:09:37.315,2:09:40.435


2:09:40.435,2:09:43.305


2:09:43.305,2:09:46.415


2:09:46.415,2:09:49.445


2:09:49.445,2:09:52.495


2:09:52.495,2:09:55.255


2:09:55.255,2:09:56.575


2:09:57.035,2:10:00.215


2:10:00.215,2:10:03.215


2:10:03.215,2:10:06.235


2:10:06.235,2:10:09.285


2:10:09.625,2:10:12.595


2:10:12.595,2:10:15.805


2:10:15.805,2:10:18.485


2:10:18.485,2:10:19.505


2:10:20.125,2:10:22.865


2:10:22.865,2:10:24.215


2:10:24.685,2:10:25.685


2:10:26.295,2:10:29.055


2:10:29.055,2:10:32.365


2:10:32.365,2:10:34.495


2:10:34.715,2:10:37.615


2:10:38.245,2:10:41.295


2:10:41.295,2:10:44.215


2:10:44.215,2:10:47.435


2:10:47.435,2:10:50.445


2:10:50.445,2:10:53.675


2:10:53.845,2:10:57.225


2:10:57.455,2:10:58.455


2:10:58.525,2:11:01.375


2:11:01.375,2:11:04.855


2:11:04.895,2:11:07.965


2:11:07.965,2:11:11.075


2:11:11.075,2:11:13.995


2:11:13.995,2:11:14.995


2:11:15.175,2:11:18.235


2:11:18.235,2:11:21.175


2:11:21.175,2:11:22.415


2:11:23.005,2:11:25.315


2:11:25.315,2:11:28.255


2:11:28.255,2:11:30.165


2:11:31.195,2:11:34.295


2:11:34.295,2:11:37.175


2:11:37.295,2:11:38.405


2:11:39.535,2:11:41.045


2:11:41.735,2:11:44.835


2:11:44.835,2:11:45.835


2:11:46.295,2:11:49.115


2:11:49.115,2:11:52.055


2:11:52.085,2:11:55.125


2:11:55.125,2:11:57.535


2:11:57.875,2:12:00.835


2:12:00.835,2:12:03.625


2:12:03.625,2:12:06.665


2:12:06.665,2:12:07.675


2:12:08.055,2:12:11.095


2:12:11.095,2:12:14.355


2:12:14.465,2:12:17.625


2:12:17.625,2:12:20.835


2:12:21.035,2:12:23.785


2:12:24.655,2:12:27.405


2:12:27.405,2:12:30.265


2:12:30.265,2:12:33.365


2:12:33.365,2:12:36.245


2:12:36.245,2:12:39.145


2:12:39.145,2:12:41.985


2:12:41.985,2:12:45.165


2:12:45.315,2:12:48.465


2:12:48.465,2:12:51.385


2:12:51.385,2:12:54.545


2:12:54.915,2:12:57.745


2:12:57.745,2:12:59.165


2:12:59.605,2:13:01.205


2:13:02.335,2:13:04.895


2:13:04.895,2:13:07.485


2:13:07.485,2:13:09.755


2:13:10.215,2:13:13.395


2:13:13.395,2:13:16.485


2:13:16.485,2:13:19.195


2:13:19.195,2:13:22.285


2:13:22.285,2:13:25.685


2:13:26.235,2:13:29.415


2:13:29.415,2:13:32.465


2:13:32.465,2:13:34.995


2:13:36.785,2:13:39.575


2:13:39.575,2:13:41.665


2:13:41.665,2:13:44.935


2:13:44.935,2:13:47.295


2:13:47.295,2:13:49.925


2:13:49.925,2:13:52.295


2:13:52.615,2:13:55.655


2:13:55.655,2:13:58.725


2:13:58.725,2:14:01.795


2:14:01.795,2:14:04.815


2:14:04.815,2:14:07.705


2:14:07.705,2:14:10.775


2:14:10.775,2:14:13.815


2:14:13.815,2:14:16.925


2:14:16.925,2:14:19.655


2:14:19.745,2:14:22.645


2:14:22.645,2:14:25.935


2:14:26.885,2:14:28.085

