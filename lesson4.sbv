0:00:00.040,0:00:03.560
돌아오신걸 환영합니다
보게되어 기쁘군요

0:00:06.960,0:00:11.420
딥러닝 하느라 바쁜 한주 보내셨나요?

0:00:12.945,0:00:15.995
몇 가지 멋진 일들이 일어나고 있더군요

0:00:15.995,0:00:19.075
지난주 처럼, 여러분들의 학우들 중 몇몇이 적은

0:00:19.080,0:00:22.160
몇가지 흥미로운 글들을 소개 해 드리려고 합니다

0:00:24.760,0:00:27.520
Vitaly 라는 분은

0:00:27.520,0:00:31.415
저도 한동안 접해보지 못할 정도의
최고의 글을 적어 주셨습니다

0:00:31.415,0:00:34.575
'차등 학습률'과

0:00:34.575,0:00:37.825
'재시작하는 확률적 경사하강'에 대한
내용을 다루고 있습니다

0:00:37.860,0:00:40.060
가능하면, 꼭 읽어보시기 바랍니다

0:00:40.060,0:00:45.940
이 분이 적은 글은
독자들의 배경지식 수준과 상관 없이

0:00:46.040,0:00:48.965
그 주제들에 대해서

0:00:48.965,0:00:52.095
많은 것을 얻어갈 수 있도록 작성되었습니다

0:00:52.095,0:00:55.115
또한, 좀 더 깊은 내용을 원하는 분들을 위해서

0:00:55.115,0:00:57.785
아카데믹 논문의 링크와

0:00:57.785,0:01:00.545
이 글의 주제에 대한 예제도 제공하고 있습니다

0:01:00.545,0:01:02.835
제 생각에 이 글은

0:01:03.080,0:01:05.740
특히나 나이스하게 잘 적힌 글로

0:01:05.740,0:01:09.175
기술적 커뮤니케이션에 대한
좋은 롤모델도 될 수 있을 것 같습니다

0:01:09.180,0:01:13.180
지난 한 주간 사람들이 적은 글에 대해서

0:01:13.360,0:01:16.300
제가 마음에 들었던 한 가지 점은

0:01:16.300,0:01:19.920
제가 마음에 들었던 한 가지 점은

0:01:20.340,0:01:23.280
포럼에서 여러가지 토론이 진행 되었다는 것입니다

0:01:23.285,0:01:26.445
많은 사람들이 서로를 도와줬는데

0:01:26.445,0:01:29.495
블로그 글의 특정 부분이 잘못 되었으면

0:01:29.500,0:01:33.740
이를 설명 해 주기도 하고

0:01:34.360,0:01:35.360
새로운 아이디어를 서로 배울 수 있는
기회도 되었습니다

0:01:35.725,0:01:38.775
확률적 경사 하강법과 차등 학습률도

0:01:38.775,0:01:41.985
토론의 몇 가지 주제중 하나였습니다.

0:01:42.400,0:01:44.440
확률적 경사 하강법과 차등 학습률도

0:01:44.500,0:01:47.800
토론의 몇 가지 주제중 하나였습니다.

0:01:48.040,0:01:51.200
Anand Saha라는 분도
비슷한 주제에 대한 글을 적었습니다

0:01:51.205,0:01:54.225
왜 이것들이 잘 동작하는지에 대한 설명과

0:01:54.275,0:01:57.295
몇 좋은 그림들과
참조 논문들의 내용을 담았고

0:01:57.295,0:02:00.745
실제로 어떻게 동작하는지를
보여주는 코드 또한 담아냈습니다

0:02:01.835,0:02:05.215
Mark Hoffman 이라는 분도 
똑같은 주제에 대한 글을 적었는데

0:02:05.215,0:02:08.295
약간 입문자들을 위한 수준의 글이고

0:02:08.295,0:02:11.485
많은 직관적인 설명들을 포함하고 있습니다

0:02:11.485,0:02:14.475
Manikanta라는 분은 특별히

0:02:14.475,0:02:17.455
차등 학습률에 대하여 
왜 이주제가 흥미로운지를 설명합니다

0:02:17.455,0:02:19.975
또한, 전이학습(transfer learning) 주제에

0:02:19.980,0:02:22.520
친숙하지 않은 사람들을 위해서

0:02:22.520,0:02:25.660
전이학습이 무엇이고
왜 흥미로운 주제인지를

0:02:25.665,0:02:28.875
잘 설명 해 주고, 그걸 기반으로
왜 차등학습률이 도움이 되는지도

0:02:28.880,0:02:30.920
함께 설명 해 줍니다

0:02:31.700,0:02:33.320
또 다른 분 으로

0:02:33.320,0:02:36.580
Arjun이 적은 글에 대해서
특별히 제가 좋았던 점은

0:02:36.585,0:02:39.505
테크놀로지 관점으로만 글을 적은 것이 아니라

0:02:39.505,0:02:42.680
상업적인 관점에서 가지는 의미를

0:02:42.680,0:02:45.825
함께 설명 해 주고 있습니다

0:02:45.825,0:02:48.595
예를 들어서,

0:02:48.595,0:02:51.595
저희가 지금까지 배워 온 내용이

0:02:51.600,0:02:58.080
실 생활에서 가지는 의미라던지, 등등
설명을 해 주고 있습니다

0:02:58.140,0:03:03.735
어쨋든 온라인에서 멋진 활동들이 일어나고 있었습니다

0:03:03.735,0:03:06.915
모든 분들에게 감사를 드리고 싶어요

0:03:07.795,0:03:10.825
지난주에 말씀 드렸다 시피

0:03:10.825,0:03:13.565
기술 블로그 글을 적어보신 적이 없어서

0:03:13.565,0:03:16.515
어떻게 글을 적는지 잘 모르겠거나

0:03:16.515,0:03:19.555
겁을 먹으셨다면, 그냥 시도해 보세요

0:03:19.555,0:03:22.245
같은 그룹원 들에게 매우 환영 받고,

0:03:22.800,0:03:26.500
용기를 받게도 될 겁니다

0:03:29.060,0:03:30.060


0:03:30.885,0:03:33.685
오늘의 레슨은 약간 흥미롭습니다

0:03:33.685,0:03:36.860
왜냐하면, 많은 여러가지

0:03:36.860,0:03:40.320
어플리케이션 주제를 다룰 것이기 때문입니다

0:03:40.325,0:03:43.355
지금까진, 컴퓨터 비젼에 많은 시간을 할애 했는데

0:03:43.360,0:03:46.960
오늘은 시간이 허락한다면
세 가지 완전히 다른 분야를

0:03:46.980,0:03:52.680
모두  다루게 될 것입니다

0:03:54.060,0:03:57.005
그 시작은
"구조화된 학습" 또는

0:03:57.005,0:04:00.045
"구조화된 데이터 학습" 을

0:04:00.045,0:04:03.005
데이터베이스의 테이블같은

0:04:03.005,0:04:07.260
서로다른 데이터 종류에 대한
컬럼으로 이뤄진 데이터를 위한

0:04:07.260,0:04:10.320
모델을 만들면서 배우게 됩니다

0:04:10.780,0:04:13.860
두 번째 주제는 딥러닝을

0:04:13.865,0:04:16.835
"자연어 처리"에 적용하는 것입니다.

0:04:17.240,0:04:21.100
그리고 세 번째 주제는 딥 러닝을
"추천 시스템"에 적용하는 것입니다.

0:04:21.100,0:04:22.780
이 세가지 주제를

0:04:23.425,0:04:26.395
하이레벨 관점에서 다루게 됩니다

0:04:26.400,0:04:28.260
따라서, 주로 집중할 부분은

0:04:28.260,0:04:30.940
내부적으로 어떻게 동작 하느냐 보단

0:04:30.940,0:04:34.460
이를 수행하기 위해 소프트웨어를 어떻게
사용하는 방법이 됩니다

0:04:34.460,0:04:37.080
그리고 나선, 이후의 다음 세 레슨에서

0:04:37.080,0:04:40.780
내부적으로 어떤일이 일어나는지
구체적으로 파헤쳐 보게 될 겁니다

0:04:40.860,0:04:44.440
또한, 컴퓨터 비젼 주제 중
우리가 건너뛴 부분에 대해서

0:04:44.440,0:04:47.940
훨씬 더 깊이있는 상세한 부분을
공부 할 것입니다.

0:04:48.380,0:04:51.080
어쨋든 오늘 레슨의 초점은

0:04:51.080,0:04:54.540
이 세가지 어플리케이션을 어떻게
수행할 수 있는지에 대한 것이고

0:04:54.540,0:04:57.560
관련된 약간의 컨셉도 포함됩니다.

0:05:00.120,0:05:03.380
일단 시작하기 전에
먼저 말씀드릴 것이 있습니다

0:05:03.380,0:05:07.500
새로운 컨셉인데, dropout 이라는 것입니다

0:05:07.500,0:05:10.320
이미 dropout이 얼마나 중요한지

0:05:10.320,0:05:13.320
많이 들어보셨을 수도 있을 겁니다

0:05:13.320,0:05:15.680
사실상 중요한 컨셉으로

0:05:15.680,0:05:18.455
설명 드리기 위해서 현재 진행 중인

0:05:18.455,0:05:21.385
개 품종 분류 Kaggle 경연을

0:05:21.385,0:05:23.585
다시 한번 살펴 보려고 합니다

0:05:23.925,0:05:26.745
우선 첫번째로 수행한 내용은

0:05:26.745,0:05:29.785
precompute=True
인자값과 함께

0:05:29.785,0:05:32.755
평소처럼 미리학습된 네트워크를 생성 했습니다

0:05:32.755,0:05:35.785
그러면, 마지막 컨볼루션 계층의

0:05:35.785,0:05:38.725
activation을 미리 계산하게 됩니다

0:05:38.725,0:05:42.125
기억하시죠? 
activation은 단순히 단일-숫자 입니다

0:05:42.175,0:05:45.455
상기 시켜 드리겠습니다

0:05:45.455,0:05:48.175
여기에 하나의 activation이 있습니다

0:05:48.175,0:05:49.525
하나의 숫자죠

0:05:49.875,0:05:52.765
구체적으론, activation은

0:05:52.765,0:05:55.320
필터/커널 을 구성하는

0:05:55.320,0:05:59.000
가중치 또는 파라메터라고도 
불리는 것이

0:05:59.000,0:06:02.920
직전 계층의 activation에 적용된 것에
기반하여 계산됩니다

0:06:02.920,0:06:04.955
직전 계층이란 것은

0:06:04.955,0:06:07.615
입력이 될 수도 있고

0:06:07.615,0:06:09.985
또 다른 계산의 결과물이 될 수도 있습니다

0:06:09.985,0:06:14.220
어쨋든 activation 이라고 하면,
계산된 숫자라는 것을 기억해 두시면 됩니다

0:06:14.660,0:06:17.920
몇 activation을 미리 계산한 후에 한 일은

0:06:17.920,0:06:21.000
초기에 무작위로 생성된

0:06:21.000,0:06:24.095
여러개의 fully connected 계층을

0:06:24.095,0:06:27.175
추가 한 것입니다

0:06:27.175,0:06:30.035
단순한 행렬의 곱셈의 수행으로

0:06:30.040,0:06:32.380
엑셀에서 간단히 보여 드리자면

0:06:32.380,0:06:35.480
가장 마지막 부분으로 가서

0:06:35.540,0:06:39.840
이렇게 생긴 행렬을 가지고
행렬의 곱셈을 수행 하였습니다

0:06:41.200,0:06:43.140
그리고 나서

0:06:43.140,0:06:45.800
단순히 learn 객체를 타이핑 해 보면

0:06:46.200,0:06:48.880
어떤 계층들이 들어 있는지와 같은

0:06:48.880,0:06:51.680
learn 객체 중 추가된 마지막 계층을
이루는게 뭔지가 출력되게 됩니다.

0:06:51.780,0:06:54.460
전 레슨들에선
계층을 추가하는 부분의

0:06:54.460,0:06:58.540
설명을 약간 건너뛰었는데,
여기 것들이 실제 learn을 구성하는 계층들 입니다

0:06:58.800,0:07:03.200
BatchNorm은 마지막 레슨에서 다루니까
지금은 일단 신경쓰지 않아도 됩니다

0:07:03.620,0:07:06.940
Linear 계층은 단순히
행렬의 곱셈을 의미합니다

0:07:07.035,0:07:09.975
(2)는 1024개의 열과

0:07:09.980,0:07:12.500
512개의 행으로 이뤄진 행렬입니다

0:07:12.500,0:07:14.920
다른말로 표현 해 보면,

0:07:14.920,0:07:17.860
1024개의 activation을 입력으로 받아들이고

0:07:17.860,0:07:20.660
512개의 출력을 뱉어 낸다고 볼 수 있습니다

0:07:20.960,0:07:25.240
그 다음에는 ReLU 계층이 있습니다.
음수를 0으로 대체한다는 것을 기억해 주세요

0:07:25.580,0:07:28.535
다시 BatchNorm 부분은 건너뛰고,
dropout은 잠시 후에 설명 드리겠습니다

0:07:28.535,0:07:31.715
두 번째 Linear 계층은 
직전 Linear 계층에서 512개의

0:07:31.715,0:07:34.775
activation을 입력으로 받아서

0:07:34.780,0:07:38.160
512x120 행렬과 곱셈을 수행 한 후

0:07:38.160,0:07:41.635
120개의 새로운 activation을 뱉어내게 됩니다

0:07:41.635,0:07:44.535
그리곤 그 결과를 Softmax에 집어 넣게 됩니다

0:07:44.535,0:07:47.645
Softmax를 기억 못하시는 분들을 위해서

0:07:47.645,0:07:50.715
지난 주의 내용을 다시 설명 드리자면,

0:07:50.715,0:07:54.085
activation 값을 가지고 하는 작업으로

0:07:54.085,0:07:56.855
dog를 예로 설명 드리겠습니다

0:07:56.860,0:08:00.160
모든 분류 카테고리에 대한
 EXP(activation) 값을 더한 후,

0:08:00.160,0:08:03.160
더해진 값으로 dog에 대한 
EXP(activation) 값을 나눠 줍니다

0:08:03.160,0:08:06.140
그렇게 모든 카테고리에 대해서 계산된 값을

0:08:06.140,0:08:09.240
더해보면 1이 되게 되고,
각각의 값들은 0~1 범위에 속하게 됩니다

0:08:10.080,0:08:12.745
지금까지 보신 것들이
precompute=True로 설정할 때

0:08:12.745,0:08:15.755
추가되는 계층들 이라고 보시면 됩니다

0:08:15.755,0:08:18.735
여기서, 제가 설명 드리고자 하는건

0:08:18.740,0:08:21.920
dropout이 무엇이고,
값을 선택할때 매우 중요한

0:08:21.920,0:08:25.320
p 가 의미하는게 뭔지에 대한 것입니다

0:08:25.400,0:08:28.360
Dropout(p=0.5) 라는 것을

0:08:28.365,0:08:31.315
엑셀 스프레드시트로 설명 드리겠습니다

0:08:31.340,0:08:34.680
일단, 아무 레이어나 선택해 봅시다

0:08:34.680,0:08:37.935
p=0.5 인 dropout을 Conv2 계층에

0:08:37.935,0:08:40.145
적용해 보겠습니다

0:08:40.700,0:08:42.100
이것의 의미는

0:08:42.140,0:08:45.000
activation들을 지나가면서
50% 확률로

0:08:45.000,0:08:46.520
셀을 선택하는 것입니다

0:08:46.855,0:08:49.825
무작위로 전체 셀의 약 50%를 선택한 후

0:08:49.825,0:08:52.045
이들을 지웁니다

0:08:53.635,0:08:54.635


0:08:54.805,0:08:57.885
이게 dropout이 하는 일입니다

0:08:57.885,0:09:00.400
그러니까, p=0.5의 의미는

0:09:00.400,0:09:02.420
셀을 삭제 할지에 대한

0:09:02.420,0:09:03.740
확률이 됩니다

0:09:04.705,0:09:05.695


0:09:05.700,0:09:08.240
이 셀들을 삭제한 후
결과를 보시면,

0:09:08.240,0:09:11.980
삭제하기 전과 비교해서

0:09:11.985,0:09:14.875
크게 바뀌지 않음을 알 수 있습니다

0:09:14.875,0:09:17.905
아주 약간만이 바뀌었습니다.
특히 Max pooling 계층 이어서

0:09:17.905,0:09:20.915
지워진 값이 4개 값의 그룹 중

0:09:20.915,0:09:23.685
가장 큰 값인 경우만, 변화가 일어납니다

0:09:24.105,0:09:27.165
max pooling이 아닌 컨볼루션 계층이

0:09:27.165,0:09:30.225
수행되는 경우에는 필터에 의해 계산된

0:09:30.225,0:09:32.665
하나의 값으로 부터만 영향이 있습니다

0:09:33.280,0:09:35.300
흥미로운점은

0:09:35.340,0:09:38.895
무작위로 계층의activation들의

0:09:38.900,0:09:42.080
절반을 버리는 이 행위가

0:09:42.080,0:09:45.780
매우 흥미로운 결과를 보여준다는 것입니다

0:09:45.785,0:09:48.745
한가지 중요히 언급할 내용이 있는데

0:09:48.745,0:09:51.040
동일 계층에 대해서,
각 미니배치 마다

0:09:51.140,0:09:54.500
서로다른 절반의 activation들이
버려질 수 있다는 것입니다

0:09:54.805,0:09:57.465
dropout이 의미하는 것은
강제로

0:09:57.465,0:09:59.535
과적합되지 않도록 하는 것입니다

0:10:00.145,0:10:03.075
다른말로 표현해 보면,
만약 특정 activation이

0:10:03.080,0:10:06.320
특정 개나 고양이를 정확히 인식하는데

0:10:06.360,0:10:09.380
학습이 되어 있다면,

0:10:09.460,0:10:12.800
이 activation을 제거 될 때
네트워크 전체는 전처럼

0:10:12.840,0:10:16.160
그 특정 이미지의 인식에 대해서
잘 동작하지 못하게 됩니다

0:10:16.160,0:10:19.980
그러면, 이렇게 제거된 네트워크가
잘 동작하게 하기 위해선

0:10:19.980,0:10:24.220
매번 무작위로 남겨진 절반의 activation만으로도

0:10:24.220,0:10:27.875
여전히 잘 동학하기 위한

0:10:27.880,0:10:31.640
값들을 찾기위한 학습이 수행되어야 합니다

0:10:33.040,0:10:37.555
dropout이 등장한지 약 3~4년 정도 되었습니다.

0:10:37.560,0:10:40.240
그리고 이 아이디어는

0:10:40.280,0:10:43.855
현대의 딥러닝을 잘 동작하게 만드는데

0:10:43.855,0:10:46.855
대단히 중요해져 왔습니다

0:10:46.920,0:10:48.880
그 이유는

0:10:48.880,0:10:52.340
일반화에 대한 문제를 해결해 주었기 때문입니다

0:10:52.340,0:10:54.920
Dropout이 등장하기 전까진

0:10:54.920,0:10:57.375
수 많은 파라메터로 구성된

0:10:57.380,0:11:00.900
모델을 학습시키는 과정에서

0:11:00.920,0:11:04.000
이미 data augmentation을 시도 해 봤고

0:11:04.000,0:11:07.440
이미 할 수 있는한 많은 데이터에
시도 해 봤는데도

0:11:07.440,0:11:11.320
과적합이 일어난다면,
여러가지를 시도해 볼 수 있겠지만

0:11:11.320,0:11:14.300
일반적론 빠져나오기
어려웠습니다

0:11:14.300,0:11:17.915
그리고 나선, Hinton 교수와 몇 동료들이
이 dropout이라는

0:11:17.915,0:11:21.085
아이디어를 생각해 냈습니다.

0:11:21.085,0:11:24.045
이 아이디어는 두뇌가
동작하는 방식과

0:11:24.045,0:11:26.875
Hinton 교수의 은행에서의
경험에 의해

0:11:26.880,0:11:29.120
영감을 받았습니다

0:11:29.880,0:11:33.060
어쨋든 이들은
무작위로 activation을 삭제하는

0:11:33.060,0:11:36.120
이 멋진 아이디어를 생각 해 냈습니다

0:11:36.660,0:11:40.060
만약 p 값이 0.01이라고

0:11:40.060,0:11:42.700
상상해 봅시다.

0:11:42.700,0:11:47.300
그러면, 계층의 activation들 중 1%만을
무작위로 버리게 되는데

0:11:47.300,0:11:52.280
무작위로 별로 큰 변화를 주진 못 할 것입니다

0:11:52.560,0:11:55.760
그 말은 과적합 발생으로 부터

0:11:55.760,0:11:59.160
별로 보호 받을 수 없다는 것입니다.
반면에,

0:11:59.325,0:12:02.495
만약 p 값이 0.99 이라면

0:12:02.495,0:12:05.745
거의 대부분의 모든것을

0:12:05.760,0:12:07.940
버리게 됩니다.

0:12:07.940,0:12:09.540
그러면,

0:12:09.540,0:12:12.000
과적합이 발생하기가 어려워 져서

0:12:12.000,0:12:14.480
일반화에 좋을 수 있지만,

0:12:14.480,0:12:17.080
동시에 좋은 정확도 또한 얻을 수 없게 됩니다

0:12:17.200,0:12:20.635
일종의 트레이드 오프가 있을 수 있다는 이야기죠

0:12:20.640,0:12:23.340
높은 p값은 일반화에 좋지만

0:12:23.360,0:12:26.105
정확도가 낮아지는 반면,

0:12:26.105,0:12:29.185
낮은 p값은 일반화에 좋진 않지만

0:12:29.185,0:12:31.660
더 나은 정확도를 가져다 줍니다

0:12:31.860,0:12:35.135
학습 초기에, 검증 데이터셋의 손실값이

0:12:35.140,0:12:38.500
학습 데이터셋의 손실값보다 나은 경향을

0:12:38.500,0:12:42.135
보인다는 사실에, 
왜 그런지 궁금하신 분들이 있을 겁니다

0:12:42.140,0:12:46.760
미지의 데이터셋(아직 보지 못한)에 대한 손실이

0:12:46.760,0:12:50.580
학습 중인 데이터셋의 손실보다 더 낫다는 것이

0:12:50.585,0:12:53.675
얼핏 보기엔 이상한 일이기 때문입니다.

0:12:53.675,0:12:56.905
그 이유는
검증 데이터셋에 대한 "검증"을 수행할 땐

0:12:56.905,0:12:58.940
dropout 기능을 끄기 때문입니다

0:12:58.940,0:13:01.915
다시 말해보면, inference(추론)을 할 때

0:13:01.915,0:13:04.605
즉, 이미지가 고양이냐 개냐를 판단 할 때

0:13:04.605,0:13:07.615
무작위의 dropout을 사용하고 싶진 않을 것입니다.

0:13:07.615,0:13:10.435
학습된 최상의 모델을 사용하고 싶을 테니까요.

0:13:10.880,0:13:14.220
따라서, 모델에 dropout이 사용된 경우

0:13:14.220,0:13:18.560
특히 학습 초기에
검증 데이터셋의 정확도와 손실이

0:13:18.560,0:13:22.140
더 나은 경향을 보이게 됩니다.

0:13:22.140,0:13:23.680


0:13:23.800,0:13:25.620
(질문)

0:13:25.840,0:13:28.620
activations들이 버려진다는 사실에 대해서

0:13:28.620,0:13:32.380
우리가 특별히 뭔가를 해야 하나요?

0:13:32.380,0:13:34.780
아주 좋은 질문 입니다

0:13:34.840,0:13:37.855
저희(fastai)는 안해도 되지만,
PyTorch는 뭔가를 합니다

0:13:37.860,0:13:41.340
PyTorch의 내부에서는
두 가지 일이 일어나는데

0:13:41.340,0:13:45.660
p=0.5 인 경우,
activation의 절반은 버려 지지만

0:13:45.660,0:13:47.240
또한,

0:13:47.240,0:13:51.180
존재하는 activation을 두 배 증가시킵니다

0:13:51.320,0:13:55.720
따라서, 평균 activation 수는 변함이 없게 됩니다

0:13:56.180,0:13:58.860
꽤나 좋은 방법 이죠

0:13:59.080,0:14:02.100
어쨋든 걱정하실 필요는 없습니다

0:14:02.100,0:14:04.580
내부적으로 핸들링 되니까요

0:14:05.020,0:14:09.280
ps 라는 인자 값을 
설정해 줄 수 있습니다.

0:14:09.280,0:14:12.755
ps 인자는 모든 추가된 
계층에 적용되는 것으로

0:14:12.755,0:14:15.595
fastai 라이브러리를 통해서

0:14:15.600,0:14:19.660
추가된 계층에 어떤 dropout을 넣어주고자 하는지
결정하기 위한 것입니다

0:14:19.660,0:14:21.880
미리학습된 네트워크의

0:14:21.885,0:14:24.845
dropout을 변경하지는 않습니다.

0:14:24.845,0:14:27.705
이미 적당한 수준의 dropout이 적용되어

0:14:27.705,0:14:30.625
있기 때문에, 이를 변경할 필요는 없습니다.

0:14:30.625,0:14:34.300
추가된 계층들에의 모든 dropout의
p값 만을 설정하기 위한 것입니다.

0:14:34.300,0:14:36.760
코드를 보시면
ps=0.5라고 했는데,

0:14:36.960,0:14:41.120
첫 번째와 두 번째의 dropout이 모두
0.5인 것을 보실 수 있습니다

0:14:41.120,0:14:44.100
출력된 추가된 계층 정보의 입력 부분은

0:14:44.100,0:14:47.560
미리 학습된 네트워크의
마지막 컨볼루션 계층의 출력과 연결되어 있습니다

0:14:47.820,0:14:50.735
그래서, 추가된 첫 번째 Linear 계층을 시작하기 전

0:14:50.740,0:14:54.100
그 출력의 절반을 버리게 되는 것입니다.

0:14:54.300,0:14:56.980
그리곤 음수값들을 버리고 (ReLU),

0:14:56.980,0:15:00.600
그로부터 다시 절반을 버리게 되고,
그 결과로 두 번째 Linear 계층을 수행합니다

0:15:00.600,0:15:03.420
그 후, 마지막으로 Softmax를 
수행하게 됩니다

0:15:03.420,0:15:05.775
마이너한 
수의 정밀도에 대한 이유로

0:15:05.780,0:15:08.180
Softmax를 바로 사용하는 것 보다

0:15:08.180,0:15:11.495
Softmax의 LOG를 사용 하는 것이
더 낫다고 밝혀졌습니다.

0:15:11.495,0:15:14.555
이 방법을 사용했기 때문에,

0:15:14.555,0:15:17.925
모델의 예측 결과에 대해서

0:15:18.000,0:15:20.560
np.ex() 를 사용 해야 했던 것입니다.

0:15:20.780,0:15:24.160
왜 그런지의 상세한 내용은
크게 중요하진 않습니다

0:15:24.165,0:15:27.145
만약 droupout을 제거하고 싶다면,

0:15:27.145,0:15:30.060
ps=0 로 값을 설정해 주면 됩니다.

0:15:30.080,0:15:33.360
이 둘을 비교해 보면
droupout이 적용된 첫 번째 에포크엔

0:15:33.365,0:15:36.305
0.76의 정확도가 나왔지만,

0:15:36.365,0:15:39.795
droupout이 미적용된 첫 번째 에포크에선
0.80의 정확도가 나왔습니다.

0:15:40.000,0:15:43.340
즉, droupout을 사용하지 않으면
첫 번째 에포크의 결과는 더 좋습니다

0:15:43.340,0:15:45.600
아무것도 버리지 않기 때문입니다

0:15:45.600,0:15:48.560
하지만, 세번째 에포크를 보시면
dropout이 사용된 경우 0.848를

0:15:48.560,0:15:51.575
그렇지 않은 경우 0.841의 정확도를 얻었습니다

0:15:51.575,0:15:54.575
dropout 대비 시작은 더 좋았지만,

0:15:54.580,0:15:58.940
세 번의 에포크 후엔 그렇지 않게 되었습니다.
보시다시피 매우 과적합 되어 있습니다

0:15:58.940,0:16:02.120
학습 데이터셋에 0.35 손실을 보이고

0:16:02.120,0:16:04.880
검증 데이터셋에는 0.55의 손실을 보입니다

0:16:05.460,0:16:08.545
지금 이 모델의 모습을 보면,

0:16:08.545,0:16:11.485
즉, p값이 0이면, dropout이 추가되지

0:16:11.485,0:16:15.380
않는 것을 확인할 수 있는 것입니다.

0:16:18.520,0:16:22.600
한가지 더 언급할 내용으론

0:16:22.600,0:16:27.780
두 개의 Linear 계층이 추가된 것에 대한 것인데

0:16:27.780,0:16:32.355
꼭 그럴 필요는 없다는 것입니다.

0:16:32.355,0:16:35.335
xtra_fc 라는 인자가 있는데,

0:16:35.340,0:16:38.440
추가 할 fully connected 계층들이

0:16:38.440,0:16:42.580
얼마나 클지에 대한 리스트를 넣어줄 수 있습니다.

0:16:42.580,0:16:45.080
다만, 최소한 하나의

0:16:45.085,0:16:48.145
fully connected 계층은 있어야 합니다.

0:16:48.145,0:16:51.175
컨볼루션 계층의 결과를 받아들여서

0:16:51.175,0:16:53.805
(여기서는 1024의 입력)

0:16:53.805,0:16:57.085
분류 카테고리의 수 만큼의
출력을 계산해야 하기 때문입니다.

0:16:57.085,0:16:59.900
출력의 크기는 고양이/개 분류의 경우 2이며

0:16:59.900,0:17:02.595
개 품종 분류 경우는 120이며

0:17:02.595,0:17:05.935
planet 위성사진 분류는 17개 였습니다

0:17:05.940,0:17:09.660
최소 하나의 Linear 계층이 필요한 이유 입니다
그리고, 분류 목록 개수에 의존적인

0:17:09.660,0:17:12.800
이 계층의 크기는 우리가 선택할 수 없습니다

0:17:12.800,0:17:15.640
하지만, 다른 Linear 계층에 대해선
선택이 가능합니다

0:17:15.685,0:17:18.705
만약 비워진 리스트를 인자로 넣어주면

0:17:18.705,0:17:22.005
꼭 필요한 하나의 Linear 계층 이외의

0:17:22.005,0:17:24.885
어떤 추가 Linear 계층도 추가하지 않게 됩니다.

0:17:24.885,0:17:28.280
ps=0, 그리고 xtra_fx=[ ]로 설정 했기 때문에

0:17:28.400,0:17:31.920
가장 최소한으로

0:17:32.100,0:17:36.220
추가할 수 있는 모델의 형태가 됩니다

0:17:36.460,0:17:43.380
이 처럼 생성한 모델을 사용한
경우의 결과를 보실 수 있는데요,

0:17:43.640,0:17:47.780
이 노트북의 특정 상황에 대해선
꽤 합리적으로 좋은 결과를 보여 줍니다

0:17:47.785,0:17:50.845
별로 오랫동안 학습을 시키지 않았고

0:17:50.845,0:17:54.660
이 특정 모델이, 주어진 특정 상황에
잘 들어맞기 때문이지요

0:17:55.060,0:18:01.420
>> 그러면, 디폴트로 어떤형태를 사용하는게 좋을까요?

0:18:01.800,0:18:05.835
일단, 디폴트로 제공되는 형태는

0:18:05.835,0:18:08.835
첫 번째 Dropout의 p값이 0.25이고

0:18:08.840,0:18:11.500
두 번째는 0.5입니다

0:18:11.500,0:18:16.040
이 정도 설정이 대부분의 상황에서
꽤 잘 동작하는 것 같더군요

0:18:16.040,0:18:17.580
그러니까

0:18:17.580,0:18:20.840
어떤 변화도 주지 않으셔도
좋을 것 같다는 것입니다.

0:18:21.000,0:18:23.800
만약 과적합을 목격 하신다면

0:18:23.800,0:18:26.740
일단 ps=0.5로 설정해서

0:18:26.740,0:18:30.300
두 계층 모두의 p를 0.5로 
설정하는 해 보는 것으로 시작을 해 봅시다

0:18:30.305,0:18:33.265
그래도 여전히 과적합이 일어난다면

0:18:33.265,0:18:36.065
값을 0.7로 증가시켜서
무엇이 알맞은 값인지

0:18:36.065,0:18:39.075
범위를 좁혀 나갈 수 있습니다.

0:18:39.075,0:18:42.095
만약, 희소적합이 발생하는 경우라면

0:18:42.100,0:18:44.560
ps 값을 낮춰볼 수 있겠죠.

0:18:44.860,0:18:48.420
아주 값을 낮출일은 많이 없을 것입니다.

0:18:48.420,0:18:52.120
개/고양이 분류 문제에서 조차도,

0:18:52.200,0:18:55.855
값을 낮출 필요가 없었습니다.

0:18:55.860,0:18:59.360
반면에, 0.6이나 0.7로 값을
증가시킬 상황이 더 자주 있습니다.

0:18:59.460,0:19:02.440
여러분이 직접 여러가지를 시도해 보실 수 있지만,

0:19:02.460,0:19:06.940
제 경우, 대부분 상황에서 디폴트로 제공되는
형태가 잘 동작함을 발견할 수 있었습니다.

0:19:06.940,0:19:09.175
저는 개 품종 분류 문제에서는

0:19:09.180,0:19:12.160
이 값을 증가시킨 적이 있습니다.

0:19:12.160,0:19:14.655
더 큰 모델을 사용했을때,

0:19:14.655,0:19:17.705
두 계층 모두 값을 0.5로 설정 했었는데

0:19:17.705,0:19:20.725
예를 들어서, ResNet34는 비교적 더 적은 수의

0:19:20.725,0:19:23.915
파라메터로 구성 되므로 과적합이 덜 발생합니다.

0:19:23.915,0:19:26.685
하지만, 이후에 ResNet50을 시도하는 경우

0:19:26.685,0:19:29.635
훨씬 더 많은 파라메터가 존재하여
과적합이 발생함을 확인할 수 있고,

0:19:29.635,0:19:32.575
결과적으로 dropout 확률값을 증가 시키게 됐습니다.

0:19:32.580,0:19:36.580
덩치가 더 큰 모델을 사용하면,

0:19:36.580,0:19:40.520
종종 더 많은 dropout / 더 큰 값의 dropout 을
추가해야 하는 경우가 생기게 됩니다.

0:19:41.980,0:19:44.800
>> p 값을 0.5로 설정하게 되면

0:19:44.800,0:19:47.740
>> 정확히 이게 몇 퍼센트인가요?

0:19:47.740,0:19:49.560
50% 입니다.

0:19:49.800,0:19:51.220


0:19:53.365,0:19:56.335
>> 만약 과적합이 발생하면, 이를 알아내기 위한

0:19:56.340,0:19:58.700
>> 특정 방법이란게 있나요?

0:19:59.180,0:20:00.620
네 있습니다.

0:20:00.695,0:20:03.425
여기를 보시면
(스크롤 내리는 중)

0:20:03.700,0:20:07.080
학습 데이터셋에 대한 손실이, 검증 데이터셋에 대한것 보다

0:20:07.080,0:20:10.400
훨씬 낮음으로 확인 할 수 있습니다.

0:20:10.460,0:20:14.560
과적합률이 전혀 없는것이 일반적으로
최적이 아닌 것 처럼

0:20:14.660,0:20:18.800
과적합의 정도가 큰지에 대해선 알 수가 없습니다.

0:20:18.805,0:20:21.745
단지 우리가 해야 할 것은 검증 데이터셋에 대한

0:20:21.745,0:20:24.825
손실값이 더 낮도록 만드는 것입니다.

0:20:24.825,0:20:27.625
결국은 여러가지를 시도해 보고

0:20:27.625,0:20:30.340
그 중 어떤 시도가 검증 데이터셋에 대한

0:20:30.340,0:20:33.220
손실값을 더 낮도록 만들었는지 알아내야 할 것입니다.

0:20:33.380,0:20:36.680
하지만, 특정 문제에 대해서 계속
시도 해 보다 보면,

0:20:36.800,0:20:40.600
과적합이 크게 발생하는게
어떤 것인지에 대한 느낌이 오실 겁니다.

0:20:43.640,0:20:44.700


0:20:45.515,0:20:48.165
지금까지 설명 드린 것이 dropout이고

0:20:48.165,0:20:51.205
앞으로 많이 사용하게 될 것입니다.

0:20:52.455,0:20:54.995
>> 두 가지 질문이 있습니다.

0:20:54.995,0:20:58.025
>> 첫 번째는, dropout 율이 0.5일때,

0:20:58.025,0:21:03.680
>> 각 셀을 지울때 50% 확률로 지우는 것인가요?

0:21:03.860,0:21:10.020
>> 아니면, 50%의 셀을 무작위로 선택하는 건가요?

0:21:10.020,0:21:12.720
전자가 맞습니다.

0:21:13.220,0:21:18.360
>> 두 번째 질문은, 왜 평균 activation이 중요 한가요?

0:21:18.940,0:21:24.980
엑셀로 설명을 드려 보겠습니다.

0:21:25.100,0:21:30.700
예를 들어서, 이 셀의 결과 값은

0:21:33.120,0:21:38.020
직전 계층의 이 9개 셀에, 필터 두 개의 9개 값을

0:21:38.140,0:21:41.520
각각 곱한 후 더한 것입니다.

0:21:41.520,0:21:44.920
만약, 직전 계층의 9개 셀의 절반을 삭제한다면

0:21:44.920,0:21:47.295
결과도 절반으로 줄어들게 되고

0:21:47.295,0:21:50.325
그 다음의 결과들도 바뀌게 되어서

0:21:50.325,0:21:52.265
의미 자체가 변하게 됩니다.

0:21:52.840,0:21:56.020
예를 들어서, activation 값이 0.6일때

0:21:56.020,0:21:58.800
솜털같은 귀라는 인식되던 것이

0:21:58.805,0:22:01.505
이제는 그 값이 0.3일때
솜털같은 귀라고 인식된다면,

0:22:01.505,0:22:04.175
그 의미 자체가 변하게 되는 것입니다.

0:22:04.180,0:22:07.420
dropout의 목표는 의미를 바꾸지 않은 채

0:22:07.420,0:22:10.360
activation들을 삭제하는 것입니다.

0:22:12.400,0:22:15.095
>> 왜 Linear activation을 사용 하나요?

0:22:15.095,0:22:18.005
왜 Linear를 사용 하느냐고요?

0:22:18.260,0:22:21.800
>> 네, 왜 그 특정 activation인가요?

0:22:22.120,0:22:25.475
이 계층들의 집합이 그러한 것입니다.

0:22:25.475,0:22:27.685
미리 학습된 네트워크는

0:22:27.685,0:22:30.625
컨볼루션 네트워크인데, 그 내용은
미리 계산되어 있는 것이라서

0:22:30.625,0:22:33.535
여기선 보이지 않습니다.

0:22:33.535,0:22:36.545
그리고 이 네트워크가 내뱉는 결과는
벡터입니다.

0:22:36.545,0:22:40.420
따라서,  이 시점에서 유일하게 선택 가능한 것은
Linear 계층을 사용하는 것입니다.

0:22:40.700,0:22:45.260
>> 레이어 단위로, 
서로 다른 dropout을 사용 할 수 있나요?

0:22:45.260,0:22:46.925
물론 입니다

0:22:46.925,0:22:49.845
>> 어떻게 할 수 있죠?

0:22:49.925,0:22:53.025
당연히 레이어 마다 서로다른 dropout을
사용할 수 있습니다.

0:22:53.025,0:22:56.095
이 파라메터명이 ps (복수)라고 불리는 이유 입니다.

0:22:56.095,0:22:58.645
배열을 넣어줄 수 있는 것이죠

0:22:59.060,0:23:00.380
예를 들어서,

0:23:00.380,0:23:03.015
ps=[0, 0.2]

0:23:03.015,0:23:05.995
그리고 xtra_fc=[512] 라고 해주면

0:23:05.995,0:23:09.125
첫 번째엔 dropout이 없게되고

0:23:09.125,0:23:11.975
두 번째엔 0.2의 dropout이 추가 됩니다.

0:23:12.200,0:23:14.440
좋은 질문 감사합니다.

0:23:14.940,0:23:17.960
저도 수 년동안 사용해 왔지만,

0:23:17.960,0:23:21.240
dropout 대한 어떤 좋은 직관력은 없습니다.

0:23:21.440,0:23:24.460
초반/후반 계층들이 언제 서로다른

0:23:24.465,0:23:27.425
dropout률을 가져야 한다던지와 같은 주제에 대해서

0:23:27.425,0:23:30.485
저도 아직 여러가지 시도를
해보고 있는 입장입니다.

0:23:30.485,0:23:33.575
그리고, 아직도 어떤 경험적인 
규칙을 찾지 못했습니다.

0:23:33.580,0:23:35.780
여러분이 이 규칙을 찾으신다면

0:23:35.780,0:23:38.800
그 내용을 들어보고 싶군요.
하지만, 확신이 서질 않는다면,

0:23:38.840,0:23:42.760
모든 fully connected 계층에 똑같은
dropout을 사용하셔도 괜찮습니다.

0:23:42.760,0:23:44.715
그 외에 시도 해 보실만한 것은

0:23:44.715,0:23:47.895
종종 사람들이 가장 마지막 Linear 계층에만

0:23:47.945,0:23:50.575
dropout을 추가한다는 사실이 있습니다.

0:23:50.575,0:23:52.575
이 두 가지를 시도 해 볼만 합니다.

0:23:54.180,0:23:57.560
>> 선생님, 정확도 대신에 손실값을

0:23:57.560,0:24:00.840
>> 모니터링 하는 이유가 뭔가요?

0:24:02.260,0:24:07.140
손실이 학습과 검증 데이터셋 모두에서

0:24:07.140,0:24:10.960
확인 가능한 유일한 것이기 때문입니다.

0:24:11.020,0:24:14.000
이 두 데이터셋에 대한 비교를 할 수 있게 해줍니다

0:24:14.180,0:24:17.320
또한, 추후 배울 내용 입니다만

0:24:17.320,0:24:20.800
손실은 알고리즘이 최적화 하는 대상입니다.

0:24:20.800,0:24:24.620
그렇기 때문에 손실을 모니터링 해 보면

0:24:24.660,0:24:28.920
좀 더 의미 파악이 쉬울 수 있습니다.

0:24:30.040,0:24:31.640


0:24:34.580,0:24:37.380
>> dropout을 사용하면, 일종의 랜덤 노이즈를

0:24:37.380,0:24:39.660
>> 매 iteration마다 추가 하는것 같습니다.

0:24:39.780,0:24:43.780
>> 그렇다면, 충분한 학습을 못한다는 의미인 것 같습니다

0:24:43.780,0:24:45.155
맞습니다

0:24:45.160,0:24:49.400
>> 그러면 학습률을  이에 맞춰서 조정 해 줄 필요가 있나요?

0:24:49.400,0:24:51.620
제가 느낀바로는 학습률에

0:24:51.620,0:24:54.100
어떤 영향을 주진 않는 것 같습니다.

0:24:54.340,0:24:56.895
아마도 이론적으론 말씀 하신것이
맞을 수 있다고 생각합니다.

0:24:56.900,0:25:00.040
하지만, 아직까지 그런 상황을
경험해 보진 못했습니다.

0:25:03.920,0:25:04.920


0:25:04.960,0:25:07.140
그러면, 지금부터는

0:25:07.140,0:25:10.020
“구조화된 데이터” 문제를 다뤄보겠습니다.

0:25:10.020,0:25:13.780
상기 차원에서 말하자면, 사용될 예제는

0:25:13.800,0:25:20.060
독일 수퍼마켓 브랜드인 
Rossman에 대한 Kaggle 경연입니다.

0:25:20.500,0:25:24.980
이 파일은 lesson3-rossman.ipynb 이니 참고 해 주세요.

0:25:27.095,0:25:29.795
사용되는 주된 데이터셋은 특정 가게가

0:25:29.800,0:25:35.060
얼마나 많이 판매를 했는지를 예측하기 위한 것입니다.

0:25:36.280,0:25:40.540
이 데이터에는 몇 중요한 정보가 있는데, 
그 중 하나는 날짜(date) 이고

0:25:40.540,0:25:42.720
다른 하나는 그날의 개점 여부입니다.

0:25:42.835,0:25:45.815
또, 그날 프로모션이 있었는지

0:25:45.820,0:25:49.180
주지정 휴일이나

0:25:49.180,0:25:52.540
휴교일 이었는지

0:25:53.520,0:25:59.180
어떤 물건을 파는 
어떤 종류의 가게인지

0:25:59.180,0:26:02.400
경쟁 가게로 부터 얼마나 떨어져 있는지

0:26:02.400,0:26:04.595
등과 같은 것들이 있었습니다.

0:26:04.595,0:26:07.015
이렇게 생긴 데이터셋의 컬럼은

0:26:07.015,0:26:09.935
두 가지 종류로 주로 나눠볼 수 있습니다.

0:26:09.935,0:26:12.945
몇 단계 수준을 가지는 범주형 컬럼이 있을 수 있습니다.

0:26:12.945,0:26:16.300
Assortment가 범주형 컬럼의 한 예로

0:26:16.300,0:26:19.300
a, b, c와 같은 수준의 범주를 가집니다.

0:26:20.325,0:26:22.505
반면에,

0:26:22.505,0:26:25.525
CompetitionDistance 컬럼은 계속형 이라고 부르는데

0:26:25.525,0:26:31.460
차이나 비율등 어떤 의미있는 
숫자 값이 저장될 수 있습니다.

0:26:31.780,0:26:34.040
이 두 가지 종류의 컬럼은 꽤

0:26:34.040,0:26:36.040
서로 다른 방법으로 다뤄져야 합니다.

0:26:36.445,0:26:39.145
어떤 종류든 머신러닝을 해 봤고

0:26:39.145,0:26:42.015
파라메터에 의해서 값이 변화하는

0:26:42.015,0:26:45.005
선형 회귀 문제를 풀어보신 분이라면,

0:26:45.005,0:26:48.320
계속형 컬럼과 친숙하실 것입니다.

0:26:49.460,0:26:52.780
범주형 컬럼은 이 보다는 좀 더 생각해 볼게 많습니다.

0:26:54.215,0:26:57.125
이 코스는 데이터 클리닝을 다루진 않습니다.

0:26:57.125,0:26:59.200
Feature 엔지니어링의 일부로,

0:26:59.200,0:27:02.040
이 과정이 이미 되어 있다고 
가정하고 진행하겠습니다.

0:27:02.480,0:27:05.595
이 Feature 엔지니어링이 끝난 후,

0:27:05.595,0:27:08.785
컬럼들의 목록을 출력 했습니다.

0:27:08.785,0:27:11.280
이 출력 결과에 대해선

0:27:11.280,0:27:14.295
제가 직접 feature 엔지니어링이나

0:27:14.295,0:27:17.135
데이터 클리닝 작업을 한 것은 전혀 없습니다

0:27:17.135,0:27:19.845
이 모든 것은 경연의 3등을 한 분의

0:27:19.845,0:27:21.720
코드를 빌려온 것입니다.

0:27:22.080,0:27:26.060
이 분이 유용하다고 판단한

0:27:26.220,0:27:29.340
여러가지 컬럼들의 리스트인 것입니다.

0:27:29.620,0:27:33.760
그 중 첫번째 컬럼들의 리스트는

0:27:33.760,0:27:37.760
범주형 컬럼으로 다뤄질 것들에 대한 것입니다.

0:27:39.060,0:27:43.400
년(Year), 월(Month), 일(Day) 같은 숫자는

0:27:43.740,0:27:47.840
2003년과 2000년 의 차이가 의미가 있을 수 있는

0:27:47.840,0:27:51.680
계속형으로 다룰 수도 있는 컬럼이지만

0:27:52.080,0:27:56.260
꼭 그럴 필요는 없습니다.

0:27:56.360,0:28:00.940
어떻게 범주형 데이터가 다뤄지는지
곧 보여 드리겠습니다

0:28:00.940,0:28:02.455
하지만, 기본적으론

0:28:02.455,0:28:05.405
어떤 데이터를 범주형으로 분류하게 되면

0:28:05.405,0:28:09.720
2000, 2001, 2002와 같은 
모든 수준의 Year 정보는

0:28:09.720,0:28:15.400
뉴럴넷에서 완전히 다른 형태로 
다뤄질 수 있게 됩니다.

0:28:16.020,0:28:18.420
반면에, 계속형이라면

0:28:18.420,0:28:21.920
계속적인 상관관계를 나타내는

0:28:21.920,0:28:24.560
어떤 함수를 생각해 내야 할 것입니다

0:28:24.965,0:28:28.335
Year와 같은 것은 종종 계속형으로 다뤄집니다

0:28:28.340,0:28:32.660
하지만, 실제로 그렇게 많은 수준이 
존재하진 않는다면

0:28:32.900,0:28:37.380
범주형으로 다뤄질 때, 
더 잘 동작하는 경우가 많습니다.

0:28:37.385,0:28:40.435
0~6 범위로 표현되는 DayOfWeek (월/화..) 가

0:28:40.440,0:28:43.080
하나의 좋은 예가 될 수 있습니다.

0:28:43.540,0:28:46.585
단순히 의미를 가지는 숫자로

0:28:46.585,0:28:51.020
3과 5라면 이틀이라는 의미가 될 수 있습니다.

0:28:51.640,0:28:54.520
하지만, 어떻게 특정 가게의 판매량이

0:28:54.520,0:28:57.440
DayOfWeek에 영향을 받을 수 있는지 생각 해 보면

0:28:57.440,0:29:00.280
토/일요일의 판매량은 이정도가 될 수 있고

0:29:00.280,0:29:03.540
금요일과 수요일은 또 다른 정도로

0:29:03.540,0:29:06.680
각 날에 대해서 질적으로 다르게 동작하게 됩니다.

0:29:06.860,0:29:09.840
그래서, 뉴럴넷이 범주형 데이터를

0:29:09.840,0:29:13.340
그런식으로 다루는 것을 곧 보시게 될 것입니다.

0:29:14.140,0:29:18.455
어떤게 범주형이고 어떤게 계속형인지는

0:29:18.460,0:29:22.940
어느 정도 모델링 할 때 결정되어야 하는 사항입니다.

0:29:23.520,0:29:27.840
어떤 데이터가 a, b, c 또는 Jeremy, Yannet (이름)

0:29:27.840,0:29:31.440
처럼 코드화 되어 있는 경우는

0:29:31.440,0:29:35.840
반드시 범주형으로 다뤄져야 합니다

0:29:36.300,0:29:40.280
이 데이터를 직접 계속형으로 
다룰 수 있는 방법은 없기 때문입니다.

0:29:40.280,0:29:41.620
반면에,

0:29:41.620,0:29:44.455
age, dayOfWeek 같이 계속형 “적인”

0:29:44.460,0:29:48.320
데이터의 경우는 계속형으로 다룰지, 범주형으로 다룰지

0:29:48.340,0:29:51.820
여러분이 직접 결정 하셔야 합니다.

0:29:51.935,0:29:54.815
요약 해 보면, 범주형 데이터는

0:29:54.815,0:29:57.805
모델에게 범주형으로만 다뤄져야 하고

0:29:57.805,0:30:00.285
계속형 데이터는 모델이 이를 계속형으로 다룰지

0:30:00.285,0:30:03.645
범주형으로 변형해서 다룰지를 선택해야 합니다.

0:30:04.135,0:30:07.385
여기서는 그게 뭐가 되었든

0:30:07.425,0:30:10.455
경연의 3등을 한 사람이 한 내용 입니다.

0:30:10.460,0:30:12.620
위에 (cat_vars)는 범주형 데이터이고

0:30:12.620,0:30:15.420
아래 (contin_vars)는 계속형으로 결정 했습니다

0:30:15.460,0:30:17.660
보시면 기본적으로

0:30:18.140,0:30:20.925
계속형으로 선택된 컬럼은 모두

0:30:20.925,0:30:23.535
사실상 실수형의 숫자로 표현되는 것들입니다.

0:30:23.540,0:30:27.900
CompetitionDistance는 소수점으로 표현되는
숫자값을 가지고,

0:30:27.900,0:30:31.740
마찬가지로 Temperature(온도)도 소수점
숫자인 것 처럼 말이죠.

0:30:31.740,0:30:34.320
이런 종류의 데이터를 범주형으로 
변형하기란 어렵습니다

0:30:34.320,0:30:37.020
너무 많은 수준이 존재하기 때문입니다.

0:30:37.120,0:30:40.720
만약 소숫점 다섯 째 까지 표현하는 데이터라면

0:30:40.720,0:30:44.480
거의 모든 데이터가 범주로 다뤄질 수 있겠죠

0:30:45.220,0:30:46.560
그런데

0:30:46.680,0:30:49.440
여기서 사용된“얼마나 많은 수준”이라는 말은

0:30:49.440,0:30:52.160
Cardinality 라고 표현 하도록 하겠습니다.

0:30:52.160,0:30:56.900
제가 Cardinality 라고 말하면,
예를 들어서 DayOfWeek 의 Cardinality는  7입니다.

0:30:56.900,0:30:59.320
7개의 서로 다른 요일이 있기 때문입니다.

0:31:02.600,0:31:07.300
>> 계속형 데이터를 bin(통계학의) 하기도 하나요?

0:31:07.300,0:31:10.840
저는 그렇게 해 본적이 없습니다.

0:31:11.820,0:31:14.055
Max_TemperatureC 같은 데이터를

0:31:14.060,0:31:18.260
0~10, 10~20, 20~30 처럼 그룹핑 해서

0:31:18.260,0:31:21.960
범주형으로 만들 수도 있을 겁니다.

0:31:21.960,0:31:26.040
흥미롭게도, bin을 만드는게 때로는 유용 하다는

0:31:26.040,0:31:30.580
내용의 한 논문이 지난 주에 나왔습니다.

0:31:31.380,0:31:33.960
근데 말 그대로 지난 주에 발표된 것이고

0:31:33.960,0:31:36.980
그 전까진 딥러닝에서 이 기법이 사용된 것을
본 적이 없습니다.

0:31:36.980,0:31:40.800
아직 논문 내용을 자세히 보진 않았습니다.
이번주 까진, 별로 좋지 않은 생각이라고

0:31:40.800,0:31:44.560
말씀 드리고 싶지만, 논문이 발표 되었으니
한번 생각 해 볼 만하다고 말할 수도 있겠습니다

0:31:48.520,0:31:49.520


0:31:49.600,0:31:53.500
>> 만약 Year을 범주형으로써 사용한다면

0:31:53.500,0:31:58.120
>> 학습때는 한번도 본 적 없는 Year가 
테스트에서 사용되면 어떤일이 발생 할까요?

0:31:59.240,0:32:02.260
잠시 후 설명 드릴게요.
간단히만 설명 드리면

0:32:02.265,0:32:05.365
알려지지 않은(unknown) 카테고리로써
다뤄질 것입니다.

0:32:05.365,0:32:09.460
DataFrame 에 사용되는 pandas 에는

0:32:09.480,0:32:13.680
특별한 unknown 이라는 카테고리가 있는데

0:32:13.680,0:32:17.560
한번도 본적이 없는 카테고리에 이를 적용합니다

0:32:18.480,0:32:23.460
그러므로, 딥러닝 모델 입장에선 unknown은 
또 다른 종류의 카테고리가 될 뿐이죠

0:32:25.620,0:32:31.260
>> 만약 학습 데이터셋엔 
unknown 카테고리가 없는데

0:32:31.260,0:32:35.440
>> 테스트 데이터셋엔 unknown이 있으면
 어떻게 되나요?

0:32:35.440,0:32:37.580
그냥 unknwon 으로써

0:32:37.580,0:32:39.800
핸들링 될 것입니다.

0:32:40.140,0:32:43.260
unknown이 0과 같은 값을 가진다고 생각 해 보면,

0:32:43.260,0:32:45.840
여전히 뭔가를 "예측"도 할 것입니다.

0:32:46.020,0:32:49.880
만약 학습 데이터의 많은 
데이터들이 unknown 이라면,

0:32:49.880,0:32:52.760
unknown에 대해서 예측할 수 있는법을 
학습할 것입니다.

0:32:52.760,0:32:56.920
하지만, 학습 데이터에 unknwon 데이터가 없다면,
예측 결과는 랜덤한 추측이 될 것입니다.

0:32:57.100,0:33:00.920
이 코스의 일부로써 이야기 해 볼 만한
학습에 관련된 흥미로운 질문이었습니다.

0:33:00.920,0:33:04.280
물론 포럼에서 토론 해 볼 수 있겠죠?

0:33:06.380,0:33:11.220
어쨋든, 범주형과 계속형 컬럼 목록을 정의 했습니다

0:33:11.220,0:33:12.320
여기선

0:33:12.320,0:33:18.760
Store와 Date 테이블을 Join한 결과의 갯수가
800,000 이상 존재 합니다.

0:33:19.580,0:33:24.580
그러면, 이제는 이 모든 컬럼들을 하나씩

0:33:24.580,0:33:29.320
확인 해 보면서 이들의 데이터 타입을

0:33:29.320,0:33:34.960
category로 변경할 수 있습니다.

0:33:35.680,0:33:38.695
pandas를 이용해서 수행한 것입니다.

0:33:38.695,0:33:41.775
제가 pandas를 가르치진 않을 것이지만,

0:33:41.775,0:33:44.695
유명한 McKinney 저서인 "python for data analysis"

0:33:44.700,0:33:46.960
외의 많은 책들이 있습니다.

0:33:46.960,0:33:49.840
하지만, 이 문법을 전에 본 적이 없더라도, 
여러분이

0:33:49.840,0:33:53.540
직관적으로 코드의 흐름을 파악할 수 있기를
희망 해 보겠습니다.

0:33:53.540,0:33:55.375
어쨋든 범주형 컬럼들의 타입을

0:33:55.380,0:33:57.560
category로 바꾸었습니다.

0:33:57.560,0:33:59.680
그리고, 계속형 컬럼의 데이터 타입은 모두

0:33:59.680,0:34:02.800
32비트의 float형으로 바꾸었습니다

0:34:02.800,0:34:07.180
이렇게 한 이유는 PyTorch에선 모든 것이

0:34:07.180,0:34:11.495
32비트의 float형의 사용이 기대되기 때문입니다.

0:34:11.500,0:34:18.060
계속형 컬럼의 몇몇, 
프로모션이나 휴일정보같은 것들은

0:34:18.060,0:34:21.895
0과 1로 표현이 되었었는데

0:34:21.895,0:34:24.495
이 값들이 소숫점을 가지는

0:34:24.500,0:34:28.680
실수형으로 바뀌게 되는 것입니다.

0:34:33.080,0:34:36.000
저는 가능한한 많은 일을 "적은 데이터셋"으로

0:34:36.000,0:34:39.335
해결 하려고 노력하곤 합니다.

0:34:39.335,0:34:42.415
이미지 관련 일을 한다면, 이미지를

0:34:42.415,0:34:45.795
64x64 또는 128x128 같은 크기로 
조절한다는 의미 입니다.

0:34:45.800,0:34:49.460
하지만, 구조화된 데이터는 이런식의 조절이
불가능 하기 때문에

0:34:49.460,0:34:52.820
전체 데이터셋에서 무작위로
적은 수의 샘플을 채취하는 방식을 사용합니다.

0:34:52.820,0:34:55.060
이렇게 일단 채취된 샘플로 
시작을 해 봅니다.

0:34:55.060,0:34:59.200
그리곤, 전제 검증 데이터셋을 구성하는 것과 
똑같은 방법으로

0:34:59.200,0:35:02.740
전체 데이터셋에서 랜덤하게 데이터를 선택해서

0:35:02.740,0:35:06.120
샘플 데이터를 구성하였는데,

0:35:06.120,0:35:11.080
이 코드 라인이 그 여러 개의 
무작위 수들을 뽑아주는 기능을 합니다.

0:35:12.660,0:35:15.720
전체 크기는 800,000 이었던 것에 비해서

0:35:15.720,0:35:19.440
이렇게 구성된 샘플 데이터셋의 크기는 150,000 입니다.

0:35:20.420,0:35:23.100
그 다음을 진행하기 전, 데이터의 모양을

0:35:23.105,0:35:26.115
살펴보면 이렇게 생긴 것을 알 수 있습니다.

0:35:26.115,0:35:27.905
Boolean형 데이터도 있고,

0:35:28.240,0:35:32.740
다양한 범위의 정수형 데이터도 있고,

0:35:32.740,0:35:36.580
여기 데이터의 Year는 2014 군요.

0:35:36.580,0:35:39.320
또, 문자형 값들도 보입니다.

0:35:39.320,0:35:43.700
이전 단계에서, pandas 에게 이것들을
category 로 만들어 달라고 요청했지만

0:35:43.720,0:35:46.540
pandas가 notebook 에선, 여전히

0:35:46.540,0:35:48.720
문자로서 이를 표현하고 있습니다

0:35:48.720,0:35:52.000
하지만, 내부적으로는 다른식으로 저장되게 됩니다.

0:35:52.220,0:35:54.875
fastai 라이브러리엔 proc_df() 라는

0:35:54.875,0:35:57.555
특별한 함수가 있습니다.

0:35:57.795,0:36:00.855
proc_df()에는 DataFrame과

0:36:00.860,0:36:03.660
의존 컬럼명을 인자로 주면

0:36:03.820,0:36:08.060
그 컬럼을 DataFrame으로 부터 추출 해서

0:36:08.060,0:36:11.080
독립적으로 따로 저장을 하고

0:36:11.080,0:36:14.140
원본 DataFrame에선 이 컬럼을 삭제하게 됩니다.

0:36:14.140,0:36:17.900
이 코드가 수행된 후 리턴되는 df에는 더 이상
Sales라는 컬럼이 존재하지 않게 되는 것입니다.

0:36:17.900,0:36:21.280
반면에, 리턴된 y에는 
Sales 컬럼의 내용이 담기게 됩니다.

0:36:21.280,0:36:23.800
그 외에 proc_df()가 하는 일은

0:36:23.800,0:36:25.160
스케일링 입니다.

0:36:25.360,0:36:30.360
뉴럴넷에 입력되는 데이터들은 표준 편차가 1인

0:36:30.360,0:36:35.960
0 근처의 값이 되는 것이 바람직 합니다.

0:36:35.960,0:36:39.615
데이터 값에서 전체 평균을 뺀 후, 
이를 표준편차로 나눠주면

0:36:39.615,0:36:42.615
이러한 값의 특성을 가지는 값으로 변형 되는데

0:36:42.665,0:36:45.735
do_scale 인자값이 True 이면, 
정확히 이를 수행 해 줍니다.

0:36:45.735,0:36:49.025
그러면, 특별한 객체를 리턴 해 주고 이를 통해서

0:36:49.025,0:36:52.820
정규화에 사용된 표준편차와 평균값 확인이 가능하고

0:36:52.820,0:36:54.515
테스트 데이터셋 구성시에

0:36:54.520,0:36:57.320
이를 활용해서 똑같은 분포의 
데이터를 구성할 수 있게 됩니다.

0:36:57.320,0:37:00.120
또한, 몇 빠진 값들도 핸들링 해 줍니다.

0:37:00.475,0:37:03.485
범주형 데이터에 대해서,

0:37:03.485,0:37:06.455
빠진 값들은 ID의 경우 0

0:37:06.460,0:37:12.000
그 외의 범주형 값들은 1, 2, 3 순으로 
채워지게 됩니다.

0:37:12.060,0:37:15.245
계속형 데이터의 경우엔,

0:37:15.245,0:37:18.640
빠진 값들은 중앙값으로 채워지게 되고

0:37:18.640,0:37:21.085
Boolean 형의 새로운 컬럼을 생성해서

0:37:21.085,0:37:24.145
이 데이터 행에 빠진 값이 있었는지를 명시 해 줍니다.

0:37:24.145,0:37:26.740
머신러닝 코스에서 자세히 다루는 내용이라서

0:37:26.740,0:37:29.520
이 부분의 설명을 아주 빠르게 진행하고 있습니다.

0:37:29.520,0:37:31.975
이 부분에 대해서 어떤 질문이 있으시다면,

0:37:31.975,0:37:34.045
딥러닝에 특화된 내용이 없는

0:37:34.045,0:37:36.480
머신러닝 코스를 수강 해 보면 좋을 것입니다

0:37:36.480,0:37:39.940
proc_df() 수행 후 결과를 보면,

0:37:39.975,0:37:43.175
예를들어 Year의 2014 값이 2로 바뀌었습니다

0:37:43.180,0:37:46.540
범주형 데이터이기 때문에, 각 범주의 값은

0:37:46.540,0:37:49.580
0부터 증가되는 값들이 할당 됩니다.

0:37:49.580,0:37:52.900
이렇게 하는 이유는 나중에 이 값들은

0:37:52.900,0:37:56.460
행렬로써 포장되어야 하는데

0:37:56.460,0:38:00.420
그 행렬이 2014개의 열이 되는 것 보단

0:38:00.420,0:38:04.520
2개의 열로 표현되면 좋기 때문 입니다.

0:38:04.960,0:38:07.760
a, c 였던 값들도 똑같은 방식으로

0:38:07.760,0:38:10.420
1, 3 값으로 대체 되었습니다.

0:38:10.900,0:38:13.985
이렇게 프로세싱된 DataFrame에는

0:38:13.985,0:38:17.005
의존컬럼이 포함되어 있지 않고,

0:38:17.005,0:38:20.155
모든 값들이 숫자입니다

0:38:20.160,0:38:25.740
이 과정이 딥러닝/RandomForest 
수행에 필요한 것이고

0:38:25.740,0:38:29.860
머신러닝 코스에서 다룬다고 했던, 윗 쪽에서

0:38:29.860,0:38:35.260
수행한 여러 작업들은 
딥러닝에 특화된 작업이 아닙니다.

0:38:36.320,0:38:39.335
머신러닝에서 많이 다루는 또 다른 내용은

0:38:39.335,0:38:42.415
검증 데이터셋에 관한 것입니다.

0:38:42.415,0:38:45.805
이 예제에서 예측해야 할 내용은

0:38:45.840,0:38:49.000
단순히 무작위로 판매량을 측정하는 것이 아니라

0:38:49.000,0:38:52.200
다음 2주동안의 판매량 입니다.

0:38:52.200,0:38:55.600
Kaggle 경연 주최자가 요구한 사항이죠.

0:38:55.880,0:38:59.300
그렇기 때문에, 
검증 데이터셋은 학습 데이터셋에서

0:38:59.300,0:39:03.480
마지막 2주 동안의 데이터로 생성 해 줍니다.

0:39:03.480,0:39:07.880
가능한 테스트 데이터와 
비슷하게 만들어 줘야 하기 때문입니다.

0:39:07.880,0:39:11.940
저희 기관의 Rachel이 얼마전 이 내용 관련의

0:39:11.940,0:39:16.120
글을 작성 했었는데,
fast.ai 에서 읽어보시길 권장합니다.

0:39:16.120,0:39:19.480
관련 링크를 레슨 wiki에도 올려 두겠습니다.

0:39:19.480,0:39:21.100
기본적으론

0:39:21.100,0:39:25.740
저희가 최근에 가르친 머신러닝 코스의 요약으로,

0:39:25.740,0:39:28.700
관련 비디오도 시청 가능하지만,

0:39:28.700,0:39:32.840
이 것은 비디오 내용을 글로 요약한 것입니다.

0:39:33.780,0:39:34.780


0:39:35.620,0:39:39.480
Rachel과 저는 어떻게 학습/검증/테스트 데이터셋이

0:39:39.480,0:39:43.560
다뤄져야 하는지를 생각하는데 많은 시간을 보냈습니다.

0:39:43.600,0:39:46.360
그 글에는 저희의 생각이 담겨져 있는데

0:39:46.360,0:39:48.920
다시 말하지만,
 딥러닝에 특화된 내용은 없습니다.

0:39:49.020,0:39:52.900
어쨋든, 이제부터는 딥러닝을 해 보도록 하겠습니다.

0:39:52.900,0:39:57.180
이 특정 경연 뿐만 아니라, 다른 경연이나

0:39:57.180,0:40:01.500
다른 종류의 머신러닝 프로젝트에서는

0:40:01.500,0:40:05.560
사용하는 metric에 대한
확고한 이해를 해야만 합니다.

0:40:05.560,0:40:08.600
결과가 어떻게 평가될지에 대한 것이죠.

0:40:08.600,0:40:11.800
Kaggle에서는 비교적 쉽게, 
어떻게 평가될지를 명시 해 줍니다.

0:40:11.800,0:40:16.680
이 경연은 RMSPE (Root Mean Squared 
Percentage Error) 로써 평가 될 것입니다.

0:40:16.680,0:40:22.960
이 함수는 실제 레이블값과 예측 결과값의

0:40:22.960,0:40:28.960
차이에 대한 평균을 백분율로 구하는 것입니다.

0:40:29.160,0:40:36.180
여러분이 Log에 대해서 잘 알아야 한다고
충고 해 드린 적이 있습니다.

0:40:36.180,0:40:40.400
여기선 예측 결과를 실제 결과로 나눈 것의

0:40:40.400,0:40:47.420
평균을 구해야 하는데,

0:40:47.740,0:40:54.860
PyTorch에는 RMSPE 라는 metric이 없습니다.

0:40:54.860,0:41:00.020
그러면, 소스를 확인 해 보면 
한줄 정도로 구현이 가능해서

0:41:00.020,0:41:04.180
직접 간단하게 이를 만들어 볼 수 있을 것입니다.

0:41:04.180,0:41:10.260
그러나, 아마도 가장 쉬운 방법은

0:41:10.340,0:41:14.040
만약 a'/b 라는 것이 있을 때

0:41:14.040,0:41:18.260
이것을 ln(a'/b')로 바꾸면

0:41:18.260,0:41:22.940
ln(a') - ln(b')처럼
뺄셈 형태로 표현이 가능 해 집니다.

0:41:23.260,0:41:26.580
Log의 단순한 법칙을 적용한 것입니다.

0:41:26.580,0:41:30.555
이런 법칙을 잘 모르시면, 관련 내용을
확인 해 보시기 바랍니다.

0:41:30.560,0:41:32.880
아주 유용할 겁니다.

0:41:32.880,0:41:38.940
어쨋든 여기선, 제가 미리 Notebook에 작성 해 뒀는데

0:41:38.940,0:41:44.440
데이터 값에 Log를 취해 주기만 하면,

0:41:44.440,0:41:50.280
별다른 노력 없이
RMSE로 RMSPE를 구할 수 있게 됩니다.

0:41:51.040,0:41:53.820
단, 출력 할 땐

0:41:53.820,0:41:58.520
Log가 취해진 값을 EXP()에 넣어줘야 합니다.

0:41:58.520,0:42:03.320
그래야 Log가 벗겨지고 
실제 백분율 값을 구할 수 있게 됩니다.

0:42:03.320,0:42:08.860
지금 보고계신 코드가 이 모든 것을 수행하는데
딥러닝에 특화된 내용은 전혀 아닙니다.

0:42:09.200,0:42:12.715
그 다음 셀에서, 
딥러닝 관련 부분이 '드디어' 시작됩니다.

0:42:12.720,0:42:17.000
여느 때와 같이, 오늘 보시게 될 모든 것은

0:42:17.000,0:42:20.360
지금까지 봐오던 것과 정확히 똑같을 것입니다.

0:42:20.360,0:42:21.720
우선 처음엔,

0:42:21.780,0:42:25.780
학습/검증/테스트(옵션) 데이터셋이 포함된

0:42:25.840,0:42:29.260
모델 데이터 객체를 만들게 됩니다.

0:42:29.260,0:42:32.580
그리고 나선, learn 객체를 만들고

0:42:32.580,0:42:35.465
learn.lr_find()를 수행하고

0:42:35.465,0:42:38.345
learn.fit()을 수행하는데

0:42:38.345,0:42:42.440
파라메터등 모든 것은
전에 보신 것과 동일한 형태로 설정 됩니다.

0:42:42.860,0:42:46.180
다른점도 있긴한데,

0:42:46.180,0:42:49.835
ImageClassifierData 를 이용한

0:42:49.840,0:42:54.100
이미지 때와는 다른 방식으로 
데이터를 가져와야 합니다.

0:42:54.100,0:42:57.780
열과 컬럼으로 구성된 이런 데이터에 대해선

0:42:57.780,0:43:01.040
ColumnarModelData 라는 것을 사용하는데

0:43:01.040,0:43:05.395
이것에 의해 리턴된
객체는 이미 친숙하신 형태의 API를 제공합니다.

0:43:05.400,0:43:08.780
그리고, 이미지때 사용된, 
from_paths(), from_csv() 대신에

0:43:08.780,0:43:13.280
여기서는 from_data_frame()이 사용 되었고
몇 가지 인자값이 전달 되어야 합니다.

0:43:13.420,0:43:16.380
PATH라는 것은 모델등 관련 파일이

0:43:16.385,0:43:19.425
어디에 저장될 지를 명시 합니다.

0:43:19.425,0:43:21.955
나중에 저장할 때,

0:43:21.960,0:43:25.120
어디에 파일을 넣어둘지를 정하는 것입니다.

0:43:25.380,0:43:28.915
val_idx는 앞서 생성한 
검증 데이터셋으로써 사용하고자 하는

0:43:28.915,0:43:32.225
데이터들의 인덱스값에 대한 리스트 입니다.

0:43:32.540,0:43:38.920
df는 DataFrame 객체이고,

0:43:39.160,0:43:42.960
yl은 proc_df() 수행 후, 떨어져 나온 (Sales)

0:43:42.960,0:43:46.840
의존 컬럼 데이터값에 Log를 취한 값들 입니다.

0:43:46.840,0:43:50.280
그래서, yl 이라고 명명 하였고

0:43:50.520,0:43:54.760
모델 데이터를 만들 때,
의존 데이터가 뭔지도 알려줬습니다

0:43:54.940,0:43:59.000
다시 종합 해 보면
검증 데이터셋에 사용될 데이터 목록,

0:43:59.000,0:44:01.005
비의존 데이터와

0:44:01.005,0:44:03.965
의존 데이터가 뭔지를 명시 해 줬습니다.

0:44:03.965,0:44:08.180
그리고, 어떤 컬럼을 범주형으로
다뤄야 하는지도 알려줘야 합니다.

0:44:08.180,0:44:12.320
최종 변형된 DataFrame에선

0:44:12.360,0:44:15.000
모든 것이 숫자 이기 때문에,

0:44:15.000,0:44:18.760
모든 것을 계속형 데이터처럼 다루게 될 수 있고

0:44:18.760,0:44:21.380
그러면 의미 없는 결과가 나올 수 있습니다.

0:44:21.480,0:44:24.840
그래서, 뭐가 범주형 으로써 다뤄져야 하는지를
알려줄 필요가 있는 것이고,

0:44:25.040,0:44:30.860
여기서는 이미 정의된, 그것들의 목록을 
인자로써 전달 해 줬습니다.

0:44:30.940,0:44:34.440
그러면, 나머지 다른 많은 파라메터들은

0:44:34.440,0:44:36.240
예전과 동일합니다.

0:44:36.240,0:44:39.120
예를 들어서 bs로 
배치크기를 지정 해 줄 수 있을겁니다.

0:44:39.485,0:44:42.105
여기까지 수행하면,

0:44:42.105,0:44:46.560
일종의 표준적인 모델 데이터 객체가
만들어 지고

0:44:46.560,0:44:48.395


0:44:48.395,0:44:51.425


0:44:51.425,0:44:54.895


0:44:54.915,0:44:57.815


0:44:57.815,0:44:58.815


0:44:59.295,0:45:02.205


0:45:02.205,0:45:03.285


0:45:04.205,0:45:07.175


0:45:07.185,0:45:10.195


0:45:10.195,0:45:11.195


0:45:11.335,0:45:14.385


0:45:14.385,0:45:17.085


0:45:17.085,0:45:19.615


0:45:19.615,0:45:21.985


0:45:22.415,0:45:25.055


0:45:25.055,0:45:27.555


0:45:28.135,0:45:31.395


0:45:31.555,0:45:34.885


0:45:35.055,0:45:38.015


0:45:38.015,0:45:39.295


0:45:39.795,0:45:42.845


0:45:42.845,0:45:46.195


0:45:46.715,0:45:50.035


0:45:50.265,0:45:52.905


0:45:54.105,0:45:55.105


0:45:56.165,0:45:59.185


0:45:59.185,0:46:00.185


0:46:01.455,0:46:04.445


0:46:04.445,0:46:06.875


0:46:07.235,0:46:10.275


0:46:11.435,0:46:12.715


0:46:16.905,0:46:20.005


0:46:20.005,0:46:21.355


0:46:22.815,0:46:25.735


0:46:25.735,0:46:28.445


0:46:28.485,0:46:31.735


0:46:31.735,0:46:35.215


0:46:35.415,0:46:37.345


0:46:37.685,0:46:41.095


0:46:41.405,0:46:44.515


0:46:44.515,0:46:47.915


0:46:48.035,0:46:50.935


0:46:50.935,0:46:51.935


0:46:52.075,0:46:55.425


0:46:55.915,0:46:58.435


0:46:58.665,0:47:01.655


0:47:01.655,0:47:04.635


0:47:04.635,0:47:07.905


0:47:07.905,0:47:10.875


0:47:10.875,0:47:13.795


0:47:13.795,0:47:16.475


0:47:17.145,0:47:18.675


0:47:19.095,0:47:20.415


0:47:20.815,0:47:23.785


0:47:23.785,0:47:26.695


0:47:26.695,0:47:29.935


0:47:30.235,0:47:33.535


0:47:33.535,0:47:34.685


0:47:35.395,0:47:37.745


0:47:38.125,0:47:40.975


0:47:40.975,0:47:44.165


0:47:44.165,0:47:45.465


0:47:46.085,0:47:49.085


0:47:49.085,0:47:52.125


0:47:52.125,0:47:55.105


0:47:55.105,0:47:57.565


0:47:57.995,0:48:00.685


0:48:00.795,0:48:03.785


0:48:03.785,0:48:04.785


0:48:04.995,0:48:05.995


0:48:06.445,0:48:08.285


0:48:08.965,0:48:12.045


0:48:12.985,0:48:15.775


0:48:15.775,0:48:17.935


0:48:18.285,0:48:20.865


0:48:21.715,0:48:22.765


0:48:23.515,0:48:25.225


0:48:25.775,0:48:28.205


0:48:28.665,0:48:29.875


0:48:31.145,0:48:32.145


0:48:32.605,0:48:35.395


0:48:36.655,0:48:38.995


0:48:39.375,0:48:42.025


0:48:42.025,0:48:45.485


0:48:45.635,0:48:48.475


0:48:48.475,0:48:51.525


0:48:51.525,0:48:52.525


0:48:53.325,0:48:56.225


0:48:56.225,0:48:57.225


0:48:57.315,0:49:00.305


0:49:00.305,0:49:03.105


0:49:03.845,0:49:06.495


0:49:07.295,0:49:09.185


0:49:13.965,0:49:17.205


0:49:17.205,0:49:20.445


0:49:20.855,0:49:23.755


0:49:23.755,0:49:26.595


0:49:26.845,0:49:29.975


0:49:31.815,0:49:32.815


0:49:33.445,0:49:34.775


0:49:36.305,0:49:37.915


0:49:39.495,0:49:40.785


0:49:41.955,0:49:42.955


0:49:43.195,0:49:46.405


0:49:46.405,0:49:49.425


0:49:49.425,0:49:51.475


0:49:52.425,0:49:55.665


0:49:56.385,0:49:59.475


0:49:59.475,0:50:02.565


0:50:02.565,0:50:05.175


0:50:05.175,0:50:08.265


0:50:08.265,0:50:10.915


0:50:10.915,0:50:13.905


0:50:13.905,0:50:17.235


0:50:17.285,0:50:20.355


0:50:20.355,0:50:23.235


0:50:23.545,0:50:24.745


0:50:25.515,0:50:26.555


0:50:26.965,0:50:29.945


0:50:29.945,0:50:33.015


0:50:33.015,0:50:34.425


0:50:34.975,0:50:36.935


0:50:37.765,0:50:40.895


0:50:40.975,0:50:41.985


0:50:43.275,0:50:46.205


0:50:46.205,0:50:49.085


0:50:50.255,0:50:52.795


0:50:52.795,0:50:55.805


0:50:55.805,0:50:57.645


0:50:58.045,0:50:59.045


0:50:59.325,0:51:00.785


0:51:01.665,0:51:02.665


0:51:04.515,0:51:07.565


0:51:07.565,0:51:10.705


0:51:11.975,0:51:12.975


0:51:13.435,0:51:14.545


0:51:15.375,0:51:16.375


0:51:17.615,0:51:20.775


0:51:20.775,0:51:23.705


0:51:23.705,0:51:25.025


0:51:25.555,0:51:28.745


0:51:28.745,0:51:29.745


0:51:31.105,0:51:33.365


0:51:34.845,0:51:37.625


0:51:37.625,0:51:38.935


0:51:39.375,0:51:41.285


0:51:42.225,0:51:43.825


0:51:44.155,0:51:46.345


0:51:47.285,0:51:50.185


0:51:50.205,0:51:53.255


0:51:53.255,0:51:55.835


0:51:56.795,0:51:59.805


0:51:59.805,0:52:03.245


0:52:03.345,0:52:04.405


0:52:05.275,0:52:08.335


0:52:08.335,0:52:11.695


0:52:12.755,0:52:13.755


0:52:13.925,0:52:17.265


0:52:17.295,0:52:19.545


0:52:19.545,0:52:22.645


0:52:22.645,0:52:23.685


0:52:24.075,0:52:25.075


0:52:25.545,0:52:28.685


0:52:28.685,0:52:31.765


0:52:31.765,0:52:34.805


0:52:34.805,0:52:37.475


0:52:37.895,0:52:40.935


0:52:40.935,0:52:43.925


0:52:43.925,0:52:47.075


0:52:47.075,0:52:49.055


0:52:49.875,0:52:52.915


0:52:52.915,0:52:56.285


0:52:57.045,0:53:00.395


0:53:00.595,0:53:03.585


0:53:03.585,0:53:06.655


0:53:06.655,0:53:09.735


0:53:09.735,0:53:12.805


0:53:12.805,0:53:15.905


0:53:15.905,0:53:18.905


0:53:18.905,0:53:22.055


0:53:22.055,0:53:25.165


0:53:25.165,0:53:26.165


0:53:26.225,0:53:27.415


0:53:27.735,0:53:29.595


0:53:29.985,0:53:33.005


0:53:33.005,0:53:35.875


0:53:35.875,0:53:38.845


0:53:38.845,0:53:41.835


0:53:41.835,0:53:44.695


0:53:44.695,0:53:45.695


0:53:45.795,0:53:48.535


0:53:48.735,0:53:51.815


0:53:52.115,0:53:53.455


0:53:53.905,0:53:57.125


0:53:57.125,0:53:58.125


0:53:58.185,0:54:00.105


0:54:03.915,0:54:07.065


0:54:07.495,0:54:10.875


0:54:11.215,0:54:14.115


0:54:14.115,0:54:15.725


0:54:16.405,0:54:18.515


0:54:18.515,0:54:21.975


0:54:22.045,0:54:24.675


0:54:25.425,0:54:26.825


0:54:27.445,0:54:30.385


0:54:30.385,0:54:31.385


0:54:31.885,0:54:34.075


0:54:34.865,0:54:37.925


0:54:37.925,0:54:39.895


0:54:40.635,0:54:43.565


0:54:43.565,0:54:46.695


0:54:46.695,0:54:49.565


0:54:49.565,0:54:52.385


0:54:52.925,0:54:55.955


0:54:55.955,0:54:58.905


0:54:58.905,0:55:01.995


0:55:01.995,0:55:05.465


0:55:05.775,0:55:07.885


0:55:07.885,0:55:10.765


0:55:13.135,0:55:16.145


0:55:16.145,0:55:19.175


0:55:19.175,0:55:20.995


0:55:21.395,0:55:22.545


0:55:23.245,0:55:26.385


0:55:26.385,0:55:29.375


0:55:29.925,0:55:32.275


0:55:32.275,0:55:35.275


0:55:35.275,0:55:38.295


0:55:38.295,0:55:41.085


0:55:41.085,0:55:44.115


0:55:44.115,0:55:46.075


0:55:46.385,0:55:49.225


0:55:49.225,0:55:52.055


0:55:52.055,0:55:53.405


0:55:54.215,0:55:55.215


0:55:55.925,0:55:56.925


0:55:56.975,0:55:59.895


0:55:59.895,0:56:02.205


0:56:02.515,0:56:03.905


0:56:08.355,0:56:10.665


0:56:11.005,0:56:13.705


0:56:13.705,0:56:16.705


0:56:16.705,0:56:17.835


0:56:18.515,0:56:21.265


0:56:21.265,0:56:24.195


0:56:24.335,0:56:26.465


0:56:26.925,0:56:29.905


0:56:29.905,0:56:33.005


0:56:33.005,0:56:36.085


0:56:36.085,0:56:39.115


0:56:39.115,0:56:42.235


0:56:42.965,0:56:45.755


0:56:45.755,0:56:48.595


0:56:48.595,0:56:51.405


0:56:51.405,0:56:53.355


0:56:54.165,0:56:56.935


0:56:57.645,0:56:58.805


0:56:59.815,0:57:01.625


0:57:02.675,0:57:04.495


0:57:04.955,0:57:07.945


0:57:07.945,0:57:10.935


0:57:10.935,0:57:14.045


0:57:14.045,0:57:17.205


0:57:17.205,0:57:18.475


0:57:19.365,0:57:21.605


0:57:22.005,0:57:24.955


0:57:24.955,0:57:26.945


0:57:27.365,0:57:28.715


0:57:30.705,0:57:33.705


0:57:33.705,0:57:36.115


0:57:36.735,0:57:39.595


0:57:41.095,0:57:42.255


0:57:42.845,0:57:44.755


0:57:46.105,0:57:47.675


0:57:49.135,0:57:52.125


0:57:52.125,0:57:54.895


0:57:55.325,0:57:58.535


0:57:58.535,0:58:01.345


0:58:01.345,0:58:04.195


0:58:04.195,0:58:06.075


0:58:06.755,0:58:09.715


0:58:10.295,0:58:13.415


0:58:13.735,0:58:16.915


0:58:16.925,0:58:18.745


0:58:18.745,0:58:21.295


0:58:22.585,0:58:23.585


0:58:23.975,0:58:27.165


0:58:27.255,0:58:28.575


0:58:28.945,0:58:32.185


0:58:32.185,0:58:35.045


0:58:35.045,0:58:37.975


0:58:37.975,0:58:40.805


0:58:40.805,0:58:43.835


0:58:43.835,0:58:46.815


0:58:46.815,0:58:49.855


0:58:49.855,0:58:52.865


0:58:52.875,0:58:56.065


0:58:56.215,0:58:59.275


0:58:59.275,0:59:02.245


0:59:02.245,0:59:04.795


0:59:04.795,0:59:08.105


0:59:08.105,0:59:11.095


0:59:11.095,0:59:13.875


0:59:14.115,0:59:17.255


0:59:17.255,0:59:20.295


0:59:23.865,0:59:26.545


0:59:27.415,0:59:29.995


0:59:29.995,0:59:32.835


0:59:32.835,0:59:35.025


0:59:35.025,0:59:37.655


0:59:37.655,0:59:40.785


0:59:40.835,0:59:42.145


0:59:42.555,0:59:45.555


0:59:45.555,0:59:48.725


0:59:48.725,0:59:51.795


0:59:51.795,0:59:54.755


0:59:55.005,0:59:56.125


0:59:56.855,0:59:59.825


0:59:59.825,1:00:02.905


1:00:02.905,1:00:05.905


1:00:05.905,1:00:08.715


1:00:08.845,1:00:11.855


1:00:11.855,1:00:12.855


1:00:13.595,1:00:16.515


1:00:16.515,1:00:17.515


1:00:17.745,1:00:20.515


1:00:20.515,1:00:23.215


1:00:23.215,1:00:26.115


1:00:27.205,1:00:30.075


1:00:30.075,1:00:33.165


1:00:33.165,1:00:36.575


1:00:36.615,1:00:39.545


1:00:39.545,1:00:42.465


1:00:42.465,1:00:45.015


1:00:45.015,1:00:48.055


1:00:48.055,1:00:51.065


1:00:51.065,1:00:54.105


1:00:54.105,1:00:57.115


1:00:57.115,1:01:00.065


1:01:00.065,1:01:02.805


1:01:02.805,1:01:05.055


1:01:06.105,1:01:08.895


1:01:08.895,1:01:12.125


1:01:12.125,1:01:14.885


1:01:14.885,1:01:18.055


1:01:18.055,1:01:19.565


1:01:22.115,1:01:25.185


1:01:25.185,1:01:28.245


1:01:28.245,1:01:31.285


1:01:31.705,1:01:34.595


1:01:34.595,1:01:37.635


1:01:37.635,1:01:40.955


1:01:40.955,1:01:43.855


1:01:43.855,1:01:46.915


1:01:46.915,1:01:49.875


1:01:49.875,1:01:52.815


1:01:52.815,1:01:55.465


1:01:55.465,1:01:58.475


1:01:58.475,1:02:01.505


1:02:01.505,1:02:03.545


1:02:04.055,1:02:07.495


1:02:07.545,1:02:10.935


1:02:11.325,1:02:14.525


1:02:14.525,1:02:17.305


1:02:17.305,1:02:20.335


1:02:20.335,1:02:23.285


1:02:23.285,1:02:26.005


1:02:26.005,1:02:29.065


1:02:29.065,1:02:32.255


1:02:32.255,1:02:35.275


1:02:35.275,1:02:38.315


1:02:38.315,1:02:39.315


1:02:41.745,1:02:44.975


1:02:45.005,1:02:46.005


1:02:46.515,1:02:47.515


1:02:48.375,1:02:51.315


1:02:51.315,1:02:54.325


1:02:54.325,1:02:57.125


1:02:57.195,1:03:00.245


1:03:00.245,1:03:03.175


1:03:03.175,1:03:06.105


1:03:06.255,1:03:09.015


1:03:09.015,1:03:11.325


1:03:11.925,1:03:14.725


1:03:14.725,1:03:17.775


1:03:17.775,1:03:20.835


1:03:20.835,1:03:24.025


1:03:24.025,1:03:27.235


1:03:27.495,1:03:30.285


1:03:30.285,1:03:33.305


1:03:33.305,1:03:35.845


1:03:35.845,1:03:37.795


1:03:38.185,1:03:41.165


1:03:41.165,1:03:44.235


1:03:44.235,1:03:47.025


1:03:47.025,1:03:49.935


1:03:49.935,1:03:53.025


1:03:53.025,1:03:55.105


1:03:55.105,1:03:58.055


1:03:58.055,1:04:01.205


1:04:01.205,1:04:02.805


1:04:03.405,1:04:05.955


1:04:07.045,1:04:10.085


1:04:10.085,1:04:13.105


1:04:13.105,1:04:16.305


1:04:16.305,1:04:19.165


1:04:19.165,1:04:22.065


1:04:22.065,1:04:25.155


1:04:25.155,1:04:28.345


1:04:29.145,1:04:31.885


1:04:31.885,1:04:35.165


1:04:35.275,1:04:38.135


1:04:38.135,1:04:41.215


1:04:41.215,1:04:43.895


1:04:43.995,1:04:47.135


1:04:47.135,1:04:50.135


1:04:50.135,1:04:53.075


1:04:53.075,1:04:56.065


1:04:56.065,1:04:59.135


1:04:59.135,1:05:02.185


1:05:02.585,1:05:05.625


1:05:05.625,1:05:06.625


1:05:06.775,1:05:09.865


1:05:11.315,1:05:14.405


1:05:14.405,1:05:17.075


1:05:18.535,1:05:19.485


1:05:19.485,1:05:20.485


1:05:22.545,1:05:23.915


1:05:24.805,1:05:27.835


1:05:27.835,1:05:29.065


1:05:31.295,1:05:32.295


1:05:32.965,1:05:36.105


1:05:36.305,1:05:38.715


1:05:38.715,1:05:40.015


1:05:40.655,1:05:43.575


1:05:43.575,1:05:44.975


1:05:45.765,1:05:48.795


1:05:48.795,1:05:51.685


1:05:51.685,1:05:53.145


1:05:53.565,1:05:56.795


1:05:56.795,1:05:59.815


1:05:59.815,1:06:00.815


1:06:01.095,1:06:04.045


1:06:04.045,1:06:07.115


1:06:07.415,1:06:09.955


1:06:10.445,1:06:13.425


1:06:13.425,1:06:16.505


1:06:16.505,1:06:19.915


1:06:20.015,1:06:22.825


1:06:22.825,1:06:24.465


1:06:24.865,1:06:26.855


1:06:26.855,1:06:29.565


1:06:29.565,1:06:32.465


1:06:32.465,1:06:35.465


1:06:35.465,1:06:38.475


1:06:38.475,1:06:41.505


1:06:41.505,1:06:44.565


1:06:44.565,1:06:47.875


1:06:47.925,1:06:50.625


1:06:50.715,1:06:51.875


1:06:52.955,1:06:55.875


1:06:55.875,1:06:57.035


1:06:59.155,1:07:02.165


1:07:02.165,1:07:05.175


1:07:05.175,1:07:07.005


1:07:07.925,1:07:10.485


1:07:11.155,1:07:13.375


1:07:13.795,1:07:16.635


1:07:16.635,1:07:19.635


1:07:19.635,1:07:20.635


1:07:23.495,1:07:24.495


1:07:24.635,1:07:27.315


1:07:28.495,1:07:30.945


1:07:32.235,1:07:33.965


1:07:34.955,1:07:37.755


1:07:37.755,1:07:40.885


1:07:40.885,1:07:44.035


1:07:44.035,1:07:46.875


1:07:46.875,1:07:48.145


1:07:48.465,1:07:51.685


1:07:51.685,1:07:54.735


1:07:54.735,1:07:57.085


1:07:57.435,1:08:00.415


1:08:00.585,1:08:03.455


1:08:03.455,1:08:05.405


1:08:05.865,1:08:08.445


1:08:08.445,1:08:11.555


1:08:12.305,1:08:15.415


1:08:15.415,1:08:16.415


1:08:17.195,1:08:20.225


1:08:20.225,1:08:23.155


1:08:23.155,1:08:26.235


1:08:26.235,1:08:27.235


1:08:27.315,1:08:30.175


1:08:30.175,1:08:33.075


1:08:33.075,1:08:35.985


1:08:35.985,1:08:38.845


1:08:38.845,1:08:41.665


1:08:41.665,1:08:44.875


1:08:44.875,1:08:47.025


1:08:47.925,1:08:49.265


1:08:49.585,1:08:52.895


1:08:52.895,1:08:55.745


1:08:55.745,1:08:58.825


1:08:58.825,1:09:01.825


1:09:01.825,1:09:04.845


1:09:04.845,1:09:07.875


1:09:07.875,1:09:10.985


1:09:10.985,1:09:14.155


1:09:14.155,1:09:17.455


1:09:17.455,1:09:20.195


1:09:20.195,1:09:23.125


1:09:23.125,1:09:25.205


1:09:25.205,1:09:27.105


1:09:27.705,1:09:30.695


1:09:30.695,1:09:32.365


1:09:32.695,1:09:33.695


1:09:33.865,1:09:36.715


1:09:36.715,1:09:39.725


1:09:39.725,1:09:42.535


1:09:42.535,1:09:45.565


1:09:45.565,1:09:48.435


1:09:48.435,1:09:51.465


1:09:51.465,1:09:54.515


1:09:54.515,1:09:57.305


1:09:57.305,1:10:00.425


1:10:00.425,1:10:03.435


1:10:04.385,1:10:07.075


1:10:07.075,1:10:09.755


1:10:12.355,1:10:15.425


1:10:15.425,1:10:18.385


1:10:18.385,1:10:21.885


1:10:22.005,1:10:25.095


1:10:25.095,1:10:28.265


1:10:28.305,1:10:29.305


1:10:29.625,1:10:32.535


1:10:32.535,1:10:35.365


1:10:35.365,1:10:37.665


1:10:37.665,1:10:40.825


1:10:40.825,1:10:43.765


1:10:43.765,1:10:44.765


1:10:45.275,1:10:48.275


1:10:48.275,1:10:51.155


1:10:51.155,1:10:52.715


1:10:53.585,1:10:54.585


1:10:55.025,1:10:58.335


1:10:58.745,1:11:01.845


1:11:02.015,1:11:04.805


1:11:05.135,1:11:07.695


1:11:08.225,1:11:10.885


1:11:10.885,1:11:14.095


1:11:14.095,1:11:17.365


1:11:17.715,1:11:20.785


1:11:20.785,1:11:23.475


1:11:23.685,1:11:25.125


1:11:25.445,1:11:26.445


1:11:26.605,1:11:27.605


1:11:28.165,1:11:30.895


1:11:30.895,1:11:34.285


1:11:34.565,1:11:36.115


1:11:36.445,1:11:39.555


1:11:39.555,1:11:42.355


1:11:42.355,1:11:45.365


1:11:45.365,1:11:46.365


1:11:46.835,1:11:49.765


1:11:49.765,1:11:52.765


1:11:52.765,1:11:55.785


1:11:55.785,1:11:59.255


1:11:59.315,1:12:02.615


1:12:02.615,1:12:05.425


1:12:05.425,1:12:08.445


1:12:08.445,1:12:11.805


1:12:11.945,1:12:14.985


1:12:14.985,1:12:17.115


1:12:17.745,1:12:19.565


1:12:19.995,1:12:20.995


1:12:21.315,1:12:24.435


1:12:24.435,1:12:27.505


1:12:27.505,1:12:30.535


1:12:30.535,1:12:33.445


1:12:33.445,1:12:36.495


1:12:36.495,1:12:37.515


1:12:37.875,1:12:38.875


1:12:39.005,1:12:40.005


1:12:40.655,1:12:43.445


1:12:43.445,1:12:46.595


1:12:46.595,1:12:49.795


1:12:49.905,1:12:52.915


1:12:52.915,1:12:55.635


1:12:55.855,1:12:57.515


1:12:58.875,1:13:01.375


1:13:03.925,1:13:04.925


1:13:05.595,1:13:07.275


1:13:09.255,1:13:10.355


1:13:11.375,1:13:14.185


1:13:16.805,1:13:19.685


1:13:19.685,1:13:22.405


1:13:23.695,1:13:26.555


1:13:26.555,1:13:28.655


1:13:29.005,1:13:31.755


1:13:31.755,1:13:33.515


1:13:34.125,1:13:36.195


1:13:36.665,1:13:37.965


1:13:38.325,1:13:41.365


1:13:41.365,1:13:44.555


1:13:44.555,1:13:47.395


1:13:48.175,1:13:49.175


1:13:49.285,1:13:52.285


1:13:52.285,1:13:55.155


1:13:55.155,1:13:58.145


1:13:58.145,1:14:01.005


1:14:01.005,1:14:03.445


1:14:03.445,1:14:06.395


1:14:07.245,1:14:10.265


1:14:10.265,1:14:12.055


1:14:12.645,1:14:15.445


1:14:15.515,1:14:18.945


1:14:18.945,1:14:21.395


1:14:21.395,1:14:24.395


1:14:24.395,1:14:27.575


1:14:27.645,1:14:30.545


1:14:30.545,1:14:33.685


1:14:33.685,1:14:36.705


1:14:36.705,1:14:39.415


1:14:39.815,1:14:42.935


1:14:42.935,1:14:45.155


1:14:45.155,1:14:48.215


1:14:48.435,1:14:51.385


1:14:51.385,1:14:54.245


1:14:54.245,1:14:56.515


1:14:58.065,1:14:59.115


1:15:02.305,1:15:04.205


1:15:04.205,1:15:07.585


1:15:08.125,1:15:10.635


1:15:10.635,1:15:13.445


1:15:13.445,1:15:16.285


1:15:16.285,1:15:17.285


1:15:17.325,1:15:20.205


1:15:20.205,1:15:23.255


1:15:23.255,1:15:26.255


1:15:26.255,1:15:29.195


1:15:29.195,1:15:31.255


1:15:31.965,1:15:35.075


1:15:35.075,1:15:36.365


1:15:36.945,1:15:39.555


1:15:40.415,1:15:43.495


1:15:43.495,1:15:46.475


1:15:46.475,1:15:49.325


1:15:49.325,1:15:52.425


1:15:52.425,1:15:55.575


1:15:55.575,1:15:59.045


1:15:59.285,1:16:02.645


1:16:02.645,1:16:05.985


1:16:05.985,1:16:08.945


1:16:08.945,1:16:11.045


1:16:11.585,1:16:14.445


1:16:14.445,1:16:16.885


1:16:16.885,1:16:19.905


1:16:19.905,1:16:23.155


1:16:23.735,1:16:26.345


1:16:26.345,1:16:29.335


1:16:29.335,1:16:30.435


1:16:31.285,1:16:33.695


1:16:34.275,1:16:37.265


1:16:37.265,1:16:40.085


1:16:40.775,1:16:43.555


1:16:43.555,1:16:46.545


1:16:46.545,1:16:49.555


1:16:49.555,1:16:52.595


1:16:52.595,1:16:55.605


1:16:55.605,1:16:58.615


1:16:58.615,1:17:01.465


1:17:01.465,1:17:02.915


1:17:04.785,1:17:07.525


1:17:07.525,1:17:10.445


1:17:10.555,1:17:13.635


1:17:13.635,1:17:16.645


1:17:16.645,1:17:19.445


1:17:19.445,1:17:22.075


1:17:22.075,1:17:24.705


1:17:24.705,1:17:28.105


1:17:28.205,1:17:31.625


1:17:32.905,1:17:36.015


1:17:36.015,1:17:38.935


1:17:38.935,1:17:42.125


1:17:42.125,1:17:45.125


1:17:45.125,1:17:47.605


1:17:47.605,1:17:50.595


1:17:50.595,1:17:53.995


1:17:53.995,1:17:56.965


1:17:56.965,1:18:00.035


1:18:00.035,1:18:02.965


1:18:02.965,1:18:06.205


1:18:06.205,1:18:09.295


1:18:10.105,1:18:13.095


1:18:13.095,1:18:14.095


1:18:14.705,1:18:17.265


1:18:17.265,1:18:19.085


1:18:19.915,1:18:20.995


1:18:22.705,1:18:24.655


1:18:25.245,1:18:28.715


1:18:29.765,1:18:32.505


1:18:32.505,1:18:35.235


1:18:36.715,1:18:39.805


1:18:40.375,1:18:43.565


1:18:43.565,1:18:46.535


1:18:46.535,1:18:49.795


1:18:50.795,1:18:53.765


1:18:53.765,1:18:54.855


1:18:59.605,1:19:02.815


1:19:02.815,1:19:03.815


1:19:04.045,1:19:07.045


1:19:07.045,1:19:10.315


1:19:10.315,1:19:11.315


1:19:11.485,1:19:14.565


1:19:14.565,1:19:17.415


1:19:17.415,1:19:19.675


1:19:21.275,1:19:24.525


1:19:24.525,1:19:27.125


1:19:27.125,1:19:28.545


1:19:29.025,1:19:30.025


1:19:30.305,1:19:32.015


1:19:33.235,1:19:35.205


1:19:35.585,1:19:38.695


1:19:38.845,1:19:41.815


1:19:43.205,1:19:44.205


1:19:45.475,1:19:48.475


1:19:48.475,1:19:50.935


1:19:51.275,1:19:54.325


1:19:54.325,1:19:56.925


1:19:57.295,1:20:00.195


1:20:00.195,1:20:03.325


1:20:03.325,1:20:04.325


1:20:05.415,1:20:08.495


1:20:11.925,1:20:14.955


1:20:14.955,1:20:17.355


1:20:20.445,1:20:21.565


1:20:23.345,1:20:25.365


1:20:26.105,1:20:27.105


1:20:28.205,1:20:29.735


1:20:31.055,1:20:33.155


1:20:33.885,1:20:36.925


1:20:36.925,1:20:40.035


1:20:40.035,1:20:41.035


1:20:41.315,1:20:43.645


1:20:44.405,1:20:47.485


1:20:47.905,1:20:48.905


1:20:48.925,1:20:49.975


1:20:50.575,1:20:53.795


1:20:53.795,1:20:56.685


1:20:56.685,1:20:59.755


1:20:59.755,1:21:01.555


1:21:02.265,1:21:03.265


1:21:03.545,1:21:06.375


1:21:06.375,1:21:09.255


1:21:09.255,1:21:12.585


1:21:12.585,1:21:15.505


1:21:15.505,1:21:18.945


1:21:19.045,1:21:22.055


1:21:22.055,1:21:25.095


1:21:25.095,1:21:28.145


1:21:28.145,1:21:31.295


1:21:31.295,1:21:32.295


1:21:33.675,1:21:36.215


1:21:36.215,1:21:39.255


1:21:39.255,1:21:40.725


1:21:41.735,1:21:44.755


1:21:44.755,1:21:47.725


1:21:47.725,1:21:50.645


1:21:50.675,1:21:53.575


1:21:53.575,1:21:56.425


1:21:56.735,1:21:59.315


1:21:59.315,1:22:00.795


1:22:01.235,1:22:04.065


1:22:04.835,1:22:07.875


1:22:07.875,1:22:10.725


1:22:10.725,1:22:13.885


1:22:13.885,1:22:16.535


1:22:16.535,1:22:19.985


1:22:21.455,1:22:24.395


1:22:24.395,1:22:26.795


1:22:27.275,1:22:30.145


1:22:30.145,1:22:33.125


1:22:33.125,1:22:36.125


1:22:36.125,1:22:39.165


1:22:39.165,1:22:40.165


1:22:40.265,1:22:43.305


1:22:43.305,1:22:45.115


1:22:45.575,1:22:48.635


1:22:48.635,1:22:51.665


1:22:51.665,1:22:53.735


1:22:53.735,1:22:56.995


1:22:56.995,1:23:00.365


1:23:00.985,1:23:03.845


1:23:04.185,1:23:07.305


1:23:07.305,1:23:10.325


1:23:10.325,1:23:13.565


1:23:13.565,1:23:15.505


1:23:15.505,1:23:19.005


1:23:19.065,1:23:22.045


1:23:22.045,1:23:24.445


1:23:24.445,1:23:27.695


1:23:27.695,1:23:31.005


1:23:31.215,1:23:34.325


1:23:35.445,1:23:36.805


1:23:37.475,1:23:40.685


1:23:40.685,1:23:43.585


1:23:43.905,1:23:46.665


1:23:46.665,1:23:49.645


1:23:49.645,1:23:50.645


1:23:51.065,1:23:54.385


1:23:54.385,1:23:57.385


1:23:57.385,1:23:58.385


1:23:58.715,1:24:01.805


1:24:01.805,1:24:04.835


1:24:04.835,1:24:07.065


1:24:07.065,1:24:09.925


1:24:09.925,1:24:12.845


1:24:12.845,1:24:15.925


1:24:15.925,1:24:18.985


1:24:18.985,1:24:21.935


1:24:22.145,1:24:23.605


1:24:24.305,1:24:27.405


1:24:27.405,1:24:30.645


1:24:30.645,1:24:33.435


1:24:33.445,1:24:36.255


1:24:36.255,1:24:38.655


1:24:38.655,1:24:41.545


1:24:41.545,1:24:44.725


1:24:44.815,1:24:47.945


1:24:47.945,1:24:50.925


1:24:50.925,1:24:53.855


1:24:53.855,1:24:56.285


1:24:57.115,1:25:00.005


1:25:00.375,1:25:03.205


1:25:03.205,1:25:06.165


1:25:06.165,1:25:09.375


1:25:09.375,1:25:12.095


1:25:12.325,1:25:15.295


1:25:15.295,1:25:18.345


1:25:18.625,1:25:21.635


1:25:21.635,1:25:23.475


1:25:24.295,1:25:27.255


1:25:27.255,1:25:30.155


1:25:30.155,1:25:33.265


1:25:33.265,1:25:35.585


1:25:35.585,1:25:38.265


1:25:38.265,1:25:41.265


1:25:41.265,1:25:44.265


1:25:44.265,1:25:47.415


1:25:47.855,1:25:50.565


1:25:50.565,1:25:51.765


1:25:52.155,1:25:55.105


1:25:55.195,1:25:56.695


1:25:57.035,1:26:00.105


1:26:00.575,1:26:03.485


1:26:03.485,1:26:06.425


1:26:06.425,1:26:08.735


1:26:09.205,1:26:12.085


1:26:12.515,1:26:15.525


1:26:15.525,1:26:18.575


1:26:18.575,1:26:21.105


1:26:21.225,1:26:22.225


1:26:22.625,1:26:25.635


1:26:25.635,1:26:28.045


1:26:28.045,1:26:31.255


1:26:32.025,1:26:35.105


1:26:35.105,1:26:38.495


1:26:39.665,1:26:42.645


1:26:42.645,1:26:43.795


1:26:44.105,1:26:47.165


1:26:47.165,1:26:48.175


1:26:48.565,1:26:51.265


1:26:51.265,1:26:53.645


1:26:53.645,1:26:56.565


1:26:56.565,1:26:59.585


1:26:59.585,1:27:01.805


1:27:02.205,1:27:05.635


1:27:05.815,1:27:08.835


1:27:08.835,1:27:09.835


1:27:10.305,1:27:13.335


1:27:13.335,1:27:16.325


1:27:16.325,1:27:19.235


1:27:19.555,1:27:22.595


1:27:22.595,1:27:25.575


1:27:25.575,1:27:28.655


1:27:28.655,1:27:31.665


1:27:31.665,1:27:34.345


1:27:34.345,1:27:37.485


1:27:37.485,1:27:40.495


1:27:40.495,1:27:42.515


1:27:42.965,1:27:45.445


1:27:45.445,1:27:48.475


1:27:48.475,1:27:49.875


1:27:50.185,1:27:53.195


1:27:53.195,1:27:56.275


1:27:56.275,1:27:57.305


1:27:57.655,1:28:00.445


1:28:00.445,1:28:03.835


1:28:03.835,1:28:06.925


1:28:06.925,1:28:10.385


1:28:10.505,1:28:13.525


1:28:13.525,1:28:16.345


1:28:16.345,1:28:19.215


1:28:19.215,1:28:21.535


1:28:21.875,1:28:24.985


1:28:24.985,1:28:28.025


1:28:28.025,1:28:30.975


1:28:30.975,1:28:34.225


1:28:34.915,1:28:37.705


1:28:38.145,1:28:40.905


1:28:41.425,1:28:44.645


1:28:44.645,1:28:47.685


1:28:47.685,1:28:50.565


1:28:50.565,1:28:53.375


1:28:53.375,1:28:56.335


1:28:56.335,1:28:59.375


1:28:59.685,1:29:02.135


1:29:02.525,1:29:05.575


1:29:05.575,1:29:08.485


1:29:08.485,1:29:11.505


1:29:11.505,1:29:14.465


1:29:14.465,1:29:17.895


1:29:17.895,1:29:20.855


1:29:20.855,1:29:23.405


1:29:23.405,1:29:26.405


1:29:26.405,1:29:29.345


1:29:29.345,1:29:32.465


1:29:32.465,1:29:34.415


1:29:36.015,1:29:38.805


1:29:38.805,1:29:41.805


1:29:41.805,1:29:44.815


1:29:44.815,1:29:46.105


1:29:46.435,1:29:49.455


1:29:49.455,1:29:52.305


1:29:52.305,1:29:53.765


1:29:54.305,1:29:57.425


1:29:57.425,1:29:59.955


1:30:00.265,1:30:03.205


1:30:03.205,1:30:06.305


1:30:06.555,1:30:09.235


1:30:09.235,1:30:12.285


1:30:12.285,1:30:15.415


1:30:15.415,1:30:16.415


1:30:17.635,1:30:20.405


1:30:20.405,1:30:23.535


1:30:23.725,1:30:25.755


1:30:26.695,1:30:29.485


1:30:29.485,1:30:32.165


1:30:32.165,1:30:35.215


1:30:35.325,1:30:38.665


1:30:39.315,1:30:42.285


1:30:42.285,1:30:45.375


1:30:45.375,1:30:48.435


1:30:48.435,1:30:51.405


1:30:51.405,1:30:53.035


1:30:53.765,1:30:56.845


1:30:56.845,1:30:58.235


1:30:58.545,1:31:01.195


1:31:01.195,1:31:04.405


1:31:04.445,1:31:07.475


1:31:07.475,1:31:10.715


1:31:10.715,1:31:11.645


1:31:11.645,1:31:14.675


1:31:14.675,1:31:17.785


1:31:17.785,1:31:20.595


1:31:20.595,1:31:23.825


1:31:23.905,1:31:26.965


1:31:26.965,1:31:30.275


1:31:30.275,1:31:33.105


1:31:33.105,1:31:34.175


1:31:35.305,1:31:38.175


1:31:38.175,1:31:41.385


1:31:41.555,1:31:43.195


1:31:43.585,1:31:44.585


1:31:44.925,1:31:47.995


1:31:47.995,1:31:50.845


1:31:50.845,1:31:53.525


1:31:53.735,1:31:56.385


1:31:56.805,1:32:00.085


1:32:00.095,1:32:03.115


1:32:03.115,1:32:05.745


1:32:05.745,1:32:07.855


1:32:08.565,1:32:10.625


1:32:10.945,1:32:14.255


1:32:15.255,1:32:18.305


1:32:18.305,1:32:21.385


1:32:21.385,1:32:24.335


1:32:24.335,1:32:27.335


1:32:27.335,1:32:28.745


1:32:29.225,1:32:30.865


1:32:31.775,1:32:34.845


1:32:34.845,1:32:37.645


1:32:38.185,1:32:41.075


1:32:41.075,1:32:43.915


1:32:43.915,1:32:46.825


1:32:47.025,1:32:50.055


1:32:50.055,1:32:52.945


1:32:52.945,1:32:55.725


1:32:55.725,1:32:58.635


1:32:59.055,1:33:02.045


1:33:02.045,1:33:04.405


1:33:04.405,1:33:07.235


1:33:07.235,1:33:08.235


1:33:11.025,1:33:14.255


1:33:14.255,1:33:16.205


1:33:17.315,1:33:20.745


1:33:20.775,1:33:23.215


1:33:24.215,1:33:27.565


1:33:27.565,1:33:30.535


1:33:30.535,1:33:33.795


1:33:34.155,1:33:36.905


1:33:36.905,1:33:39.765


1:33:39.985,1:33:43.445


1:33:44.015,1:33:47.185


1:33:47.185,1:33:48.185


1:33:49.455,1:33:51.945


1:33:51.945,1:33:55.075


1:33:55.075,1:33:57.655


1:33:57.655,1:34:00.465


1:34:00.645,1:34:03.635


1:34:03.635,1:34:06.675


1:34:06.675,1:34:09.945


1:34:09.945,1:34:11.285


1:34:11.965,1:34:15.075


1:34:15.075,1:34:18.265


1:34:18.265,1:34:21.195


1:34:21.195,1:34:24.425


1:34:24.425,1:34:27.705


1:34:28.305,1:34:30.895


1:34:31.285,1:34:34.395


1:34:34.395,1:34:37.125


1:34:37.125,1:34:40.165


1:34:40.165,1:34:43.005


1:34:43.005,1:34:45.795


1:34:46.295,1:34:49.235


1:34:49.235,1:34:52.155


1:34:52.155,1:34:54.775


1:34:55.245,1:34:58.185


1:34:58.185,1:35:01.395


1:35:01.495,1:35:04.205


1:35:04.205,1:35:07.275


1:35:07.275,1:35:10.365


1:35:10.365,1:35:13.365


1:35:13.365,1:35:15.185


1:35:15.535,1:35:18.185


1:35:18.185,1:35:19.185


1:35:19.465,1:35:22.015


1:35:22.495,1:35:25.485


1:35:25.725,1:35:28.315


1:35:28.315,1:35:31.475


1:35:31.475,1:35:34.235


1:35:34.235,1:35:36.295


1:35:36.975,1:35:39.135


1:35:39.135,1:35:41.145


1:35:42.035,1:35:45.405


1:35:45.405,1:35:48.145


1:35:48.145,1:35:51.255


1:35:51.255,1:35:54.345


1:35:54.345,1:35:56.745


1:35:56.745,1:35:58.225


1:35:58.605,1:36:01.475


1:36:01.475,1:36:04.385


1:36:04.385,1:36:07.295


1:36:07.295,1:36:10.215


1:36:10.215,1:36:12.695


1:36:13.325,1:36:16.655


1:36:16.655,1:36:19.645


1:36:19.645,1:36:22.675


1:36:22.675,1:36:25.785


1:36:25.785,1:36:26.985


1:36:27.935,1:36:30.725


1:36:30.725,1:36:33.715


1:36:34.205,1:36:36.115


1:36:36.505,1:36:37.935


1:36:38.395,1:36:41.385


1:36:41.385,1:36:42.385


1:36:42.655,1:36:46.105


1:36:46.575,1:36:49.385


1:36:49.385,1:36:52.095


1:36:52.205,1:36:55.265


1:36:55.265,1:36:58.075


1:36:58.075,1:36:59.855


1:37:00.515,1:37:03.305


1:37:03.305,1:37:04.445


1:37:05.735,1:37:08.745


1:37:08.745,1:37:11.145


1:37:11.445,1:37:14.645


1:37:15.375,1:37:17.995


1:37:18.935,1:37:21.945


1:37:21.945,1:37:22.835


1:37:22.835,1:37:25.865


1:37:25.865,1:37:28.765


1:37:28.765,1:37:29.815


1:37:30.135,1:37:32.845


1:37:32.845,1:37:35.855


1:37:35.855,1:37:38.895


1:37:38.895,1:37:41.955


1:37:42.375,1:37:45.305


1:37:45.305,1:37:48.305


1:37:48.305,1:37:51.265


1:37:51.265,1:37:53.285


1:37:54.865,1:37:57.685


1:37:57.685,1:38:00.695


1:38:00.695,1:38:03.725


1:38:04.075,1:38:06.895


1:38:06.895,1:38:09.865


1:38:09.865,1:38:12.875


1:38:12.875,1:38:15.985


1:38:16.075,1:38:19.275


1:38:19.275,1:38:22.595


1:38:22.595,1:38:25.515


1:38:25.515,1:38:27.405


1:38:28.435,1:38:31.025


1:38:31.025,1:38:34.175


1:38:34.175,1:38:37.195


1:38:37.195,1:38:40.155


1:38:40.315,1:38:43.425


1:38:43.555,1:38:45.135


1:38:45.485,1:38:46.485


1:38:46.485,1:38:49.895


1:38:49.925,1:38:52.885


1:38:52.885,1:38:55.895


1:38:55.895,1:38:58.875


1:38:59.085,1:39:00.625


1:39:01.435,1:39:02.435


1:39:02.945,1:39:05.965


1:39:05.965,1:39:09.305


1:39:10.445,1:39:13.455


1:39:13.455,1:39:16.925


1:39:17.135,1:39:20.635


1:39:22.395,1:39:25.445


1:39:25.445,1:39:28.475


1:39:28.475,1:39:31.675


1:39:32.515,1:39:35.175


1:39:36.535,1:39:39.465


1:39:39.465,1:39:42.445


1:39:42.445,1:39:45.675


1:39:45.675,1:39:48.215


1:39:48.215,1:39:50.865


1:39:50.865,1:39:53.835


1:39:53.835,1:39:56.445


1:39:56.775,1:39:59.785


1:39:59.785,1:40:02.955


1:40:03.045,1:40:06.045


1:40:06.045,1:40:09.335


1:40:09.335,1:40:12.155


1:40:12.155,1:40:15.325


1:40:15.325,1:40:16.775


1:40:17.565,1:40:20.875


1:40:22.055,1:40:25.185


1:40:25.185,1:40:28.335


1:40:28.335,1:40:30.615


1:40:30.965,1:40:33.905


1:40:33.905,1:40:37.105


1:40:37.105,1:40:39.965


1:40:39.965,1:40:41.775


1:40:42.175,1:40:43.175


1:40:43.355,1:40:46.315


1:40:46.315,1:40:49.335


1:40:49.415,1:40:52.525


1:40:52.525,1:40:55.555


1:40:55.555,1:40:58.655


1:40:58.655,1:41:01.625


1:41:02.005,1:41:05.095


1:41:05.095,1:41:08.245


1:41:08.385,1:41:11.405


1:41:11.405,1:41:14.295


1:41:14.295,1:41:17.515


1:41:17.515,1:41:20.855


1:41:20.855,1:41:24.205


1:41:24.365,1:41:27.365


1:41:27.365,1:41:30.675


1:41:31.185,1:41:34.295


1:41:34.295,1:41:35.295


1:41:35.445,1:41:38.645


1:41:38.645,1:41:41.465


1:41:41.465,1:41:44.035


1:41:44.035,1:41:47.065


1:41:47.065,1:41:50.045


1:41:50.045,1:41:52.905


1:41:52.905,1:41:54.445


1:41:55.655,1:41:58.575


1:41:58.575,1:42:01.505


1:42:01.505,1:42:03.725


1:42:04.055,1:42:06.405


1:42:06.405,1:42:09.105


1:42:09.105,1:42:12.515


1:42:12.515,1:42:15.625


1:42:15.625,1:42:18.555


1:42:18.555,1:42:21.195


1:42:21.195,1:42:24.265


1:42:24.665,1:42:26.935


1:42:27.555,1:42:29.955


1:42:30.315,1:42:33.295


1:42:33.295,1:42:36.435


1:42:36.435,1:42:38.035


1:42:38.355,1:42:41.345


1:42:41.345,1:42:44.335


1:42:44.335,1:42:47.305


1:42:47.305,1:42:49.585


1:42:49.585,1:42:52.235


1:42:52.625,1:42:54.165


1:42:54.625,1:42:57.675


1:42:57.675,1:42:59.215


1:43:00.175,1:43:03.325


1:43:03.325,1:43:06.375


1:43:06.375,1:43:08.665


1:43:09.125,1:43:12.185


1:43:12.185,1:43:15.245


1:43:15.245,1:43:16.245


1:43:16.365,1:43:19.205


1:43:19.205,1:43:20.965


1:43:21.435,1:43:24.255


1:43:24.255,1:43:27.015


1:43:27.785,1:43:30.765


1:43:30.765,1:43:32.965


1:43:32.965,1:43:36.255


1:43:36.255,1:43:39.255


1:43:39.745,1:43:43.155


1:43:43.385,1:43:45.125


1:43:45.935,1:43:48.795


1:43:48.795,1:43:51.805


1:43:51.805,1:43:55.265


1:43:55.385,1:43:58.285


1:43:58.315,1:44:01.515


1:44:01.515,1:44:04.525


1:44:04.525,1:44:07.715


1:44:07.765,1:44:10.715


1:44:10.715,1:44:13.715


1:44:13.715,1:44:16.755


1:44:16.755,1:44:17.755


1:44:18.065,1:44:21.065


1:44:21.375,1:44:22.375


1:44:23.025,1:44:24.265


1:44:24.835,1:44:27.765


1:44:27.765,1:44:30.665


1:44:30.665,1:44:33.585


1:44:33.585,1:44:36.655


1:44:36.875,1:44:39.545


1:44:39.945,1:44:43.255


1:44:44.925,1:44:46.295


1:44:46.855,1:44:48.545


1:44:49.385,1:44:52.415


1:44:52.415,1:44:55.375


1:44:55.375,1:44:56.375


1:44:56.565,1:44:59.345


1:44:59.345,1:45:00.345


1:45:00.605,1:45:04.005


1:45:05.165,1:45:08.535


1:45:08.965,1:45:09.965


1:45:10.205,1:45:13.145


1:45:13.485,1:45:16.975


1:45:17.105,1:45:18.625


1:45:19.465,1:45:22.485


1:45:22.845,1:45:25.745


1:45:25.745,1:45:29.235


1:45:29.545,1:45:32.685


1:45:32.685,1:45:36.115


1:45:36.435,1:45:39.295


1:45:39.405,1:45:42.295


1:45:42.295,1:45:44.765


1:45:44.765,1:45:47.055


1:45:48.765,1:45:51.995


1:45:51.995,1:45:55.015


1:45:55.015,1:45:58.205


1:45:59.665,1:46:02.835


1:46:02.835,1:46:05.645


1:46:05.645,1:46:08.725


1:46:08.725,1:46:11.625


1:46:11.625,1:46:14.055


1:46:14.375,1:46:17.215


1:46:17.215,1:46:19.935


1:46:19.935,1:46:21.035


1:46:21.495,1:46:22.805


1:46:24.455,1:46:27.235


1:46:27.235,1:46:29.815


1:46:29.815,1:46:32.875


1:46:32.875,1:46:35.895


1:46:35.895,1:46:37.545


1:46:38.825,1:46:40.995


1:46:40.995,1:46:44.345


1:46:44.345,1:46:47.215


1:46:47.215,1:46:50.105


1:46:50.105,1:46:53.095


1:46:53.095,1:46:56.175


1:46:56.175,1:46:59.245


1:46:59.245,1:47:02.105


1:47:02.105,1:47:04.815


1:47:04.815,1:47:06.875


1:47:07.785,1:47:10.795


1:47:10.795,1:47:13.445


1:47:13.445,1:47:16.325


1:47:16.325,1:47:19.365


1:47:19.365,1:47:22.275


1:47:22.355,1:47:24.785


1:47:24.785,1:47:27.915


1:47:27.915,1:47:31.355


1:47:31.845,1:47:32.845


1:47:33.865,1:47:36.265


1:47:36.265,1:47:37.795


1:47:38.835,1:47:41.545


1:47:41.545,1:47:44.675


1:47:44.675,1:47:48.025


1:47:49.115,1:47:50.115


1:47:53.215,1:47:56.155


1:47:56.155,1:47:57.305


1:47:59.035,1:48:02.065


1:48:02.065,1:48:03.165


1:48:05.575,1:48:08.615


1:48:08.615,1:48:11.635


1:48:11.635,1:48:14.665


1:48:14.665,1:48:17.725


1:48:17.725,1:48:20.725


1:48:20.725,1:48:23.635


1:48:23.635,1:48:26.365


1:48:26.365,1:48:29.505


1:48:29.505,1:48:30.505


1:48:30.845,1:48:32.635


1:48:33.185,1:48:36.615


1:48:36.885,1:48:39.555


1:48:39.555,1:48:42.575


1:48:42.975,1:48:44.595


1:48:45.015,1:48:48.055


1:48:48.055,1:48:50.985


1:48:50.985,1:48:53.555


1:48:53.755,1:48:56.335


1:48:56.665,1:48:59.325


1:49:00.185,1:49:02.175


1:49:04.045,1:49:07.005


1:49:07.935,1:49:09.805


1:49:11.645,1:49:13.355


1:49:13.665,1:49:14.785


1:49:16.105,1:49:19.425


1:49:20.125,1:49:21.545


1:49:28.785,1:49:29.995


1:49:31.485,1:49:34.495


1:49:34.495,1:49:37.395


1:49:37.395,1:49:40.385


1:49:40.415,1:49:43.405


1:49:43.405,1:49:46.495


1:49:46.495,1:49:47.495


1:49:47.595,1:49:49.805


1:49:50.165,1:49:53.195


1:49:53.425,1:49:56.775


1:49:57.065,1:49:58.665


1:49:59.045,1:50:01.005


1:50:01.855,1:50:05.135


1:50:05.605,1:50:08.605


1:50:08.605,1:50:12.055


1:50:12.415,1:50:15.475


1:50:15.475,1:50:17.315


1:50:17.665,1:50:20.335


1:50:20.815,1:50:23.875


1:50:23.875,1:50:26.885


1:50:26.885,1:50:29.605


1:50:29.605,1:50:32.215


1:50:32.215,1:50:33.685


1:50:34.115,1:50:37.275


1:50:37.335,1:50:40.345


1:50:40.345,1:50:42.295


1:50:42.605,1:50:45.525


1:50:45.525,1:50:47.885


1:50:47.915,1:50:51.055


1:50:51.055,1:50:52.055


1:50:52.285,1:50:54.585


1:50:54.585,1:50:57.335


1:50:57.335,1:50:59.855


1:51:00.305,1:51:03.165


1:51:03.875,1:51:07.325


1:51:07.685,1:51:10.275


1:51:11.055,1:51:12.585


1:51:13.165,1:51:16.215


1:51:16.215,1:51:18.215


1:51:18.215,1:51:20.285


1:51:20.895,1:51:23.815


1:51:23.815,1:51:26.855


1:51:26.855,1:51:29.735


1:51:30.285,1:51:31.695


1:51:32.655,1:51:35.325


1:51:35.325,1:51:37.805


1:51:37.805,1:51:40.835


1:51:40.835,1:51:43.805


1:51:43.805,1:51:46.795


1:51:46.795,1:51:47.795


1:51:48.535,1:51:51.685


1:51:55.125,1:51:56.125


1:51:56.495,1:51:59.285


1:51:59.285,1:52:02.405


1:52:02.405,1:52:05.245


1:52:05.245,1:52:08.285


1:52:08.285,1:52:09.285


1:52:09.405,1:52:12.395


1:52:12.395,1:52:15.395


1:52:15.395,1:52:18.155


1:52:18.155,1:52:21.395


1:52:21.435,1:52:24.505


1:52:24.505,1:52:27.885


1:52:29.035,1:52:30.035


1:52:32.305,1:52:33.825


1:52:34.485,1:52:37.495


1:52:37.885,1:52:38.885


1:52:39.225,1:52:42.225


1:52:42.225,1:52:45.035


1:52:45.035,1:52:47.805


1:52:47.805,1:52:50.885


1:52:50.885,1:52:53.485


1:52:53.655,1:52:56.665


1:52:56.665,1:52:59.625


1:52:59.625,1:53:01.685


1:53:02.105,1:53:03.325


1:53:03.775,1:53:06.885


1:53:06.925,1:53:09.595


1:53:09.595,1:53:12.585


1:53:12.585,1:53:15.235


1:53:15.655,1:53:18.415


1:53:18.415,1:53:21.445


1:53:21.445,1:53:24.585


1:53:24.585,1:53:27.665


1:53:27.665,1:53:30.515


1:53:30.515,1:53:33.665


1:53:33.665,1:53:36.585


1:53:37.785,1:53:38.785


1:53:41.515,1:53:44.695


1:53:44.695,1:53:47.765


1:53:48.055,1:53:51.145


1:53:51.145,1:53:53.975


1:53:53.975,1:53:56.835


1:53:56.835,1:54:00.205


1:54:00.935,1:54:03.955


1:54:03.955,1:54:06.025


1:54:06.025,1:54:09.005


1:54:09.005,1:54:11.675


1:54:11.675,1:54:14.385


1:54:14.385,1:54:17.385


1:54:17.385,1:54:20.715


1:54:20.715,1:54:21.715


1:54:22.575,1:54:24.345


1:54:25.465,1:54:28.125


1:54:28.125,1:54:31.135


1:54:31.135,1:54:34.155


1:54:34.155,1:54:37.225


1:54:37.225,1:54:38.225


1:54:38.385,1:54:41.435


1:54:41.435,1:54:44.135


1:54:47.535,1:54:48.535


1:54:50.885,1:54:53.945


1:54:53.945,1:54:56.765


1:54:56.765,1:54:59.775


1:54:59.775,1:55:02.885


1:55:02.885,1:55:05.295


1:55:05.325,1:55:08.325


1:55:08.325,1:55:09.325


1:55:09.875,1:55:13.175


1:55:13.175,1:55:16.205


1:55:16.205,1:55:19.265


1:55:19.265,1:55:22.395


1:55:22.395,1:55:25.385


1:55:25.385,1:55:28.355


1:55:28.355,1:55:31.615


1:55:31.795,1:55:33.735


1:55:34.075,1:55:36.915


1:55:36.915,1:55:40.345


1:55:40.415,1:55:43.145


1:55:43.445,1:55:46.115


1:55:46.765,1:55:49.655


1:55:49.655,1:55:52.825


1:55:52.825,1:55:55.995


1:55:55.995,1:55:56.995


1:55:57.245,1:56:00.525


1:56:00.525,1:56:03.645


1:56:04.705,1:56:07.335


1:56:08.575,1:56:11.645


1:56:12.395,1:56:15.305


1:56:15.305,1:56:18.515


1:56:18.515,1:56:21.155


1:56:21.155,1:56:22.645


1:56:23.205,1:56:26.135


1:56:26.135,1:56:29.175


1:56:29.175,1:56:31.135


1:56:31.975,1:56:34.945


1:56:34.945,1:56:37.945


1:56:37.945,1:56:40.875


1:56:40.875,1:56:43.895


1:56:43.895,1:56:46.975


1:56:46.975,1:56:49.895


1:56:49.895,1:56:52.375


1:56:52.905,1:56:53.905


1:56:55.315,1:56:57.855


1:56:58.165,1:57:01.235


1:57:02.085,1:57:05.175


1:57:05.175,1:57:08.175


1:57:08.175,1:57:09.925


1:57:10.335,1:57:12.325


1:57:12.905,1:57:15.885


1:57:15.885,1:57:18.625


1:57:18.625,1:57:19.625


1:57:20.715,1:57:23.195


1:57:25.765,1:57:28.845


1:57:28.845,1:57:32.085


1:57:32.085,1:57:35.005


1:57:35.145,1:57:36.145


1:57:36.365,1:57:37.365


1:57:37.465,1:57:40.495


1:57:40.495,1:57:41.495


1:57:41.595,1:57:44.425


1:57:44.425,1:57:45.745


1:57:46.235,1:57:47.395


1:57:47.765,1:57:50.815


1:57:51.165,1:57:52.345


1:57:53.145,1:57:56.255


1:57:56.255,1:57:59.375


1:57:59.375,1:58:02.475


1:58:02.475,1:58:05.535


1:58:05.535,1:58:08.525


1:58:08.525,1:58:11.525


1:58:11.725,1:58:14.815


1:58:14.815,1:58:17.835


1:58:17.835,1:58:20.895


1:58:20.895,1:58:22.095


1:58:22.775,1:58:25.665


1:58:25.665,1:58:28.235


1:58:28.725,1:58:31.955


1:58:31.955,1:58:34.875


1:58:34.925,1:58:38.025


1:58:38.025,1:58:39.255


1:58:39.655,1:58:40.655


1:58:40.945,1:58:41.825


1:58:41.825,1:58:45.125


1:58:45.125,1:58:48.275


1:58:48.275,1:58:50.695


1:58:51.085,1:58:53.025


1:58:53.655,1:58:54.655


1:58:55.105,1:58:57.655


1:58:57.655,1:59:00.745


1:59:00.745,1:59:03.335


1:59:03.335,1:59:06.115


1:59:06.765,1:59:07.765


1:59:08.235,1:59:11.225


1:59:11.225,1:59:14.285


1:59:14.285,1:59:17.365


1:59:17.365,1:59:20.305


1:59:20.305,1:59:23.255


1:59:23.255,1:59:26.435


1:59:26.435,1:59:29.505


1:59:30.545,1:59:33.555


1:59:33.555,1:59:36.665


1:59:36.735,1:59:39.125


1:59:39.535,1:59:42.435


1:59:42.435,1:59:45.495


1:59:45.495,1:59:48.585


1:59:49.135,1:59:52.215


1:59:52.215,1:59:55.225


1:59:55.225,1:59:58.155


1:59:58.155,2:00:01.455


2:00:01.515,2:00:04.345


2:00:04.505,2:00:07.465


2:00:07.465,2:00:08.465


2:00:08.995,2:00:12.015


2:00:12.015,2:00:15.075


2:00:15.075,2:00:18.095


2:00:18.095,2:00:21.175


2:00:21.175,2:00:24.155


2:00:24.155,2:00:27.145


2:00:27.145,2:00:30.105


2:00:30.105,2:00:32.515


2:00:32.515,2:00:35.375


2:00:35.375,2:00:38.575


2:00:38.575,2:00:41.575


2:00:41.575,2:00:42.605


2:00:43.515,2:00:46.085


2:00:46.085,2:00:49.245


2:00:49.245,2:00:51.765


2:00:52.565,2:00:55.155


2:00:55.155,2:00:58.125


2:00:58.125,2:01:01.085


2:01:01.085,2:01:04.195


2:01:04.195,2:01:05.715


2:01:07.475,2:01:10.495


2:01:10.495,2:01:13.215


2:01:13.215,2:01:16.285


2:01:16.635,2:01:19.505


2:01:19.505,2:01:22.385


2:01:22.385,2:01:25.325


2:01:25.325,2:01:28.405


2:01:28.405,2:01:29.405


2:01:29.445,2:01:31.015


2:01:31.345,2:01:34.025


2:01:34.025,2:01:37.105


2:01:37.385,2:01:39.985


2:01:40.005,2:01:42.875


2:01:42.875,2:01:45.555


2:01:45.555,2:01:48.595


2:01:48.595,2:01:52.005


2:01:52.005,2:01:53.005


2:01:54.215,2:01:57.025


2:01:57.025,2:01:58.585


2:01:58.995,2:02:02.025


2:02:02.025,2:02:04.755


2:02:04.755,2:02:06.255


2:02:07.435,2:02:10.145


2:02:10.175,2:02:11.665


2:02:12.175,2:02:14.935


2:02:14.935,2:02:18.125


2:02:18.675,2:02:20.705


2:02:21.035,2:02:22.035


2:02:22.245,2:02:24.815


2:02:29.325,2:02:31.915


2:02:31.915,2:02:34.705


2:02:34.705,2:02:37.775


2:02:37.775,2:02:40.885


2:02:40.885,2:02:42.695


2:02:43.455,2:02:45.845


2:02:45.845,2:02:48.855


2:02:48.855,2:02:51.945


2:02:52.565,2:02:55.545


2:02:55.805,2:02:58.005


2:02:58.465,2:03:01.645


2:03:01.645,2:03:04.575


2:03:04.575,2:03:07.555


2:03:07.555,2:03:10.665


2:03:10.665,2:03:12.685


2:03:13.425,2:03:14.425


2:03:15.115,2:03:18.215


2:03:18.515,2:03:19.685


2:03:20.355,2:03:21.355


2:03:21.715,2:03:22.715


2:03:24.105,2:03:26.755


2:03:26.755,2:03:29.685


2:03:29.685,2:03:32.775


2:03:32.775,2:03:35.765


2:03:35.765,2:03:38.815


2:03:38.815,2:03:41.925


2:03:41.925,2:03:43.285


2:03:43.835,2:03:46.775


2:03:46.775,2:03:49.815


2:03:49.815,2:03:52.815


2:03:52.815,2:03:55.745


2:03:55.745,2:03:58.645


2:03:58.645,2:04:00.325


2:04:01.775,2:04:04.815


2:04:04.815,2:04:07.775


2:04:07.775,2:04:10.345


2:04:10.345,2:04:11.845


2:04:12.235,2:04:13.235


2:04:16.435,2:04:17.465


2:04:17.995,2:04:20.945


2:04:20.945,2:04:23.985


2:04:23.985,2:04:25.975


2:04:26.745,2:04:29.825


2:04:29.825,2:04:32.755


2:04:32.755,2:04:35.795


2:04:35.795,2:04:38.185


2:04:38.315,2:04:41.215


2:04:41.215,2:04:44.285


2:04:44.335,2:04:47.285


2:04:47.285,2:04:50.345


2:04:50.345,2:04:53.255


2:04:53.525,2:04:56.385


2:04:56.385,2:04:59.425


2:04:59.425,2:05:02.265


2:05:02.265,2:05:05.715


2:05:05.725,2:05:07.905


2:05:09.215,2:05:12.195


2:05:12.745,2:05:15.165


2:05:15.505,2:05:18.585


2:05:18.585,2:05:21.455


2:05:21.455,2:05:24.635


2:05:26.305,2:05:29.375


2:05:29.375,2:05:30.975


2:05:31.295,2:05:34.235


2:05:34.235,2:05:37.185


2:05:37.185,2:05:40.255


2:05:40.255,2:05:43.495


2:05:43.495,2:05:46.525


2:05:46.525,2:05:48.985


2:05:48.985,2:05:51.175


2:05:51.565,2:05:52.565


2:05:53.685,2:05:56.275


2:05:56.275,2:05:59.565


2:06:00.095,2:06:01.095


2:06:01.175,2:06:04.305


2:06:04.535,2:06:07.535


2:06:07.535,2:06:10.585


2:06:10.585,2:06:13.515


2:06:13.515,2:06:16.185


2:06:16.375,2:06:19.345


2:06:19.345,2:06:22.345


2:06:22.345,2:06:24.025


2:06:24.875,2:06:27.915


2:06:27.955,2:06:30.165


2:06:30.945,2:06:33.915


2:06:33.915,2:06:36.865


2:06:36.865,2:06:39.845


2:06:39.845,2:06:42.685


2:06:42.685,2:06:45.795


2:06:45.795,2:06:48.785


2:06:48.785,2:06:51.525


2:06:51.525,2:06:54.215


2:06:54.215,2:06:57.265


2:06:57.265,2:07:00.165


2:07:00.165,2:07:03.175


2:07:03.175,2:07:06.015


2:07:06.015,2:07:08.975


2:07:08.975,2:07:12.085


2:07:12.085,2:07:14.895


2:07:14.895,2:07:17.985


2:07:18.305,2:07:21.085


2:07:21.695,2:07:23.895


2:07:24.395,2:07:25.455


2:07:25.895,2:07:27.145


2:07:27.555,2:07:30.495


2:07:30.495,2:07:31.495


2:07:31.555,2:07:34.635


2:07:34.635,2:07:37.675


2:07:37.675,2:07:40.625


2:07:40.625,2:07:43.505


2:07:43.505,2:07:46.215


2:07:46.215,2:07:49.135


2:07:49.135,2:07:52.105


2:07:52.285,2:07:55.275


2:07:55.275,2:07:58.475


2:07:58.485,2:08:01.565


2:08:02.105,2:08:05.025


2:08:05.025,2:08:06.535


2:08:06.895,2:08:10.025


2:08:10.025,2:08:13.015


2:08:13.015,2:08:15.975


2:08:15.975,2:08:17.405


2:08:18.155,2:08:19.155


2:08:19.325,2:08:22.235


2:08:22.435,2:08:25.535


2:08:25.535,2:08:28.695


2:08:28.695,2:08:31.855


2:08:31.985,2:08:35.095


2:08:35.095,2:08:37.625


2:08:37.625,2:08:40.375


2:08:40.375,2:08:43.485


2:08:43.835,2:08:46.075


2:08:46.515,2:08:49.515


2:08:49.675,2:08:52.375


2:08:52.375,2:08:55.425


2:08:55.505,2:08:57.435


2:08:57.745,2:08:58.745


2:08:58.865,2:09:02.255


2:09:02.635,2:09:05.205


2:09:05.205,2:09:08.285


2:09:08.285,2:09:10.525


2:09:11.305,2:09:14.225


2:09:14.225,2:09:17.155


2:09:17.155,2:09:19.945


2:09:19.945,2:09:22.775


2:09:22.775,2:09:25.855


2:09:25.855,2:09:29.205


2:09:29.715,2:09:32.915


2:09:32.915,2:09:35.155


2:09:35.725,2:09:36.725


2:09:37.315,2:09:40.435


2:09:40.435,2:09:43.305


2:09:43.305,2:09:46.415


2:09:46.415,2:09:49.445


2:09:49.445,2:09:52.495


2:09:52.495,2:09:55.255


2:09:55.255,2:09:56.575


2:09:57.035,2:10:00.215


2:10:00.215,2:10:03.215


2:10:03.215,2:10:06.235


2:10:06.235,2:10:09.285


2:10:09.625,2:10:12.595


2:10:12.595,2:10:15.805


2:10:15.805,2:10:18.485


2:10:18.485,2:10:19.505


2:10:20.125,2:10:22.865


2:10:22.865,2:10:24.215


2:10:24.685,2:10:25.685


2:10:26.295,2:10:29.055


2:10:29.055,2:10:32.365


2:10:32.365,2:10:34.495


2:10:34.715,2:10:37.615


2:10:38.245,2:10:41.295


2:10:41.295,2:10:44.215


2:10:44.215,2:10:47.435


2:10:47.435,2:10:50.445


2:10:50.445,2:10:53.675


2:10:53.845,2:10:57.225


2:10:57.455,2:10:58.455


2:10:58.525,2:11:01.375


2:11:01.375,2:11:04.855


2:11:04.895,2:11:07.965


2:11:07.965,2:11:11.075


2:11:11.075,2:11:13.995


2:11:13.995,2:11:14.995


2:11:15.175,2:11:18.235


2:11:18.235,2:11:21.175


2:11:21.175,2:11:22.415


2:11:23.005,2:11:25.315


2:11:25.315,2:11:28.255


2:11:28.255,2:11:30.165


2:11:31.195,2:11:34.295


2:11:34.295,2:11:37.175


2:11:37.295,2:11:38.405


2:11:39.535,2:11:41.045


2:11:41.735,2:11:44.835


2:11:44.835,2:11:45.835


2:11:46.295,2:11:49.115


2:11:49.115,2:11:52.055


2:11:52.085,2:11:55.125


2:11:55.125,2:11:57.535


2:11:57.875,2:12:00.835


2:12:00.835,2:12:03.625


2:12:03.625,2:12:06.665


2:12:06.665,2:12:07.675


2:12:08.055,2:12:11.095


2:12:11.095,2:12:14.355


2:12:14.465,2:12:17.625


2:12:17.625,2:12:20.835


2:12:21.035,2:12:23.785


2:12:24.655,2:12:27.405


2:12:27.405,2:12:30.265


2:12:30.265,2:12:33.365


2:12:33.365,2:12:36.245


2:12:36.245,2:12:39.145


2:12:39.145,2:12:41.985


2:12:41.985,2:12:45.165


2:12:45.315,2:12:48.465


2:12:48.465,2:12:51.385


2:12:51.385,2:12:54.545


2:12:54.915,2:12:57.745


2:12:57.745,2:12:59.165


2:12:59.605,2:13:01.205


2:13:02.335,2:13:04.895


2:13:04.895,2:13:07.485


2:13:07.485,2:13:09.755


2:13:10.215,2:13:13.395


2:13:13.395,2:13:16.485


2:13:16.485,2:13:19.195


2:13:19.195,2:13:22.285


2:13:22.285,2:13:25.685


2:13:26.235,2:13:29.415


2:13:29.415,2:13:32.465


2:13:32.465,2:13:34.995


2:13:36.785,2:13:39.575


2:13:39.575,2:13:41.665


2:13:41.665,2:13:44.935


2:13:44.935,2:13:47.295


2:13:47.295,2:13:49.925


2:13:49.925,2:13:52.295


2:13:52.615,2:13:55.655


2:13:55.655,2:13:58.725


2:13:58.725,2:14:01.795


2:14:01.795,2:14:04.815


2:14:04.815,2:14:07.705


2:14:07.705,2:14:10.775


2:14:10.775,2:14:13.815


2:14:13.815,2:14:16.925


2:14:16.925,2:14:19.655


2:14:19.745,2:14:22.645


2:14:22.645,2:14:25.935


2:14:26.885,2:14:28.085

